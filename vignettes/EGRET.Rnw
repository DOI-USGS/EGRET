%\VignetteIndexEntry{Introduction to the EGRET package}
%\VignetteEngine{knitr::knitr}
%\VignetteDepends{}
%\VignetteSuggests{xtable,extrafont,knitr}
%\VignetteImports{methods,survival, fields,dataRetrieval, stats, lubridate}
%\VignettePackage{EGRET}

\documentclass[a4paper,11pt]{article}

\usepackage{amsmath}
\usepackage{times}
\usepackage{hyperref}
\usepackage[numbers, round]{natbib}
\usepackage[american]{babel}
\usepackage{authblk}
\usepackage{subfig}
\usepackage{placeins}
\usepackage{footnote}
\usepackage{tabularx}
\usepackage{parskip}
\usepackage{threeparttable}
\renewcommand\Affilfont{\itshape\small}

\usepackage{csquotes}
\usepackage{setspace}

% \doublespacing

\renewcommand{\topfraction}{0.85}
\renewcommand{\textfraction}{0.1}
\usepackage{graphicx}


\usepackage{mathptmx}% Times Roman font
\usepackage[scaled=.90]{helvet}% Helvetica, served as a model for arial

% \usepackage{indentfirst}
% \setlength\parindent{20pt}
\setlength{\parskip}{0pt}

\usepackage{courier}

\usepackage{titlesec}
\usepackage{titletoc}

\titleformat{\section}
  {\normalfont\sffamily\bfseries\LARGE}
  {\thesection}{0.5em}{}
\titleformat{\subsection}
  {\normalfont\sffamily\bfseries\Large}
  {\thesubsection}{0.5em}{}
\titleformat{\subsubsection}
  {\normalfont\sffamily\large}
  {\thesubsubsection}{0.5em}{}
  
\titlecontents{section}
[2.3em]                 % adjust left margin
{\sffamily}             % font formatting
{\contentslabel{2.3em}} % section label and offset
{\hspace*{-2.3em}}
{\titlerule*[0.25pc]{.}\contentspage}
  
\titlecontents{subsection}
[4.6em]                 % adjust left margin
{\sffamily}             % font formatting
{\contentslabel{2.3em}} % section label and offset
{\hspace*{-2.3em}}
{\titlerule*[0.25pc]{.}\contentspage}
  
\titlecontents{subsubsection}
[6.9em]                 % adjust left margin
{\sffamily}             % font formatting
{\contentslabel{2.3em}} % section label and offset
{\hspace*{-2.3em}}
{\titlerule*[0.25pc]{.}\contentspage}

\titlecontents{table}
[0em]                 % adjust left margin
{\sffamily}             % font formatting
{Table\hspace*{2em} \contentslabel {2em}} % section label and offset
{\hspace*{4em}}
{\titlerule*[0.25pc]{.}\contentspage}

\titlecontents{figure}
[0em]                 % adjust left margin
{\sffamily}             % font formatting
{Figure\hspace*{2em} \contentslabel {2em}} % section label and offset
{\hspace*{4em}}
{\titlerule*[0.25pc]{.}\contentspage}

%Italisize and change font of urls:
\urlstyle{sf}
\renewcommand\UrlFont\itshape

\usepackage{caption}
\captionsetup{
  font={sf},
  labelfont={bf,sf},
  labelsep=period,
  justification=justified,
  singlelinecheck=false
}



\textwidth=6.5in
\textheight=9.2in
\parskip=.3cm
\oddsidemargin=.1in
\evensidemargin=.1in
\headheight=-.3in

%------------------------------------------------------------
% newcommand
%------------------------------------------------------------
\newcommand{\scscst}{\scriptscriptstyle}
\newcommand{\scst}{\scriptstyle}
\newcommand{\Robject}[1]{{\texttt{#1}}}
\newcommand{\Rfunction}[1]{{\texttt{#1}}}
\newcommand{\Rclass}[1]{\textit{#1}}
\newcommand{\Rpackage}[1]{\textit{#1}}
\newcommand{\Rexpression}[1]{\texttt{#1}}
\newcommand{\Rmethod}[1]{{\texttt{#1}}}
\newcommand{\Rfunarg}[1]{{\texttt{#1}}}

\begin{document}

\renewenvironment{knitrout}{\begin{singlespace}}{\end{singlespace}}
\renewcommand*\listfigurename{Figures}
\renewcommand*\listtablename{Tables}

<<openLibrary, echo=FALSE>>=
library(xtable)
options(continue=" ")
options(width=60)
library(knitr)
library(EGRET)
@


<<include=TRUE ,echo=FALSE,eval=TRUE>>=
opts_chunk$set(highlight=TRUE, tidy=TRUE, keep.space=TRUE, keep.blank.space=FALSE, keep.comment=TRUE, concordance=TRUE,tidy=FALSE,comment="")

knit_hooks$set(inline = function(x) {
   if (is.numeric(x)) round(x, 3)})
knit_hooks$set(crop = hook_pdfcrop)

bold.colHeaders <- function(x) {
  x <- gsub("\\^(\\d)","$\\^\\1$",x)
  x <- gsub("\\%","\\\\%",x)
  x <- gsub("\\_"," ",x)
  returnX <- paste("\\multicolumn{1}{c}{\\textbf{\\textsf{", x, "}}}", sep = "")
}

addSpace <- function(x) ifelse(x != "1", "[5pt]","")

@

%------------------------------------------------------------
\title{Introduction to the EGRET package}
%------------------------------------------------------------
\author[1]{Robert M. Hirsch}
\author[1]{Laura A. De Cicco}
\affil[1]{United States Geological Survey}

\noindent{\huge\textsf{\textbf{Introduction to the EGRET package}}}

\noindent\textsf{By Robert M. Hirsch and Laura A. De Cicco}

\noindent\textsf{\today}

% \maketitle
% 
% \newpage 

\tableofcontents
\listoffigures
\listoftables

\newpage


%------------------------------------------------------------
\section{Introduction to Exploration and Graphics for RivEr Trends (EGRET)}
%------------------------------------------------------------ 

EGRET includes statistics and graphics for streamflow history, water quality trends, and the statistical modeling algorithm Weighted Regressions on Time, Discharge, and Season (WRTDS).  Please see the official EGRET User Guide \cite{HirschI} for more information on the EGRET package:

(\url{http://dx.doi.org/10.3133/tm4A10}) 

For information on getting started in R, downloading and installing the package, see section \ref{sec:appendix1}.

The best ways to learn about the WRTDS approach is to read the User Guide and two journal articles. These articles are available, for free, from the journals in which they were published. The first relates to nitrate and total phosphorus data for 9 rivers draining to Chesapeake Bay. The URL is \cite{HirschII}: \url{http://onlinelibrary.wiley.com/doi/10.1111/j.1752-1688.2010.00482.x/full}. The second is an application to nitrate data for 8 monitoring sites on the Mississippi River or its major tributaries \cite{HirschIII}.  The URL is: \url{http://pubs.acs.org/doi/abs/10.1021/es201221s}

This vignette assumes that you understand the concepts underlying WRTDS, and reading the relevant sections of the User Guide at least the first of these papers. 

Any use of trade, firm, or product names is for descriptive purposes only and does not imply endorsement by the U.S. Government.


%------------------------------------------------------------
\section{EGRET Workflow}
%------------------------------------------------------------ 
Subsequent sections of this vignette discuss the EGRET workflow steps in greater detail. This section provides a handy cheat sheet for diving into an EGRET analysis. The first example is for a flow history analysis:


<<workflowFlowHistory, echo=TRUE,eval=FALSE>>=
library(EGRET)

# Flow history analysis

############################
# Gather discharge data:
siteNumber <- "01491000" #Choptank River at Greensboro, MD
startDate <- "" # Get earliest date
endDate <- "" # Get latest date
Daily <- readNWISDaily(siteNumber,"00060",startDate,endDate)
# Gather site and parameter information:
# Here user must input some values for
# the default (interactive=TRUE)
INFO <- readNWISInfo(siteNumber,"00060")
INFO$shortName <- "Choptank River near Greensboro, MD"
############################

############################
# Check flow history data:
eList <- as.egret(INFO, Daily, NA, NA)
plotFlowSingle(eList, istat=7,qUnit="thousandCfs")
plotSDLogQ(eList)
plotQTimeDaily(eList, qLower=1,qUnit=3)
plotFour(eList, qUnit=3)
plotFourStats(eList, qUnit=3)
############################

# modify this for your own computer file structure:
savePath<-"/Users/rhirsch/Desktop/" 
saveResults(savePath, eList)

@

The second workflow example is for a water quality analysis. It includes data retrieval, merging of water quality and streamflow data, running the WRTDS estimation, and various plotting functions available in the EGRET package.


<<workflowWaterQuality, echo=TRUE,eval=FALSE>>=
library(EGRET)

############################
# Gather discharge data:
siteNumber <- "01491000" #Choptank River near Greensboro, MD
startDate <- "" #Gets earliest date
endDate <- "2011-09-30"
# Gather sample data:
parameter_cd<-"00631" #5 digit USGS code
Sample <- readNWISSample(siteNumber,parameter_cd,startDate,endDate)
#Gets earliest date from Sample record:
#This is just one of many ways to assure the Daily record
#spans the Sample record
startDate <- min(as.character(Sample$Date)) 
# Gather discharge data:
Daily <- readNWISDaily(siteNumber,"00060",startDate,endDate)
# Gather site and parameter information:

# Here user must input some values:
INFO<- readNWISInfo(siteNumber,parameter_cd)
INFO$shortName <- "Choptank River at Greensboro, MD"

# Merge discharge with sample data:
eList <- mergeReport(INFO, Daily, Sample)
############################

############################
# Check sample data:
boxConcMonth(eList)
boxQTwice(eList)
plotConcTime(eList)
plotConcQ(eList)
multiPlotDataOverview(eList)
############################

############################
# Run WRTDS model:
eList <- modelEstimation(eList)
############################

############################
#Check model results:

#Require Sample + INFO:
plotConcTimeDaily(eList)
plotFluxTimeDaily(eList)
plotConcPred(eList)
plotFluxPred(eList)
plotResidPred(eList)
plotResidQ(eList)
plotResidTime(eList)
boxResidMonth(eList)
boxConcThree(eList)

#Require Daily + INFO:
plotConcHist(eList)
plotFluxHist(eList)

# Multi-line plots:
date1 <- "2000-09-01"
date2 <- "2005-09-01"
date3 <- "2009-09-01"
qBottom<-5
qTop<-1000
plotConcQSmooth(eList, date1, date2, date3, qBottom, qTop, 
                   concMax=2,qUnit=1)
q1 <- 10
q2 <- 25
q3 <- 75
centerDate <- "07-01"
yearEnd <- 2009
yearStart <- 2000
plotConcTimeSmooth(eList, q1, q2, q3, centerDate, yearStart, yearEnd)

# Multi-plots:
fluxBiasMulti(eList)

#Contour plots:
clevel<-seq(0,2,0.5)
maxDiff<-0.8
yearStart <- 2000
yearEnd <- 2010

plotContours(eList, yearStart,yearEnd,qBottom,qTop, 
             contourLevels = clevel,qUnit=1)
plotDiffContours(eList, yearStart,yearEnd,
                 qBottom,qTop,maxDiff,qUnit=1)

# modify this for your own computer file structure:
savePath<-"/Users/rhirsch/Desktop/" 
saveResults(savePath, eList)
@


%------------------------------------------------------------ 
\section{EGRET Data Frames and Retrieval Options}
\label{sec:data frames}
%------------------------------------------------------------ 
The EGRET package uses 3 default data frames throughout the calculations, analysis, and graphing. These data frames are Daily (\ref{sec:data framesDaily}), Sample (\ref{sec:data framesSample}), and INFO (\ref{sec:data framesINFO}). The data frames are combined into a named list for all EGRET functions using the \texttt{as.egret} function (\ref{sec:eList}). 

A package that EGRET depends on is called dataRetrieval. This package provides the core functionality to import hydrologic data from USGS and EPA web services. See the dataRetrieval vignette for more information.

<<openDataRetrieval, eval=FALSE>>=
library(dataRetrieval)
vignette("dataRetrieval")
@

EGRET uses entirely SI units to store the data, but for purposes of output, it can report results in a wide variety of units, which will be discussed in (\ref{sec:units}). To start our exploration, you must install the packages (check Section \ref{sec:appendix1} for detailed instructions), and then open EGRET with the following command:

<<openlibraries, echo=TRUE,eval=TRUE>>=
library(EGRET)
@

%------------------------------------------------------------ 
\subsection{Daily}
\label{sec:data framesDaily}
%------------------------------------------------------------ 
The Daily data frame can be imported into R either from USGS web services (\texttt{readNWISDaily}) or user-generated files (\texttt{readUserDaily}).  After you run the WRTDS calculations by using the function \texttt{modelEstimation} (as will be described in section \ref{sec:wrtds}), additional columns are inserted (Table \ref{table:Daily2}).

\begin{table}[!ht]
{\footnotesize
\caption{Daily data frame} 
\label{table:Daily1}
\begin{tabular}{llll}
  \hline
\multicolumn{1}{c}{\textbf{\textsf{ColumnName}}} &
\multicolumn{1}{c}{\textbf{\textsf{Type}}} &
\multicolumn{1}{c}{\textbf{\textsf{Description}}} &
\multicolumn{1}{c}{\textbf{\textsf{Units}}} \\ 
  \hline
  Date & Date & Date & date \\ 
  [5pt]Q & number & Discharge in m\textsuperscript{3}/s & m\textsuperscript{3}/s \\ 
  [5pt]Julian & number & Number of days since January 1, 1850 & days \\ 
  [5pt]Month & integer & Month of the year [1-12] & months \\ 
  [5pt]Day & integer & Day of the year [1-366] & days \\ 
  [5pt]DecYear & number & Decimal year & years \\ 
  [5pt]MonthSeq & integer & Number of months since January 1, 1850 & months \\ 
  [5pt]Qualifier & character & Qualifying code & string \\ 
  [5pt]i & integer & Index of days, starting with 1 & days \\ 
  [5pt]LogQ & number & Natural logarithm of Q & numeric \\ 
  [5pt]Q7 & number & 7 day running average of Q & m\textsuperscript{3}/s \\ 
  [5pt]Q30 & number & 30 day running average of Q & m\textsuperscript{3}/s \\ 
   \hline
\end{tabular}
}
\end{table}

\begin{table}[!ht]
{\footnotesize
\caption{Columns added to Daily data frame after running \texttt{modelEstimation} }
\label{table:Daily2}
\begin{tabular}{llll}
  \hline
\multicolumn{1}{c}{\textbf{\textsf{ColumnName}}} &
\multicolumn{1}{c}{\textbf{\textsf{Type}}} &
\multicolumn{1}{c}{\textbf{\textsf{Description}}} &
\multicolumn{1}{c}{\textbf{\textsf{Units}}} \\ 
  \hline
yHat & number & The WRTDS estimate of the log of concentration & numeric \\ 
  [5pt]SE & number & The WRTDS estimate of the standard error of yHat & numeric \\ 
  [5pt]ConcDay & number & The WRTDS estimate of concentration & mg/L \\ 
  [5pt]FluxDay & number & The WRTDS estimate of flux & kg/day \\ 
  [5pt]FNConc & number & Flow-normalized estimate of concentration & mg/L \\ 
  [5pt]FNFlux & number & Flow-normalized estimate of flux & kg/day \\ 
   \hline
\end{tabular}
}
\end{table}

\FloatBarrier

Notice that the \enquote{Day of the year} column can span from 1 to 366. The 366 accounts for leap years. Every day has a consistent day of the year. This means, February 28\textsuperscript{th} is always the 59\textsuperscript{th} day of the year, Feb. 29\textsuperscript{th} is always the 60\textsuperscript{th} day of the year, and March 1\textsuperscript{st} is always the 61\textsuperscript{st} day of the year whether or not it is a leap year.

%------------------------------------------------------------ 
\subsubsection{readNWISDaily}
\label{sec:nwisDailyFile}
%------------------------------------------------------------ 

The \texttt{readNWISDaily} function retrieves the daily values (discharge in this case) from a USGS web service.  It requires the inputs siteNumber, parameterCd, startDate, endDate, interactive, and convert. 

These arguments are described in detail in the \texttt{dataRetrieval} vignette, however \texttt{"}convert\texttt{"} is a new argument (which defaults to TRUE). The convert argument tells the program to convert the values from cubic feet per second (ft\textsuperscript{3}/s) to cubic meters per second (m\textsuperscript{3}/s) as shown in the example Daily data frame in Table \ref{table:Daily1}. For EGRET applications with NWIS Web retrieval, do not use this argument (the default is TRUE), EGRET assumes that discharge is always stored in units of cubic meters per second. If you don't want this conversion and are not using EGRET, set convert=FALSE in the function call. 

<<firstExample, echo=TRUE, eval=FALSE>>=
siteNumber <- "01491000"
startDate <- "2000-01-01"
endDate <- "2013-01-01"
# This call will get NWIS (ft3/s) data , and convert it to m3/s:
Daily <- readNWISDaily(siteNumber, "00060", startDate, endDate)
@


If discharge values are negative or zero, the code will set all of these values to zero and then add a small constant to all of the daily discharge values.  This constant is 0.001 times the mean discharge.  The code will also report on the number of zero and negative values and the size of the constant.  Use EGRET analysis only if the number of zero values is a very small fraction of the total days in the record (say less than 0.1\% of the days), and there are no negative discharge values.  Columns Q7 and Q30 are the 7 and 30 day running averages for the 7 or 30 days ending on this specific date. Table \ref{table:Daily1} lists details of the Daily data frame.

\FloatBarrier

%------------------------------------------------------------ 
\subsubsection{readUserDaily}
\label{sec:DailyFile}
%------------------------------------------------------------ 
The \texttt{readUserDaily} function will load a user-supplied text file and convert it to the Daily data frame. The file should have two columns, the first dates, the second values.  The dates are formatted either mm/dd/yyyy or yyyy-mm-dd. Using a 4-digit year is required. This function has the following inputs: filePath, fileName,hasHeader (TRUE/FALSE), separator, qUnit, and interactive (TRUE/FALSE). filePath is a character that defines the path to your file, and the character can either be a full path, or path relative to your R working directory. The input fileName is a character that defines the file name (including the extension).

Text files that contain this sort of data require some sort of a separator, for example, a \enquote{csv} file (comma-separated value) file uses a comma to separate the date and value column. A tab delimited file would use a tab (\verb@"\t"@) rather than the comma (\texttt{"},\texttt{"}). Define the type of separator you choose to use in the function call in the \texttt{"}separator\texttt{"} argument, the default is \texttt{"},\texttt{"}. Another function input is a logical variable: hasHeader.  The default is TRUE. If your data does not have column names, set this variable to FALSE.

Finally, qUnit is a numeric argument that defines the discharge units used in the input file.  The default is qUnit = 1 which assumes discharge is in cubic feet per second.  If the discharge in the file is already in cubic meters per second then set qUnit = 2.  If it is in some other units (like liters per second or acre-feet per day), the user must pre-process the data with a unit conversion that changes it to either cubic feet per second or cubic meters per second.

So, if you have a file called \enquote{ChoptankRiverFlow.txt} located in a folder called \enquote{RData} on the C drive (this example is for the Windows\textregistered\ operating systems), and the file is structured as follows (tab-separated):


% \singlespacing
\begin{verbatim}
date  Qdaily
10/1/1999  107
10/2/1999  85
10/3/1999  76
10/4/1999  76
10/5/1999	113
10/6/1999	98
...
\end{verbatim}
% \doublespacing

The call to open this file, convert the discharge to cubic meters per second, and populate the Daily data frame would be:
<<openDaily, eval = FALSE>>=
fileName <- "ChoptankRiverFlow.txt"
filePath <-  "C:/RData/"
Daily <-readDataFromFile(filePath,fileName,
                    separator="\t")
@

Microsoft\textregistered\ Excel files can be a bit tricky to import into R directly. The simplest way to get Excel data into R is to open the Excel file in Excel, then save it as a .csv file (comma-separated values). 

\FloatBarrier

%------------------------------------------------------------ 
\subsection{Sample}
\label{sec:data framesSample}
%------------------------------------------------------------ 
The Sample data frame initially is populated with columns generated by either the \texttt{readNWISSample}, \texttt{readWQPSample}, or \texttt{readUserSample} functions (Table \ref{table:Sample1}). After you run the WRTDS calculations using the \texttt{modelEstimation} function (as described in section \ref{sec:wrtds}), additional columns are inserted (Table \ref{table:Sample2}):

\begin{table}[!ht]
{\footnotesize
  \begin{threeparttable}[b]
\caption{Sample data frame} 
\label{table:Sample1}
\begin{tabular}{llll}
  \hline
\multicolumn{1}{c}{\textbf{\textsf{ColumnName}}} &
\multicolumn{1}{c}{\textbf{\textsf{Type}}} &
\multicolumn{1}{c}{\textbf{\textsf{Description}}} &
\multicolumn{1}{c}{\textbf{\textsf{Units}}} \\ 
  \hline
Date & Date & Date & date \\ 
  [5pt]ConcLow & number & Lower limit of concentration & mg/L \\ 
  [5pt]ConcHigh & number & Upper limit of concentration & mg/L \\ 
  [5pt]Uncen & integer & Uncensored data (1=true, 0=false) & integer \\ 
  [5pt]ConcAve & number & Average concentration & mg/L \\ 
  [5pt]Julian & number & Number of days since January 1, 1850 & days \\ 
  [5pt]Month & integer & Month of the year [1-12] & months \\ 
  [5pt]Day & integer & Day of the year [1-366] & days \\ 
  [5pt]DecYear & number & Decimal year & years \\ 
  [5pt]MonthSeq & integer & Number of months since January 1, 1850 & months \\ 
  [5pt]SinDY & number & Sine of DecYear & numeric \\ 
  [5pt]CosDY & number & Cosine of DecYear & numeric \\ 
  [5pt]Q \tnote{1} & number & Discharge & cms \\ 
  [5pt]LogQ \tnote{1} & number & Natural logarithm of discharge & numeric \\ 
   \hline
\end{tabular}
  \begin{tablenotes}
    \item[1] Populated after calling \texttt{mergeReport}.
  \end{tablenotes}
 \end{threeparttable}
}
\end{table}


\begin{table}[!ht]
{\footnotesize
\begin{threeparttable}[b]
\caption{Columns added to Sample data frame after running \texttt{modelEstimation}}
\label{table:Sample2}
\begin{tabular}{llll}
  \hline
\multicolumn{1}{c}{\textbf{\textsf{ColumnName}}} &
\multicolumn{1}{c}{\textbf{\textsf{Type}}} &
\multicolumn{1}{c}{\textbf{\textsf{Description}}} &
\multicolumn{1}{c}{\textbf{\textsf{Units}}} \\ 
  \hline
yHat\tnote{1} & number & estimate of the log of concentration & numeric \\ 
  [5pt]SE\tnote{1} & number & estimate of the standard error of yHat & numeric \\ 
  [5pt]ConcHat\tnote{1} & number & unbiased estimate of concentration & mg/L \\ 
   \hline
\end{tabular}

  \begin{tablenotes}
    \item[1] These estimates are \enquote{leave-one-out cross validation} estimates.  See the EGRET User Guide for more details.
  \end{tablenotes}
 \end{threeparttable}
}
\end{table}

As with the Daily data frame, the \enquote{Day of the year} column can span from 1 to 366. The 366 accounts for leap years. Every day has a consistent day of the year. This means, February 28\textsuperscript{th} is always the 59\textsuperscript{th} day of the year, Feb. 29\textsuperscript{th} is always the 60\textsuperscript{th} day of the year, and March 1\textsuperscript{st} is always the 61\textsuperscript{st} day of the year whether or not it is a leap year.

Section \ref{sec:cenValues} is about summing multiple constituents, including how interval censoring is used. Since the Sample data frame is structured to only contain one constituent, when more than one parameter codes are requested, the \texttt{readNWISSample} function will sum the values of each constituent as described below.


\FloatBarrier

%------------------------------------------------------------ 
\subsubsection{readNWISSample}
\label{sec:readNWISSample}
%------------------------------------------------------------ 
The \texttt{readNWISSample} function retrieves USGS sample data from NWIS. The arguments for this function are also siteNumber, parameterCd, startDate, endDate, interactive. These are the same inputs as \texttt{readNWISDaily} as described in the previous section.

<<secondExample,echo=TRUE,eval=FALSE>>=
siteNumber <- "01491000"
parameterCd <- "00618"
Sample <-readNWISSample(siteNumber,parameterCd,
      startDate, endDate)
@

Information on USGS parameter codes can be found here:

\url{http://help.waterdata.usgs.gov/codes-and-parameters/parameters}

%------------------------------------------------------------ 
\subsubsection{readWQPSample}
\label{sec:readWQPSample}
%------------------------------------------------------------ 

The \texttt{readWQPSample} function retrieves Water Quality Portal sample data (STORET, NWIS, STEWARDS). The arguments for this function are siteNumber, characteristicName, startDate, endDate, interactive. 

<<STORET,echo=TRUE,eval=FALSE>>=
site <- 'WIDNR_WQX-10032762'
characteristicName <- 'Specific conductance'
Sample <-readWQPSample(site,characteristicName,
      startDate, endDate)
@

To request USGS data from the Water Quality Portal, the siteNumber must have \enquote{USGS-} pasted before the identification number. For USGS data, the \texttt{characteristicName} argument can be either a list of 5-digit parameter codes, or the characteristic name. A table that describes how USGS parameters relate with the defined characteristic name can be found here:

\url{http://www.waterqualitydata.us/public_srsnames.jsp}

\FloatBarrier

%------------------------------------------------------------ 
\subsubsection{readUserSample}
\label{sec:SampleFile}
%------------------------------------------------------------ 

The \texttt{readUserSample} function will import a user-generated file and populate the Sample data frame. The difference between sample data and discharge data is that the code requires a third column that contains a remark code, either blank or \verb@"<"@, which will tell the program that the data were \enquote{left-censored} (or, below the detection limit of the sensor). Therefore, the data must be in the form: date, remark, value.   An example of a comma-delimited file is:

\singlespacing
\begin{verbatim}
cdate;remarkCode;Nitrate
10/7/1999,,1.4
11/4/1999,<,0.99
12/3/1999,,1.42
1/4/2000,,1.59
2/3/2000,,1.54
...
\end{verbatim}

The call to open this file, and populate the Sample data frame is:
<<openSample, eval = FALSE>>=
fileName <- "ChoptankRiverNitrate.csv"
filePath <-  "C:/RData/"
Sample <-readUserSample(filePath,fileName,
                                separator=",")
@

When multiple constituents are to be summed, the format can be date, remark\_A, value\_A, remark\_b, value\_b, etc... A tab-separated example might look like the file below, where the columns are date, remark dissolved phosphate (rdp), dissolved phosphate (dp), remark particulate phosphorus (rpp), particulate phosphorus (pp), remark total phosphate (rtp), and total phosphate (tp):

\singlespacing
\begin{verbatim}
date  rdp  dp	rpp	pp	rtp	tp
2003-02-15		0.020		0.500		
2003-06-30	<	0.010		0.300		
2004-09-15	<	0.005	<	0.200		
2005-01-30						0.430
2005-05-30					<	0.050
2005-10-30					<	0.020
...
\end{verbatim}


<<openSample2, eval = FALSE>>=
fileName <- "ChoptankPhosphorus.txt"
filePath <-  "C:/RData/"
Sample <-readUserSample(filePath,fileName,
                                separator="\t")
@


\FloatBarrier

%------------------------------------------------------------
\subsubsection{Censored Values: Summation Explanation}
\label{sec:cenValues}
%------------------------------------------------------------
In the typical case where none of the data are censored (that is, no values are reported as \enquote{less-than} values), the ConcLow = ConcHigh = ConcAve which are all equal to the reported value, and Uncen = 1 for all values.  For the most common type of censoring, where a value is reported as less than the reporting limit, then ConcLow = NA, ConcHigh = reporting limit, ConcAve = 0.5 * reporting limit, and Uncen = 0.

To illustrate how the EGRET package handles a more complex censoring problem, let us say that in 2004 and earlier, we computed total phosphorus (tp) as the sum of dissolved phosphorus (dp) and particulate phosphorus (pp). From 2005 and onward, we have direct measurements of total phosphorus (tp). A small subset of this fictional data looks like Table \ref{tab:exampleComplexQW}.



<<label=tab:exampleComplexQW, echo=FALSE, eval=TRUE,results='asis'>>=
cdate <- c("2003-02-15","2003-06-30","2004-09-15","2005-01-30","2005-05-30","2005-10-30")
rdp <- c("", "<","<","","","")
dp <- c(0.02,0.01,0.005,NA,NA,NA)
rpp <- c("", "","<","","","")
pp <- c(0.5,0.3,0.2,NA,NA,NA)
rtp <- c("","","","","<","<")
tp <- c(NA,NA,NA,0.43,0.05,0.02)

DF <- data.frame(cdate,rdp,dp,rpp,pp,rtp,tp,stringsAsFactors=FALSE)

xTab <- xtable(DF, caption="Example data",digits=c(0,0,0,3,0,3,0,3),label="tab:exampleComplexQW")

print(xTab,
       caption.placement="top",
       size = "\\footnotesize",
       latex.environment=NULL,
       sanitize.colnames.function =  bold.colHeaders,
       sanitize.rownames.function = addSpace
      )

@

EGRET will \enquote{add up} all the values in a given row to form the total for that sample when using the Sample data frame. Thus, you only want to enter data that should be added together. If you want a data frame with multiple constituents that are not summed, do not use \texttt{readNWISSample}, \texttt{readWQPSample}, or \texttt{readUserSample}. The raw data functions: \texttt{getWQPData}, \texttt{getNWISqwData}, \texttt{getWQPqwData}, \texttt{getWQPData} from the EGRET package will not sum constituents, but leave them in their individual columns. 

For example, we might know the value for dp on 5/30/2005, but we don't want to put it in the table because under the rules of this data set, we are not supposed to add it in to the values in 2005.

For every sample, the EGRET package requires a pair of numbers to define an interval in which the true value lies (ConcLow and ConcHigh). In a simple uncensored case (the reported value is above the detection limit), ConcLow equals ConcHigh and the interval collapses down to a single point. In a simple censored case, the value might be reported as \verb@<@0.2, then ConcLow=NA and ConcHigh=0.2. We use NA instead of 0 as a way to elegantly handle future logarithm calculations.

For the more complex example case, let us say dp is reported as \verb@<@0.01 and pp is reported as 0.3. We know that the total must be at least 0.3 and could be as much as 0.31. Therefore, ConcLow=0.3 and ConcHigh=0.31. Another case would be if dp is reported as \verb@<@0.005 and pp is reported \verb@<@0.2. We know in this case that the true value could be as low as zero, but could be as high as 0.205. Therefore, in this case, ConcLow=NA and ConcHigh=0.205. The Sample data frame for the example data would be:

<<thirdExample,echo=FALSE>>=
  compressedData <- compressData(DF)
  Sample <- populateSampleColumns(compressedData)
@

<<thirdExampleView,echo=TRUE>>=
  Sample
@

\FloatBarrier


%------------------------------------------------------------ 
\subsection{INFO}
\label{sec:data framesINFO}
%------------------------------------------------------------ 
The INFO data frame stores information about the measurements, such as station name, parameter name, drainage area, and so forth. There can be many additional, optional columns, but the columns in Table \ref{table:Info1} are required to initiate the EGRET analysis. After you run the WRTDS calculations (as described in section \ref{sec:wrtds}), additional columns (Table \ref{table:Info2}) are automatically inserted into the INFO data frame (see the EGRET User Guide for complete description of each term):

\begin{table}[!ht]
{\footnotesize
\begin{threeparttable}[b]
\caption{INFO data frame}
\label{table:Info1}
\begin{tabular}{lll}
  \hline
\multicolumn{1}{c}{\textbf{\textsf{ColumnName}}} &
\multicolumn{1}{c}{\textbf{\textsf{Type}}} &
\multicolumn{1}{c}{\textbf{\textsf{Description}}} \\ 
  \hline
  shortName & character & Name of site, suitable for use in graphical headings \\ 
  [5pt]staAbbrev & character & Abbreviation for station name, used in saveResults \\ 
  [5pt]paramShortName & character & Name of constituent, suitable for use in graphical headings \\ 
  [5pt]constitAbbrev & character & Abbreviation for constituent name, used in saveResults \\ 
  [5pt]drainSqKm & numeric & Drainage area in  km\textsuperscript{2} \\ 
  [5pt]paStart \tnote{1} & integer (1-12) & Starting month of period of analysis \\ 
  [5pt]paLong \tnote{1} & integer (1-12) & Length of period of analysis in months \\ 
   \hline
\end{tabular}

\begin{tablenotes}
    \item[1] Inserted with the \texttt{setPA} function.
  \end{tablenotes}
 \end{threeparttable}
}
\end{table}


\begin{table}[!ht]
{\footnotesize
\caption{INFO data frame after running \texttt{modelEstimation}} 
\label{table:Info2}
\begin{tabular}{lll}
  \hline
\multicolumn{1}{c}{\textbf{\textsf{ColumnName}}} &
\multicolumn{1}{c}{\textbf{\textsf{Description}}} &
\multicolumn{1}{c}{\textbf{\textsf{Units}}} \\ 
  \hline
bottomLogQ & Lowest discharge in prediction surfaces & dimensionless \\ 
  [5pt]stepLogQ & Step size in log discharge in prediction surfaces & dimensionless \\ 
  [5pt]nVectorLogQ & Number of steps in discharge, prediction surfaces & integer \\ 
  [5pt]bottomYear & Starting year in prediction surfaces & years \\ 
  [5pt]stepYear & Step size in years in prediction surfaces & years \\ 
  [5pt]nVectorYear & Number of steps in years in prediction surfaces & integer \\ 
  [5pt]windowY & Half-window width in the time dimension & year \\ 
  [5pt]windowQ & Half-window width in the log discharge dimension & dimensionless \\ 
  [5pt]windowS & Half-window width in the seasonal dimension & years \\ 
  [5pt]minNumObs & Minimum number of observations for regression & integer \\ 
  [5pt]minNumUncen & Minimum number of uncensored observations & integer \\ 
   \hline
\end{tabular}
}
\end{table}


\FloatBarrier

%------------------------------------------------------------ 
\subsubsection{readNWISInfo}
\label{sec:nwisINFO}
%------------------------------------------------------------ 

The function \texttt{readNWISInfo} combines \texttt{readNWISsite} and \texttt{readNWISpCode} from the \texttt{dataRetrieval} package, producing one data frame called INFO.

<<ThirdExample, eval=FALSE>>=
parameterCd <- "00618"
siteNumber <- "01491000"
INFO <- readNWISInfo(siteNumber,parameterCd, interactive=FALSE)
@

%------------------------------------------------------------ 
\subsubsection{readWQPInfo}
\label{sec:wqpINFO}
%------------------------------------------------------------ 
It is also possible to create the INFO data frame using information from the Water Quality Portal. As with \texttt{readWQPSample}, if the requested site is a USGS siteNumber, \enquote{USGS-} needs to be appended to the siteNumber.

<<WQPInfo, eval=FALSE>>=
parameterCd <- "00618"
INFO_WQP <- readWQPInfo("USGS-01491000",parameterCd)
@

%------------------------------------------------------------ 
\subsubsection{readUserInfo}
\label{sec:userINFO}
%------------------------------------------------------------ 

The function \texttt{readUserInfo} can be used to convert comma separated files into an INFO data frame. At a minimum, EGRET analysis uses columns: param.units, shortName, paramShortName, constitAbbrev, and drainSqKm. For example, if the following comma-separated file (csv) was available as a file called \enquote{INFO.csv}, located in a folder called \enquote{RData} on the C drive (this examples is for s Windows\textregistered\  operation system), the function to convert it to an INFO data frame is as follows.


\begin{verbatim}
param.units, shortName, paramShortName, constitAbbrev, drainSqKm
mg/l, Choptank River, Inorganic nitrogen, N, 292.67
\end{verbatim}


<<addInfoCustom, eval=FALSE, echo=TRUE>>=

fileName <- "INFO.csv"
filePath <- "C:/RData/"

INFO <- readUserInfo(filePath, fileName)

@


%------------------------------------------------------------ 
\subsubsection{Inserting Additional Info}
\label{sec:addINFO}
%------------------------------------------------------------ 


Any supplemental column that would be useful can be added to the INFO data frame. 

<<addInfo, eval=FALSE, echo=TRUE>>=

INFO$riverInfo <- "Major tributary of the Chesapeake Bay"
INFO$GreensboroPopulation <- 1931

@


\FloatBarrier



%------------------------------------------------------------
\subsection{Merge Report: eList}
\label{sec:eList}
%------------------------------------------------------------
Finally, there is a function called \texttt{mergeReport} that will look at both the Daily and Sample data frame, and populate Q and LogQ columns into the Sample data frame. Once \texttt{mergeReport} has been run, the Sample data frame will be augmented with the daily discharges for all the days with samples, and a named list with all of the data frames will be created.  In this vignette, we will refer to this named list as \texttt{eList}: it is a list with potentially 3 data frames: Daily, Sample, and INFO. For flow history analysis, the Sample data frame can be NA.You can use the function \texttt{as.egret} to create this \enquote{EGRET} object.

None of the water quality functions in EGRET will work without first having run the \texttt{mergeReport} function.

<<mergeExample, eval=FALSE>>=
siteNumber <- "01491000"
parameterCd <- "00631"  # Nitrate
startDate <- "2000-01-01"
endDate <- "2013-01-01"

Daily <- readNWISDaily(siteNumber, "00060", startDate, endDate)
Sample <- readNWISSample(siteNumber,parameterCd, startDate, endDate)
INFO <- readNWISInfo(siteNumber, parameterCd)


eList <- mergeReport(INFO, Daily,Sample)

@

Perhaps you already have Daily, Sample, and INFO data frames, and surfaces matrix (created after running the WRTDS \texttt{modelEstimation}) that have gone though a deprecated version of EGRET. You can create and edit an EGRET object as follows:


<<egretObedit, echo=TRUE, eval=FALSE>>=

eListNew <- as.egret(INFO, Daily, Sample, surfaces)

#To pull out the INFO data frame:
INFO <- getInfo(eListNew)
#Edit the INFO data frame:
INFO$importantNews <- "New EGRET workflow started"
#Put new data frame in eListNew
eListNew$INFO <- INFO

#To pull out Daily:
Daily <- getDaily(eListNew)
#Edit for some reason:
DailyNew <- Daily[Daily$DecYear > 1985,]
#Put new Daily data frame back in eListNew:
eListNew$Daily <- DailyNew

#To create a whole new egret object:
eList_2 <- as.egret(INFO, DailyNew, getSample(eListNew), NA)

@


\FloatBarrier


%------------------------------------------------------------ 
\section{Units}
\label{sec:units}
%------------------------------------------------------------ 
EGRET uses entirely SI units to store the data, but for purposes of output, it can report results in a wide variety of units. The defaults are mg/L for concentration, cubic meters per second (m$^3$/s) for discharge, kg/day for flux, and km\textsuperscript{2} for drainage area. When discharge values are imported from USGS Web services, they are automatically converted from cubic feet per second (cfs) to cms unless the argument \texttt{"}convert\texttt{"} in function \texttt{readNWISDaily} is set to FALSE.  This can cause confusion if you are not careful. 

For all functions that provide output, you can define two arguments to set the output units: qUnit and fluxUnit.  qUnit and fluxUnit are defined by either a numeric code or name.  You can call two functions that can be called to see the options: \texttt{printqUnitCheatSheet} and \texttt{printFluxUnitCheatSheet}.


<<cheatSheets,echo=TRUE,eval=TRUE,results='markup'>>=
printqUnitCheatSheet()
@

When a function has an input argument qUnit, you can define the discharge units that will be used in the figure or table that is generated by the function with the index (1-4) as shown above. Base your choice on the units that are customary for your intended audience, but also so that the discharge values don't have too many digits to the right or left of the decimal point.

<<cheatSheets2,echo=TRUE,eval=TRUE,results='markup'>>=
printFluxUnitCheatSheet()
@

When a function has an input argument fluxUnit, you can define the flux units with the index (1-12) as shown above. Base the choice on the units that are customary for your intended audience, but also so that the flux values don't have too many digits to the right or left of the decimal point. Tons are always \enquote{short tons} and not \enquote{metric tons}.

\FloatBarrier

%------------------------------------------------------------ 
\section{Flow History}
\label{sec:flowHistory}
%------------------------------------------------------------ 
This section describes functions included in the EGRET package that provide a variety of table and graphical outputs for examining discharge statistics based on time-series smoothing. These functions are designed for studies of long-term change and work best for daily discharge data sets of 50 years or longer. This type of analysis might be useful for studying issues such as the influence of land use change, water management change, or climate change on discharge conditions.  This includes potential impacts on average discharges, high discharges, and low discharges, at annual time scales as well as seasonal or monthly time scales. 

Consider this example from Columbia River at The Dalles, OR.

<<flowHistory,echo=TRUE,eval=FALSE>>=
siteNumber <- "14105700"  
startDate <- ""
endDate <- ""

Daily <- readNWISDaily(siteNumber,"00060",startDate,endDate)
INFO <- readNWISInfo(siteNumber,"",interactive=FALSE)
INFO$shortName <- "Columbia River at The Dalles, OR"

eList <- as.egret(INFO, Daily, NA, NA)

@

<<flowHistoryLoad,echo=FALSE,eval=TRUE>>=
filePath <- system.file("extdata", package="EGRET")
fileName <- "eListColumbia.RData"

load(paste(filePath,fileName,sep="/"))
eList <- eListColumbia

@

You first must determine the period of analysis to use (PA). What is the period of analysis?  If you want to examine your data set as a time series of water years, then the period of analysis is October through September.  If you want to examine the data set as calendar years then the period of analysis is January through December.  You might want to examine the winter season, which you could define as December through February, then those 3 months become the period of analysis. The only constraints on the definition of a period of analysis are these: it must be defined in terms of whole months; it must be a set of contiguous months (like March-April-May), and have a length that is no less than 1 month and no more than 12 months.  Define the PA by using two arguments: paLong and paStart.  paLong is the length of the PA, and paStart is the first month of the PA. Table \ref{table:paINFO} summarizes paLong and paStart.

\begin{table}[!ht]
{\footnotesize
\caption{Period of Analysis Information} 
\label{table:paINFO}
\begin{tabular}{lll}
  \hline
\multicolumn{1}{c}{\textbf{\textsf{Period of Analysis}}} &
\multicolumn{1}{c}{\textbf{\textsf{paStart}}} &
\multicolumn{1}{c}{\textbf{\textsf{paLong}}} \\ 
  \hline
Calendar Year & 1 & 12 \\ 
  [5pt]Water Year & 10 & 12 \\ 
  [5pt]Winter & 12 & 3 \\ 
  [5pt]September & 9 & 1 \\ 
   \hline
\end{tabular}
}
\end{table}

To set a period running from December through February:
<<newChunckWinter, echo=TRUE,eval=FALSE>>=
eList <- setPA(eList,paStart=12,paLong=3)
@

To set the default value (water year):
<<newChunck, echo=TRUE,eval=TRUE>>=
eList <- setPA(eList)
@

The next step can be to create the annual series of discharge statistics.  These are returned in a matrix that contain the statistics described in table \ref{table:istat}. The statistics are based on the period of analysis set with the \texttt{setPA} function.

\begin{table}[!ht]
{\footnotesize
\caption{Index of discharge statistics information} 
\label{table:istat}
\begin{tabular}{ll}
  \hline
\multicolumn{1}{c}{\textbf{\textsf{istat}}} &
\multicolumn{1}{c}{\textbf{\textsf{Name}}} \\ 
  \hline
1 & minimum 1-day daily mean discharg \\ 
  [5pt]2 & minimum 7-day mean of the daily mean discharges \\ 
  [5pt]3 & minimum 30-day mean of the daily mean discharges \\ 
  [5pt]4 & median of the daily mean discharges \\ 
  [5pt]5 & mean of the daily mean discharges \\ 
  [5pt]6 & maximum 30-day mean of the daily mean discharges \\ 
  [5pt]7 & maximum 7-day mean of the daily mean discharges \\ 
  [5pt]8 & maximum 1-day daily mean discharge \\ 
   \hline
\end{tabular}
}
\end{table}

\FloatBarrier

%------------------------------------------------------------ 
\subsection{Plotting Options}
\label{sec:plotOptions}
%------------------------------------------------------------ 

This section shows examples of the available plots appropriate for studying discharge history. The plots here use the default variable input options.  For any function, you can get a complete list of input variables (as described in section \ref{sec:flowHistoryVariables}) in a help file by typing a ? before the function name in the R console. The EGRET user guide has more detailed information for each plot type (\url{http://pubs.usgs.gov/tm/04/a10/}). Finally, see section \ref{app:savingPlots} for information on saving plots.

The simplest way to look at these time series is with the function \texttt{plotFlowSingle}. The statistic index (istat) must be defined by the user, but for all other arguments there are default values so the user isn't required to specify anything else. To see a list of these optional arguments and other information about the function, type \text{?plotFlowSingle} in the R console. All of the graphs in \texttt{plotFlowSingle}, \texttt{plotFourStats}, and all but one of the graphs in plotFour, show both the individual annual values of the selected discharge statistic (e.g. the annual mean or 7-day minimum), but they also show a curve that is a smooth fit to those data.  The curve is a LOWESS (locally weighted scatterplot smooth).  The algorithm for computing it is provided in the User Guide (\url{http://pubs.usgs.gov/tm/04/a10/}), in the section titled \enquote{The Smoothing Method Used in Flow History Analyses.} The default is that the annual values of the selected discharge statistics are smoothed with a \enquote{half-window width} of 20 years.  The smoothing window is an optional user-defined option. 

\texttt{plotSDLogQ} produces a graphic of the running standard deviation of the log of daily discharge over time to visualize how variability of daily discharge is changing over time.  By using the standard deviation of the log discharge the statistic becomes dimensionless.  The standard deviation plot is a way of looking at variability quite aside from average values, so, in the case of a system where discharge might be increasing over a period of years, this graphic provides a way of looking at the variability relative to that changing mean value.  The standard deviation of the log discharge is much like a coefficient of variation, but it has sample properties that make it a smoother measure of variability.  People often comment about how things like urbanization or enhanced greenhouse gases in the atmosphere are bringing about an increase in variability, and this analysis is one way to explore that idea. \texttt{plotFour}, \texttt{plotFourStats}, and \texttt{plot15} are all designed to plot several graphs from the other functions in a single figure. 

\newpage
\FloatBarrier
<<plotSingleandSD, echo=TRUE, fig.cap="Plots of discharge statistics",fig.subcap=c("plotFlowSingle(eList, istat=5,qUnit='thousandCfs')","plotSDLogQ(eList)"),out.width='.5\\linewidth',out.height='.5\\linewidth',fig.show='hold',fig.pos="h",cache=TRUE>>=
plotFlowSingle(eList, istat=5,qUnit="thousandCfs")
plotSDLogQ(eList)
@

\newpage
\FloatBarrier

Here is an example of looking at daily mean discharge for the full water year and then looking at mean daily discharge for the winter season only for the Merced River at Happy Isles Bridge in Yosemite National Park in California.  First, we look at the mean daily discharge for the full year (after having read in the data and metadata):


<<Merced, echo=TRUE,eval=FALSE>>=
# Merced River at Happy Isles Bridge, CA:
siteNumber<-"11264500"
Daily <-readNWISDaily(siteNumber,"00060",startDate="",endDate="")
INFO <- readNWISInfo(siteNumber,"",interactive=FALSE)
INFO$shortName <- "Merced River at Happy Isles Bridge, CA"
eListMerced <- as.egret(INFO, Daily, NA, NA)
@

<<Merceddata, echo=FALSE,eval=TRUE>>=
filePath <- system.file("extdata", package="EGRET")
fileName <- "eListMerced.RData"

load(paste(filePath,fileName,sep="/"))
@


<<Mercedplot, echo=TRUE,eval=TRUE,fig.cap="Merced River winter trend",fig.subcap=c("Water Year", "December - February"),out.width='.5\\linewidth',out.height='.5\\linewidth',fig.show='hold',fig.pos="h">>=
plotFlowSingle(eListMerced, istat=5)

# Then, we can run the same function, but first set 
# the pa to start in December and only run for 3 months.
eListMerced <- setPA(eListMerced,paStart=12,paLong=3)
plotFlowSingle(eListMerced,istat=5,qMax=200)

@

What these figures show us is that on an annual basis there is very little indication of a long-term trend in mean discharge, but for the winter months there is a pretty strong indication of an upward trend.  This could well be related to the climate warming in the Sierra Nevada, resulting in a general increase in the ratio of rain to snow in the winter and more thawing events.

\newpage
\FloatBarrier
<<plotFour, echo=TRUE, fig.cap="\\texttt{plotFour(eListMerced, qUnit=3)}",fig.show='asis',out.width='1\\linewidth',out.height='1\\linewidth',fig.pos="h">>=
plotFour(eListMerced, qUnit=3)
@

\newpage
\FloatBarrier
<<plotFourStats,echo=TRUE, fig.cap="\\texttt{plotFourStats(eListMerced, qUnit=3)}",fig.show='asis',out.width='1\\linewidth',out.height='1\\linewidth',fig.pos="h",cache=TRUE>>=
plotFourStats(eListMerced, qUnit=3)
@

\newpage
\FloatBarrier

\texttt{plotQTimeDaily} is simply a time series plot of discharge.  But, it is most suited for showing events above some discharge threshold.  In the simplest case, it can plot the entire record, but given the line weight and use of an arithmetic scale it primarily provides a visual focus on the higher values.

The example shown in Figure \ref{fig:MississippiPlot} illustrates a very long record with a long gap of more than 60 years with no discharges above 300,000 cfs, followed by the last followed by the 49 years from 1965 through 2013 with 6 events above that threshold. \texttt{plotQTimeDaily} requires startYear and endYear, along with some other optional arguements (see \texttt{?plotQTimeDaily} for more details).


<<MississippiData, echo=TRUE,eval=FALSE>>=
#Mississippi River at Keokuk Iowa:
siteNumber<-"05474500"
Daily <-readNWISDaily(siteNumber,"00060",startDate="",endDate="")
INFO <- readNWISInfo(siteNumber,"",interactive=FALSE)
INFO$shortName <- "Mississippi River at Keokuk Iowa"
eListMiss <- as.egret(INFO, Daily, NA, NA)
@

<<MissDataRetrieval, echo=FALSE, eval=TRUE>>=
filePath <- system.file("extdata", package="EGRET")
fileName <- "eListMiss.RData"

load(paste(filePath,fileName,sep="/"))

@

<<MississippiPlot, echo=TRUE,eval=TRUE,fig.cap="Mississippi River at Keokuk Iowa",fig.subcap=c("Water Year", "Dec-Feb"),out.width='1\\linewidth',out.height='1\\linewidth',fig.show='hold',fig.pos="h">>=

plotQTimeDaily(eListMiss, qUnit=3,qLower=300)

@


\FloatBarrier

%------------------------------------------------------------ 
\subsection{Table Options}
\label{sec:tableOptions}
%------------------------------------------------------------ 
Sometimes it is easier to consider results in table formats rather than graphically. Similar to the function \texttt{plotFlowSingle}, the \texttt{printSeries} will print the requested discharge statistics (Table \ref{table:istat}), as well as return the results in a data frame. A small sample of the output is printed below.


<<printSeries, eval=FALSE,echo=TRUE>>=
seriesResult <- printSeries(eListMiss, istat=3, qUnit=3)
@

\singlespacing
\begin{verbatim}
Mississippi River at Keokuk Iowa
 Water Year
    30-day minimum
    Thousand Cubic Feet per Second
   year   annual   smoothed
           value    value
   1879     22.6     30.1
   1880     31.7     28.7
   1881     23.0     27.5
...
   2011     51.0     32.4
   2012     34.3     32.1
   2013     16.2     31.8
\end{verbatim}
% \doublespacing


Another way to look at the results is to consider how much the smoothed values change between various pairs of years.  These changes can be represented in four different ways.  
\begin{itemize}
  \item As a change between the first and last year of the pair, expressed in the discharge units selected.
  \item As a change between the first and last year of the pair, expressed as a percentage of the value in the first year
  \item As a slope between the first and last year of the pair, expressed in terms of the discharge units per year.
  \item As a slope between the first and last year of the pair, expressed as a percentage change per year (a percentage based on the value in the first year).
\end{itemize}

Another argument can be very useful in this function: yearPoints.  In the default case, the set of years that are compared are at 5 year intervals along the whole data set.  If the data set was quite long this can be a daunting number of comparisons.  For example, in an 80 year record, there would be 136 such pairs. Instead, we could look at changes between only 3 year points: 1890, 1950, and 2010: 


<<tfc, eval=TRUE,echo=TRUE>>=
tableFlowChange(eListMiss, istat=3, qUnit=3,yearPoints=c(1890,1950,2010))
@

See section \ref{app:createWordTable} for instructions on converting an R data frame to a table in Microsoft\textregistered\ software. Excel, Microsoft, PowerPoint, Windows, and Word are registered trademarks of Microsoft Corporation in the United States and other countries.

\FloatBarrier


%------------------------------------------------------------ 
\section{Summary of Water Quality Data (without using WRTDS)}
\label{sec:wqa}
%------------------------------------------------------------ 
\FloatBarrier

Before you run the WRTDS model, it is helpful to examine the measured water quality data graphically to better understand its behavior, identify possible data errors, and visualize the temporal distribution of the data (identify gaps).  It is always best to clear up these issues before moving forward.

The examples below use the Choptank River at Greensboro, MD. The Choptank River is a small tributary of the Chesapeake Bay. Inorganic nitrogen (nitrate and nitrite) has been measured from 1979 onward. First, we need to load the discharge and nitrate data into R. Before we can graph or use it for WRTDS analysis, we must bring the discharge data into the Sample data frame.  We do this by using the \texttt{mergeReport} function which merges the discharge information and also provides a compact report about some major features of the data set.

<<wrtds1,eval=FALSE,echo=TRUE>>=
#Choptank River at Greensboro, MD:
siteNumber <- "01491000" 
startDate <- "1979-10-01"
endDate <- "2011-09-30"
param<-"00631"
Daily <- readNWISDaily(siteNumber,"00060",startDate,endDate)
INFO<- readNWISInfo(siteNumber,param,interactive=FALSE)
INFO$shortName <- "Choptank River"

Sample <- readNWISSample(siteNumber,param,startDate,endDate)
eList <- mergeReport(INFO, Daily, Sample)
@

<<wrtds2,eval=TRUE,echo=FALSE>>=
siteNumber <- "01491000" #Choptank River at Greensboro, MD
startDate <- "1979-10-01"
endDate <- "2011-09-30"
param<-"00631"
Daily <- getDaily(eList)
Sample <- getSample(eList)
INFO <- getInfo(eList)
eList <- Choptank_eList
@


%------------------------------------------------------------ 
\subsection{Plotting Options}
\label{sec:plotOptionsWQ}
%------------------------------------------------------------ 
\FloatBarrier

This section shows examples of the available plots appropriate for analyzing data prior to performing a WRTDS analysis. The plots here use the default variable input options.  For any function, you can get a complete list of input variables in a help file by typing a ? before the function name in the R console. See section \ref{sec:wqVariables} for information on the available input variables for these plotting functions.

Note that for any of the plotting functions that show the sample data, if a value in the data set is a non-detect (censored), it is displayed on the graph as a vertical line.  The top of the line is the reporting limit and the bottom is either zero, or if the graph is plotting log concentration values the minimum value on the y-axis.  This line is an \enquote{honest} representation of what we know about about that observation and doesn't attempt to use a statistical model to make an estimate below the reporting limit. 

\newpage
\FloatBarrier
<<plotBoxes, echo=TRUE, fig.cap="Concentration box plots",fig.subcap=c("\\texttt{boxConcMonth(eList)}","\\texttt{boxQTwice(eList, qUnit=1)}"),out.width='.5\\linewidth',out.height='.5\\linewidth',fig.show='hold',fig.pos="h">>=
boxConcMonth(eList)
boxQTwice(eList,qUnit=1)
@

Note that the statistics to create the boxplot in \texttt{boxQTwice} are performed after the data are log-transformed.

\newpage
\FloatBarrier
<<plotConcTime,echo=TRUE, fig.cap="The relation of concentration vs time or discharge",fig.subcap=c("\\texttt{plotConcTime(eList)}","\\texttt{plotConcQ(eList)}"),out.width='.5\\linewidth',out.height='.5\\linewidth',fig.show='hold',fig.pos="h">>=
plotConcTime(eList)
plotConcQ(eList, qUnit=1)
@

It is interesting to note in Figure \ref{fig:plotConcTime} the change in the convention for rounding of data values that occurred around 1995.

\newpage
\FloatBarrier
<<plotFluxQ,echo=TRUE, fig.cap="The relation of flux vs discharge",out.width='.5\\linewidth',out.height='.5\\linewidth',fig.show='hold',fig.pos="h">>=
plotFluxQ(eList, fluxUnit=4)
@

The \texttt{plotFluxQ} (Figure \ref{fig:plotFluxQ}) function only plots in a log-log scale.

\newpage
\FloatBarrier
<<multiPlotDataOverview, echo=TRUE, fig.cap="\\texttt{multiPlotDataOverview(eList, qUnit=1)}",fig.show='asis',out.width='1\\linewidth',out.height='1\\linewidth',fig.pos="h">>=
multiPlotDataOverview(eList, qUnit=1)
@

The \texttt{multiPlotDataOverview} (Figure \ref{fig:multiPlotDataOverview}) function uses a log scale as default. To change the concentration axes to an arithmetic scale, use logScaleConc=FALSE in the \texttt{multiPlotDataOverview} function call.

\FloatBarrier

%------------------------------------------------------------ 
\subsection{Table Options}
\label{sec:tableOptionsWQ}
%------------------------------------------------------------ 
Another useful tool for checking the data before running the WRTDS estimations is \texttt{flowDuration}. This is a utility function that can help you define the discharge ranges that we want to explore.  It prints out key points on the discharge duration curve.  Define the points for a particular part of the year using the \texttt{"}centerDate\texttt{"} and \texttt{"}span\texttt{"} arguments, although the points can be defined for the entire year (default).  

<<flowDuration, eval=TRUE, echo=TRUE>>=
flowDuration(eList, qUnit=1)

flowDuration(eList, qUnit=1, centerDate="09-30", span=30)
@

\FloatBarrier
%------------------------------------------------------------ 
\section{Weighted Regressions on Time, Discharge and Season (WRTDS)}
\label{sec:wrtds}
%------------------------------------------------------------ 
WRTDS creates a model of the behavior of concentration as a function of three components: time trend, discharge, and season.  You can use WRTDS to estimate annual or seasonal mean concentrations and fluxes as well as describe long-term trends in the behavior of the system. In this section, we will step though the process required for a WRTDS analysis. Section (\ref{sec:wrtdsResults}) provides details about the available methods for viewing and evaluating the model results. 

Once you have looked at your data using the tools described in section \ref{sec:wqa}, and have determined there are sufficient representative data, it is time to run the WRTDS model. Assuming you are using the defaults, with data frames called Daily, Sample, and INFO, the \texttt{modelEstimation} function runs the WRTDS modeling algorithm:

<<wrtds3, eval=FALSE, echo=TRUE>>=
eList <- modelEstimation(eList)
@

Details of the options available when running \texttt{modelEstimation} can be found in Section \ref{sec:wrtdsInputVariables}. This function is slow, and shows the progress in percent complete. See the references and manual for more information. It's important to understand that this is the one function that will globally change your Daily, Sample, and INFO data frames. It also creates a new matrix \texttt{"}surfaces\texttt{"}, and a new data frame \texttt{"}AnnualResults\texttt{"}. It is unusual R programming behavior to create global variables, but global variables were chosen to make it easy for you.

Finally, it is a good idea to save your results because of the computational time that has been invested in producing these results. The workspace is saved to a directory that you designate savePath and the file name is determined by the abbreviations for station and constituent that were required entries when the \texttt{readNWISInfo} function was used. The command for saving the workspace is:

<<wrtds5, eval=FALSE, echo=TRUE>>=
#An example directory name
savePath <- "C:/Users/egretUser/WRTDS_Output/" 
saveResults(savePath, eList) 
@

This saves all of the objects in your workspace. If you have saved workspaces from R versions earlier than 3.0, a warning will appear when you open them in R 3.0 (or later). Re-saving the workspace using R 3.0 (or later) should get rid of the warning.

Using \texttt{saveResults}, the workspace is saved with INFO\$staAbbrev and INFO\$constitAbbrev as the filename (separated by a period), and the extension .RData. So, if staAbbrev was \enquote{Chop} and the constitAbbrev was \enquote{NO3} the file name would be \enquote{Chop.NO3.RData}. To load the data in some future session the commands could be:

<<wrtds8, eval=FALSE, echo=TRUE>>=
loadPath <- "C:/Users/egretUser/WRTDS_Output/"
staAbbrev <- "Chop"
constitAbbrev <- "NO3"
pathToFile <- paste0(loadPath,staAbbrev,".",
                    constitAbbrev,".RData")
load(pathToFile) 
@



\FloatBarrier

%------------------------------------------------------------ 
\section{WRTDS Results}
\label{sec:wrtdsResults}
%------------------------------------------------------------ 
At this point (after having run \texttt{modelEstimation}) we can start considering how to view the annual averages for the variables that have been calculated.  See section \ref{sec:wrtdsOutputVariables} for common input variables for these functions. Additionally, check the help files (in the R console, type ? followed by the function name). 

%------------------------------------------------------------ 
\subsection{Plotting Options}
\label{sec:wrtdsPlotting}
%------------------------------------------------------------ 

\FloatBarrier

Check the help files or manual for more details on the following functions.  See section \ref{app:savingPlots} for information on saving plots. In these examples, we will return to looking at the data in the water year by using the \texttt{setPA} function. Most plotting functions will use the period of analysis information in the INFO data frame to determine what data are plotted. There are only four graph or table functions that don't allow the user to specify a Period of Analysis (PA). These are: \texttt{plotContour}, \texttt{plotDiffContour}, \texttt{plotConcTimeSmooth}, \texttt{plotConcQSmooth}.


<<getChopData1,echo=FALSE,eval=TRUE>>=
# Sample <- getSample(eList)
# Daily <- getDaily(eList)
# INFO <- getInfo(eList)
# surfaces <- getSurfaces(eList)
eList <- Choptank_eList
@

\clearpage
<<plotConcTimeDaily, echo=TRUE, fig.cap="Concentration and flux vs time",fig.subcap=c("\\texttt{plotConcTimeDaily(2008, 2010)}","\\texttt{plotFluxTimeDaily(2008, 2010)}"),out.width='.5\\linewidth',out.height='.5\\linewidth',fig.show='hold',fig.pos="h">>=
# Return to water year:
eList <- setPA(eList)

yearStart <- 2008
yearEnd <- 2010

plotConcTimeDaily(eList, yearStart, yearEnd)
plotFluxTimeDaily(eList, yearStart, yearEnd)
@

\clearpage
<<plotFluxPred, echo=TRUE, fig.cap="Concentration and flux predictions",fig.subcap=c('\\texttt{plotConcPred(eList)}','\\texttt{plotFluxPred(eList)}'),out.width='.5\\linewidth',out.height='.5\\linewidth',fig.show='hold',fig.pos="h">>=
plotConcPred(eList)
plotFluxPred(eList)
@

\clearpage
<<plotResidQ, echo=TRUE, fig.cap="Residuals",fig.subcap=c("\\texttt{plotResidPred(eList)}","\\texttt{plotResidQ(eList, qUnit=1)}"),out.width='.5\\linewidth',out.height='.5\\linewidth',fig.show='hold',fig.pos="h">>=
plotResidPred(eList)
plotResidQ(eList, qUnit=1)
@

\clearpage
<<boxResidMonth, echo=TRUE, fig.cap="Residuals with respect to time",fig.subcap=c("\\texttt{plotResidTime(eList)}","\\texttt{boxResidMonth(eList)}"),out.width='.5\\linewidth',out.height='.5\\linewidth',fig.show='hold',fig.pos="h">>=
plotResidTime(eList)
boxResidMonth(eList)
@

\clearpage
<<boxConcThree, echo=TRUE, fig.cap="Default \\texttt{boxConcThree(eList)}",out.width='.5\\linewidth',out.height='.5\\linewidth',fig.show='asis',results='hide',fig.pos="h">>=
boxConcThree(eList)
@

\clearpage
<<plotFluxHist, echo=TRUE, fig.cap="Concentration and flux history",fig.subcap=c("\\texttt{plotConcHist(eList)}", "\\texttt{plotFluxHist(eList)}"),out.width='.5\\linewidth',out.height='.5\\linewidth',fig.show='hold',fig.pos="h">>=
plotConcHist(eList)
plotFluxHist(eList)
@

\clearpage

Figures \ref{fig:plotConcQSmooth} and \ref{fig:plotConcTimeSmooth} contain legends. The placement of the legend is controlled by legendLeft and legendTop. If both are set to 0 (the default values), the legend is placed near the lower left corner of the graphic. Otherwise, the value specified for legendLeft places the left edge of the legend, and legendTop specifies the top edge of the legend. The units for legendLeft and legendTop are discharge (in units specified by qUnit) and concentration, respectively. The legend can also be turned off with printLegend=FALSE. These are also functions that do not recognize the period of analysis in the INFO data frame. However, by choosing centering dates and appropriate half-windows, seasonal behavior can easily be observed in these plots. 

<<plotConcQSmooth, echo=TRUE, fig.cap="Concentration vs. discharge",fig.subcap=c("\\texttt{plotConcQSmooth}","\\texttt{plotConcQSmooth(logScale=TRUE)}"),out.width='.5\\linewidth',out.height='.5\\linewidth',fig.show='hold',fig.pos="h">>=
qLow <- 20
qHigh <- 700
date1 <- "2000-09-01"
date2 <- "2005-09-01"
date3 <- "2009-09-01"
plotConcQSmooth(eList, date1, date2, date3,
                qLow, qHigh, qUnit=1)
plotConcQSmooth(eList, date1, date2, date3,
                qLow, qHigh, qUnit=1,logScale=TRUE)
@

\clearpage
<<plotConcTimeSmooth, echo=TRUE, fig.cap="\\texttt{plotConcTimeSmooth(eList))}",fig.subcap=c("\\texttt{plotConcTimeSmooth}","\\texttt{plotConcTimeSmooth(logScale=TRUE)}"),out.width='.5\\linewidth',out.height='.5\\linewidth',fig.show='hold',results='hide',fig.pos="h">>=
q1 <- 10
q2 <- 25
q3 <- 75
centerDate <- "07-01"
plotConcTimeSmooth(eList, q1, q2, q3, centerDate, 2000, 2010)
plotConcTimeSmooth(eList, q1, q2, q3, centerDate, 
                   2000, 2010,logScale=TRUE)
@

Figure \ref{fig:fluxBiasMulti} shows a predefined multipanel graph using \texttt{fluxBiasMulti}.

<<fluxBiasMulti, echo=TRUE, fig.cap="\\texttt{fluxBiasMulti(eList, qUnit=1)}",fig.show='asis',fig.width=8, fig.height=10,fig.pos="h">>=
fluxBiasMulti(eList, qUnit=1)
@

\clearpage

The contour plot functions also do not recognize the PA from the INFO data frame. They represent the overall results of the WRTDS analysis. To specify contourLevels in the contour plots use the \texttt{seq} function (type \texttt{?seq} for details).  In general, use of the \texttt{seq} function would look like this: \texttt{contourLevels = seq(from,to,by)}.  In the example shown above we are requesting contour levels that run from 0 to 2 in steps of 0.2.


<<plotContours, echo=TRUE,fig.cap="\\texttt{plotContours(eList)}",fig.show='asis',out.width='1\\linewidth',out.height='1\\linewidth',fig.pos="h">>=
clevel<-seq(0,2,0.2)
plotContours(eList, yearStart=2008,yearEnd=2010,qBottom=20,qTop=1000, 
             contourLevels = clevel,qUnit=1)
@

\clearpage

The function \texttt{plotDiffContours} plots the difference between two selected years (year0 and year1). It can help clarify what combinations of seasons and flow conditions have been showing increases and decreases over the period covered.


<<plotDiffContours, echo=TRUE, fig.cap="\\texttt{plotDiffContours(eList)}",fig.show='asis',out.width='1\\linewidth',out.height='1\\linewidth',fig.pos="h">>=
plotDiffContours(eList, year0=2000,year1=2010,
                 qBottom=20,qTop=1000,maxDiff=0.6,qUnit=1)
@

\clearpage

%------------------------------------------------------------ 
\subsection{Table Options}
\label{sec:wrtdsTable}
%------------------------------------------------------------ 
Sometimes it is easier to consider the results in table form rather than graphically. The function \texttt{tableResults} produces a simple text table that contains the annual values for the results.  Each row of the output represents a year and includes: year, average discharge, average concentration, flow-normalized concentration, average flux, and flow-normalized flux.  A small sample of the output is printed below. 

<<tableResults1, echo=TRUE, eval=FALSE>>=
tableResults(eList)
returnDF <- tableResults(eList)
@

\singlespacing
\begin{verbatim}
   Choptank River 
   Inorganic nitrogen (nitrate and nitrite)
   Water Year 

   Year   Discharge    Conc    FN_Conc     Flux    FN_Flux
             cms            mg/L             10^6 kg/yr 
   1980      4.25     0.949     1.003    0.1154     0.106
   1981      2.22     1.035     0.999    0.0675     0.108
...
   2010      7.19     1.323     1.438    0.2236     0.149
   2011      5.24     1.438     1.457    0.1554     0.148
\end{verbatim}
% \doublespacing

<<tableResults2, echo=FALSE, eval=TRUE,results='hide'>>=
returnDF <- tableResults(eList)
@

<<tableResultshead, echo=FALSE, results='asis'>>=
print(xtable(head(returnDF),
       label="table:tableChangeHead",
       caption="Table created from \\texttt{head(returnDF)}",
       digits=c(0,0,2,3,3,3,3)),
       caption.placement="top",
       size = "\\footnotesize",
       latex.environment=NULL,
       sanitize.text.function = function(x) {x},
       sanitize.colnames.function =  bold.colHeaders,
       sanitize.rownames.function = addSpace
      )
@


The other table option is \texttt{tableChange}. This is a function that provides for the computation of changes or slopes between any selected pairs of time points.  These computations are made only on the flow-normalized results. A detailed explaination of \enquote{flow-normalized} result is in the official EGRET user guide.


<<tableChange1, eval=TRUE, echo=TRUE>>=
tableChange(eList, yearPoints=c(2000,2005,2010))
@

Finally, \texttt{tableChangeSingle} (Table \ref{table:tableChangeSingle}) operates exactly the same as \texttt{tableChange} except for the addition argument flux. This function provides either concentration results or flux results, but not both.  This can be useful when you are producing many output tables for a report that is entirely focused on concentration or one that is entirely focused on flux.  The arguments are identical to those for tableChange, except for the final two arguments.  The argument \texttt{"}flux\texttt{"} defaults to TRUE.  When flux=TRUE the output is only for flux, and when flux=FALSE the output is only for concentration.  See section \ref{app:createWordTable} for instructions on converting an R data frame to a table in Microsoft\textregistered\ software.

<<tableChangeSingleR, eval=TRUE, echo=TRUE,results='hide'>>=
returnDF <- tableChangeSingle(eList, yearPoints=c(2000,2005,2010))
@

<<tableResultsShow, echo=FALSE, results='asis'>>=
print(xtable(returnDF,
       label="tableChangeSingle",
       caption="Table created from \\texttt{tableChangeSingle} function",
       digits=c(0,0,0,3,2,1,1)),
       caption.placement="top",
       size = "\\footnotesize",
       latex.environment=NULL,
       sanitize.text.function = function(x) {x},
       sanitize.colnames.function =  bold.colHeaders,
       sanitize.rownames.function = addSpace
      )
@

\clearpage


%------------------------------------------------------------ 
\section{Extending Plots Past Defaults}
\label{sec:extendedPlots}
%------------------------------------------------------------ 

The basic plotting options were shown in the section \ref{sec:wrtdsResults}.  This section demonstrates some ways to extend the capabilities of the EGRET plots. EGRET plots use R's basic plotting options. You set many of the formatting details of plotting routines in R by using \enquote{Graphical Parameters}.  To read about all of these graphical parameters see \texttt{?par}.  When the graphical functions in EGRET are coded, a set of default values for many of these parameters are chosen, but you can override all of these default values. Additionally, you can add features to a plot after calling the plot function. To change the plot margins (mar), font, or other graphical parameters initially assigned, set the argument customPar to TRUE.

A few of R's base graphical parameters are especially useful within the plot functions. These are shown in Table \ref{table:tableChangeSingle}.

\begin{table}[ht]
{\footnotesize
\begin{tabular}{rrr}
  \hline
\multicolumn{1}{c}{\textbf{\textsf{Argument}}} &
\multicolumn{1}{c}{\textbf{\textsf{Description}}} &
\multicolumn{1}{c}{\textbf{\textsf{Values}}}  \\ 
  \hline
cex &  Size of data point symbols, relative to default & decimal number \\ 
[5pt]cex.main & Size of font for plot title, relative to default & decimal number \\ 
[5pt]cex.lab &  Size of font for axis label text, relative to default & decimal number \\ 
[5pt]cex.axis & Size of font for axis annotation (numbers), relative to default & decimal number\\
[5pt]col & Color of data point symbols or lines & color name in \texttt{"}\texttt{"} \\
[5pt]lwd & Width of lines, relative to default & decimal number\\
[5pt]pch & Type of symbol to use for data points & integer values\\
[5pt]lty & Line type number (such as dash or dot) & integer values\\
   \hline
\end{tabular}
\caption{Useful plotting parameters to adjust in EGRET plotting functions.  For details of any of these see ?par.} 
\label{table:tableChangeSingle}
}
\end{table}

After the plot is made, many other functions that might be useful to call, such as to add text, legend, lines, etc. Table \ref{table:addOns} lists a few common options.

\begin{table}[ht]
{\footnotesize
\begin{tabular}{rr}
  \hline
\multicolumn{1}{c}{\textbf{\textsf{Function}}} &
\multicolumn{1}{c}{\textbf{\textsf{Description}}}  \\ 
  \hline
mtext & add text based on specified side of plot\\
[5pt]text & add text to a specific point on plot\\
[5pt]legend & add a legend \\ 
[5pt]grid & add grid\\ 
[5pt]abline & add line \\
[5pt]arrows & add arrow \\ 
   \hline
\end{tabular}
\caption{Useful functions to add on to default plots. Type ? then the function name to get help on the individual function.} 
\label{table:addOns}
}
\end{table}

Some basic examples are shown below.

Figure \ref{fig:adjustSize} shows a larger title and axis number (left), and larger axis labels and point size (right).
<<adjustSize,echo=TRUE,eval=TRUE,fig.cap="Modifying text and point size, as shown using the \\texttt{plotConcQ} function", fig.subcap=c("\\texttt{(cex.axis=2,cex.main=1.5)}","\\texttt{(cex.lab=2,cex=2)}"),out.width='.5\\linewidth',out.height='.5\\linewidth',fig.show='hold',fig.pos="h">>=
plotConcQ(eList, cex.axis=2,cex.main=1.5,logScale=TRUE)
plotConcQ(eList, cex.lab=2,cex=2,logScale=TRUE)
@

\clearpage

Figure \ref{fig:plotConcQComparison} shows the default on the left, and several features on the right. First, the margin is adjusted to c(8,8,8,8), requiring customPar set to TRUE. The margin vector represents the margin spacing of the 4 \texttt{"}sides\texttt{"} of a plot in the order: bottom, left, top, right. Next, the text labels were adjusted, color set to \texttt{"}blue\texttt{"}, point and line size increased, and the point type changed form a solid circle(pch=20) to solid diamond (pch=18). A grid, legend, arrow, and text are added after the plot is produced.
<<plotConcQComparison,echo=TRUE,eval=TRUE,fig.cap="Modified \\texttt{plotConcQ}", fig.subcap=c("Default","Modified"),out.width='.5\\linewidth',out.height='.5\\linewidth',fig.show='hold',fig.pos="h">>=
plotConcQ(eList, logScale=TRUE)
par(mar=c(8,8,8,8))
plotConcQ(eList, customPar=TRUE,col="blue",cex=1.1,
          cex.axis=1.4,cex.main=1.5,cex.lab=1.2,
          pch=18,lwd=2,logScale=TRUE)
grid(lwd=2)
legend(4.5,.09,"Choptank Nitrogen", pch=18, col="blue",bg="white")
arrows(3, 0.14, 1, .05,lwd=2)
text(12,.14,"Censored Value")
@

\clearpage

Only a few fonts are consistent on all operating systems. Figure \ref{fig:easyFontChange} shows how to change to the Serif font, as well as how to use the mtext function. To see the available fonts for pdf output on your computer, type \texttt{names(pdfFonts())}.The available fonts are quite limited in base R. To expand the font choices, a useful R library, \enquote{extrafont} can help.

<<easyFontChange,echo=TRUE,eval=TRUE,fig.cap="Serif font",fig.show='asis',out.width='1\\linewidth',out.height='1\\linewidth', fig.pos="h">>=
# Switching to serif font:
par(family="serif")
plotFluxPred(eList, customPar=TRUE)
mtext(side=3,line=-3,"Serif font example",cex=3)
@

\clearpage

You can also extend the contour plots. The default y-axis is determined from qTop and qBottom. Occasionally, you may need to use a custom axis by specifying yTicks. It is also nice to be able to adjust the color scheme of the contour plots. There are some color schemes built into base R such as heat.colors, topo.colors, terrain.colors, and cm.colors. Alternatively, you can set colors by using the \texttt{colorRampPalette} function. For example, a black and white color scheme might be required. In another example, the \texttt{plotDiffContours} might make more sense to go from yellow to white for the negative values, and white to blue for the positive values. Examples are shown below for modifying a contour plot in Figure \ref{fig:modifiedContour1} and modifying a difference contour plot in Figure \ref{fig:modifiedDiffContour}. 

\clearpage

<<modifiedContour1,echo=TRUE,eval=TRUE,fig.cap="Contour plot with modified axis and color scheme",fig.show='hold',out.width='1\\linewidth',out.height='1\\linewidth',fig.pos="h">>=
colors <- colorRampPalette(c("white","black"))
yTicksModified <- c(.5,1,10,25)
plotContours(eList, 2001,2010,0.5,50, 
             contourLevels = seq(0,2.5,0.5),qUnit=2,
             yTicks=yTicksModified,
             color.palette=colors,
             flowDuration=FALSE,
             tcl=0.2,tick.lwd=2.5)  
@

\clearpage

<<modifiedDiffContour,echo=TRUE,eval=TRUE,fig.cap="Difference contour plot with modified color scheme",fig.show='asis',out.width='1\\linewidth',out.height='1\\linewidth',fig.pos="h">>=
colors <- colorRampPalette(c("yellow","white","blue"))
maxDiff<-0.6
par(oma=c(1,1,1,1))
plotDiffContours(eList, year0=2001,year1=2010,qBottom=0.5,qTop=50, 
             maxDiff,lwd=2,qUnit=2,
             color.palette=colors,
             flowDuration=FALSE, customPar=TRUE)
@


\clearpage

It is also possible to create custom multi-panel plots. In the simplest example (figure \ref{fig:tinyPlot1}), you can use the \texttt{"tinyPlot=TRUE"} option.

<<tinyPlot1,echo=TRUE,eval=TRUE,fig.cap="Custom multipanel plot using tinyPlot",fig.show='asis',out.width='1\\linewidth',out.height='1\\linewidth',fig.pos="h">>=
par(mfcol = c(2, 2), oma = c(0, 1.7, 6, 1.7))

plotFluxQ(eList, tinyPlot=TRUE,printTitle=FALSE,
          fluxUnit=9,logScale=TRUE,fluxMax=1)
plotConcQ(eList, tinyPlot=TRUE,printTitle=FALSE)
plotFluxHist(eList, tinyPlot=TRUE,printTitle=FALSE,fluxMax=1)
plotConcHist(eList, tinyPlot=TRUE,printTitle=FALSE,concMax=3)
mtext("Custom multi-pane graph using tinyPlot=TRUE", outer=TRUE, font=2)
@

\clearpage

Finally, figure \ref{fig:customPanel} shows a method to create a panel of plots with a finer control.


<<customPanel,echo=TRUE,eval=TRUE,fig.cap="Custom multipanel plot",fig.show='asis',out.width='1\\linewidth',out.height='1\\linewidth',fig.pos="h">>=
par(mar=c(3.5,3.5,0.2,0.2), # whitespace around the plots
    oma=c(1,1,3,1), # outer margin
    mgp=c(2,0.5,0), # spacing between the label numbers and plots
    mfcol = c(2,2)) # rows/columns

plotFluxQ(eList, tinyPlot=TRUE,printTitle=FALSE,
          fluxUnit=9,logScale=TRUE,fluxMax=1,
          showXLabels=FALSE,showXAxis=FALSE, 
          showYLabels=TRUE,customPar=TRUE)

plotConcQ(eList, tinyPlot=TRUE,printTitle=FALSE, customPar=TRUE,
          removeLastY=TRUE,removeLastX=TRUE,
          showYLabels=TRUE)

plotFluxHist(eList, tinyPlot=TRUE,printTitle=FALSE,fluxMax=1,
          showYLabels=FALSE,showYAxis=FALSE,
          showXLabels=FALSE,showXAxis=FALSE, customPar=TRUE)
plotConcHist(eList, tinyPlot=TRUE,printTitle=FALSE,concMax=3,
          showYLabels=FALSE, showYAxis=FALSE, customPar=TRUE)
mtext("Custom multi-pane graph using customPar", outer=TRUE, font=2)
@


\clearpage

%------------------------------------------------------------ 
\section{Getting Started in R}
\label{sec:appendix1}
%------------------------------------------------------------ 
This section describes the options for installing the EGRET package.

%------------------------------------------------------------
\subsection{New to R?}
%------------------------------------------------------------ 
If you are new to R, you will need to first install the latest version of R, which can be found here: \url{http://www.r-project.org/}.

At any time, you can get information about any function in R by typing a question mark before the function's name.  This opens a file that describes the function, the required arguments, and provides working examples.

<<helpFunc,eval = FALSE>>=
?plotConcQ
@

To see the raw code for a particular function, type the name of the function, without parentheses:
<<rawFunc,eval = FALSE>>=
plotConcQ
@


%------------------------------------------------------------
\subsection{R User: Installing EGRET}
%------------------------------------------------------------ 
To install the EGRET packages and its dependencies:

<<installFromCran,eval = FALSE>>=
install.packages("EGRET")
@

It is a good idea to re-start R after installing the package if installing an updated version. 

After installing the package, you need to open the library each time you re-start R.  This is done with the simple command:
<<openLibraryTest, eval=FALSE>>=
library(EGRET)
@

\newpage
\FloatBarrier
%------------------------------------------------------------ 
\section{Common Function Variables}
\label{sec:appendixPlot}
%------------------------------------------------------------ 
This section describes variables that are common for a variety of function types. 

%------------------------------------------------------------ 
\subsection{Flow History Plotting Input}
\label{sec:flowHistoryVariables}
%------------------------------------------------------------
\begin{table}[ht]
{\footnotesize
  \begin{threeparttable}[b]
\caption{Variables used in flow history plots (\texttt{plot15}, \texttt{plotFour}, \texttt{plotFourStats}, \texttt{plotQTimeDaily}, \texttt{plotSDLogQ}) 
\label{tab:flowHistoryVariables}}
\begin{tabularx}{\textwidth}{lXl}
\hline
\multicolumn{1}{c}{\textbf{\textsf{Argument}}} &
\multicolumn{1}{c}{\textbf{\textsf{Definition}}} &
\multicolumn{1}{c}{\textbf{\textsf{Default}}} \\
\hline
istat & The discharge statistic to be plotted: 1-8.  Must be specified, see Table \ref{table:istat}. & \\
yearStart\tnote{1} & The decimal year (decYear) value where you want the graph to start & NA\\
yearEnd\tnote{1} & The decimal year (decYear) value where you want the graph to end & NA\\
qMax & User specified upper limit on y axis (can be used when we want several graphs to all share the same scale). Value is specified in the discharge units that the user selects. & NA\\
printTitle & can be TRUE or FALSE, you may want FALSE if it is going to be a figure with a caption or if it is a part of a multipanel plot. & TRUE\\
tinyPlot & Can be TRUE or FALSE, the TRUE option assures that there will be a small number of tick marks, consistent with printing in a small space & FALSE\\
runoff & Can be TRUE or FALSE.  If true then discharge values are reported as runoff in mm/day.  This can be very useful in multi-site analyses. & FALSE\\
qUnit & An index indicating what discharge units to use.  Options run from 1 to 6 (see section \ref{sec:units}).  The choice should be based on the units that are customary for the audience but also, the choice should be made so that the discharge values don't have too many digits to the right or left of the decimal point. & 1\\
printStaName\tnote{2} & Can be TRUE or FALSE, if TRUE the name of the streamgage is stated in the plot title. & TRUE\\
printPA\tnote{2} & Can be TRUE or FALSE, if TRUE the period of analysis is stated in the plot title. & TRUE\\
printIstat\tnote{2} & Can be TRUE or FALSE, if TRUE the name of the statistic (e.g. 7-day minimum discharge) is stated in the plot title. & TRUE\\

\hline
\end{tabularx}
  \begin{tablenotes}
    \item[1] Setting yearStart and yearEnd will determine where the graphs start and end, but they don't determine where the smoothing analysis starts and ends.  There are situations, typically where many sites are be analyzed together, where you may want to run the smoothing on a consistent period of record across all sites, which requires subsetting the Daily data frame before running \texttt{makeAnnualSeries} (see \texttt{?subset}).
    \item[2] If the printTitle argument is set to FALSE, then it really makes no difference what you do with printSta, printPA, or printIstat.  They can all be left as their default values and thus there is no need to include them in the call for the function.
  \end{tablenotes}
 \end{threeparttable}
}
\end{table}

\FloatBarrier
\clearpage

%------------------------------------------------------------ 
\subsection{Water Quality Plotting Input}
\label{sec:wqVariables}
%------------------------------------------------------------

\begin{table}[ht]
{\footnotesize
\caption{Selected variables used in water quality analysis plots  \label{tab:wqVariables}}
\begin{tabularx}{\textwidth}{lXl}
\hline
\multicolumn{1}{c}{\textbf{\textsf{Argument}}} &
\multicolumn{1}{c}{\textbf{\textsf{Definition}}} &
\multicolumn{1}{c}{\textbf{\textsf{Default}}} \\
\hline
qUnit & Determines what units will be used for discharge, see section \ref{sec:units} & 2\\
printTitle & If TRUE the plot has a title.  If FALSE no title (useful for publications where there will be a caption) & TRUE\\
qLower & The lower bound on the discharge on the day of sampling that will be used in forming a subset of the sample data set that will be displayed in the graph.  It is expressed in the units specified in qUnit.  If qLower = NA, then the lower bound is set to zero. & \\
qUpper & The upper bound on the discharge on the day of sampling that will be used in forming a subset of the sample data set that will be displayed in the graph.  It is expressed in the units specified in qUnit.  If qUpper = NA, then the upper bound is set to infinity. & \\
% paLong & The length of the time period that will be used in forming a subset of the sample data set that will be displayed in the graph, expressed in months. & 12\\ 
% paStart & The starting month for the time period that will be used in forming a subset of the sample data set that will be displayed in the graph.  It is expressed in months (calendar months). & 10\\
concMax & The upper limit on the vertical axis of graphs showing concentration values in mg/L (NA sets value to just above maximum).  & NA\\
concMin & The lower limit on the vertical axis of graphs showing concentration values in mg/L (NA sets value to just below minimum for log scales, zero for linear). & NA\\
fluxUnit & Determines what units will be used for flux (see Section \ref{sec:units}). & 9\\
fluxMax & The upper limit on the vertical axis of graphs showing flux values. & \\
\hline
\end{tabularx}
}
\end{table}

\FloatBarrier
\clearpage


%------------------------------------------------------------ 
\subsection{WRTDS Estimation Input}
\label{sec:wrtdsInputVariables}
%------------------------------------------------------------
\begin{table}[ht]
{\footnotesize
\caption{Selected variables in WRTDS  \label{tab:WRTDS}}
\begin{tabularx}{\textwidth}{lXl}
\hline
\multicolumn{1}{c}{\textbf{\textsf{Argument}}} &
\multicolumn{1}{c}{\textbf{\textsf{Definition}}} &
\multicolumn{1}{c}{\textbf{\textsf{Default}}} \\
\hline
windowY & The half window width for the time weighting, measured in years.  Values much shorter than 7 usually result in a good deal of oscillations in the system that are likely not very realistic & 7\\
windowQ & The half window width for the weighting in terms of ln(Q).  For very large rivers (average discharge values in the range of many tens of thousands of cfs) a smaller value than 2 may be appropriate, but probably not less than 1 & 2 \\
windowS & The half window width for the seasonal weighting, measured in years.  Any value \texttt{>}0.5 will make data from all seasons have some weight.  Values should probably not be lower than 0.3 & 0.5 \\
minNumObs & This is the minimum number of observations with non-zero weight that the individual regressions will require before they will be used.  If there too few observations the program will iterate, making the windows wider until the number increases above this minimum.  The only reason to lower this is in cases where the data set is rather small.  It should always be set to a number at least slightly smaller than the sample size.  Any value less than about 60 is probably in the \enquote{dangerous} range, in terms of the reliability of the regression & 100 \\ 
minNumUncen & This is the minimum number of uncensored observations with non-zero weight that the individual regressions will require before they will be used.  If there are too few uncensored observations the program will iterate, making the windows wider until the number increases above this minimum.  The only reason to lower this is in cases where the number of uncensored values is rather small.  The method has never been tested in situations where there are very few uncensored values & 50 \\
edgeAdjust & Specify whether to use the modified method for calculating the windows at the edge of the record.  The modified method tends to reduce curvature near the start and end of record & TRUE \\
\hline
\end{tabularx}
}
\end{table}

\FloatBarrier
\clearpage

%------------------------------------------------------------ 
\subsection{Post-WRTDS Plotting Input}
\label{sec:wrtdsOutputVariables}
%------------------------------------------------------------

\begin{table}[ht]
{\footnotesize
\caption{Selected variables used in plots for analysis of WRTDS model results 
\label{tab:wrtdsVariables}}
\begin{tabularx}{\textwidth}{lXl}
\hline
\multicolumn{1}{c}{\textbf{\textsf{Argument}}} &
\multicolumn{1}{c}{\textbf{\textsf{Definition}}} &
\multicolumn{1}{c}{\textbf{\textsf{Default}}} \\
\hline
qUnit & Determines what units will be used for discharge, see section \ref{sec:units} & 2\\
fluxUnit & An index indicating what flux units will be used , see section \ref{sec:units} & 3\\
stdResid & This is an option.  If FALSE, it prints the regular residuals (they are in ln concentration units).  If TRUE, it is the standardized residuals.  These are the residuals divided by their estimated standard error (each residual has its own unique standard error).  In theory, the standardized residuals should have mean zero and standard deviation of 1 & FALSE\\
printTitle & If TRUE the plot has a title.  If FALSE no title (useful for publications where there will be a caption) & TRUE\\
startYear & The starting date for the graph, expressed as decimal years, for example, 1989 & NA\\
endYear & The ending date for the graph, expressed as decimal years, for example, 1996 & NA\\
moreTitle & A character variable that adds additional information to the graphic title.  Typically used to indicate the estimation method.\\
fluxMax & The upper limit on the vertical axis of graphs showing flux values. & NA\\
concMax & The upper limit on the vertical axis of graphs showing concentration values. & NA\\
plotFlowNorm & If TRUE the graph shows the annual values as circles and the flow-normalized values as a green curve.  If false, it only shows the annual values. & TRUE\\
\hline
\end{tabularx}
}
\end{table}


\begin{table}[ht]
{\footnotesize
\caption{Variables used in EGRET contour plots: \texttt{plotContours} and \texttt{plotDiffContours} \label{tab:wrtdsContourVariables}}
\begin{tabularx}{\textwidth}{lXl}
\hline
\multicolumn{1}{c}{\textbf{\textsf{Argument}}} &
\multicolumn{1}{c}{\textbf{\textsf{Definition}}} &
\multicolumn{1}{c}{\textbf{\textsf{Defaults}}}\\
\hline
qUnit & Determines what units will be used for discharge, see section \ref{sec:units} & 2\\
qBottom & The lower limit of the discharge value for the graphs in the units specified by qUnit &\\
qTop & The upper limit of the discharge value for the graphs in the units specified by qUnit &\\
printTitle & If TRUE the plot has a title.  If FALSE no title (useful for publications where there will be a caption) & TRUE \\
yearStart & The starting date for the graph, expressed as decimal years, for example, 1989 & \\
yearEnd & The ending date for the graph, expressed as decimal years, for example, 1996 & \\
whatSurface & This should generally be at its default value.  At whatSurface = 3, the plotted surface shows the expected value of concentration.  For whatSurface = 1, it shows the yHat surface (natural log of concentration).  For whatSurface = 2, it shows the SE surface (the standard error in log concentration). & 3\\
contourLevels & With the default value the contour intervals are set automatically, which generally will NOT be a very good choice, but they may provide a starting point.  If you want to specify contourLevels, use the \texttt{seq} function.  In general it would look like: contourLevels = seq(from,to,by). & NA\\
maxDiff & In the \texttt{plotDiffCountours} function instead of using contourLevels, the contours are set by maxDiff which is the absolute value of the maximum difference to be plotted.  Contour intervals are set to run from -maxDiff to maxDiff. &\\
span & Specifies the smoothness of the discharge duration information that goes on this graph.  A larger value will make it smoother.  The default should work well in most cases. & 60\\
pval & The probability value for the discharge frequency information shown on the plot.  When flowDuration=TRUE, the plot has two black curves on it.  In the default value case these are at the 5 and 95 percent levels on the seasonal discharge duration curve.  pval = 0.01 would place these at the 1 and 99 percent points.  pval = 0.1 would place them at 10  and 90. & 0.05\\
vert1 & This simply plots a vertical black line on the graph at a particular time (defined in decimal years).  It is used to illustrate the idea of a \enquote{vertical slice} through the contour plot, which might then be shown in a subsequent use of \texttt{plotConcQSmooth}. & NA  \\
vert2 & This gives the location of a second vertical black line on the graph at a particular time (defined in decimal years). & NA\\
horiz & This simply plots a horizontal black line on the graph at a particular discharge value (defined in the units specified by qUnit).  It is used to illustrate the idea of the seasonal cycle in concentrations for a given discharge and the long-term change in this cycle.  & NA\\
flowDuration & If TRUE it draws the discharge duration lines at the specified probabilities.  If FALSE, the discharge duration lines are left off. & TRUE\\
\hline
\end{tabularx}
}
\end{table}


\begin{table}[ht]
{\footnotesize
\caption{Variables used in EGRET \texttt{plotConcQSmooth} and/or \texttt{plotConcTimeSmooth} functions \label{tab:wrtdsMultiVariables}}
\begin{tabularx}{\textwidth}{lXl}
\hline
\multicolumn{1}{c}{\textbf{\textsf{Argument}}} &
\multicolumn{1}{c}{\textbf{\textsf{Definition}}} &
\multicolumn{1}{c}{\textbf{\textsf{Default}}}\\
\hline
date1 & This is the date for the first curve to be shown on the \texttt{plotConcQSmooth} graph.  It must be in the form \texttt{"}yyyy-mm-dd\texttt{"} (it must be in quotes) &\\
date2 & This is the date for the second curve to be shown on the plot (\texttt{"}yyyy-mm-dd\texttt{"}), If you don't want a second curve then the argument must be date2=NA &\\
date3 & This is the date for the third curve to be shown on the plot (\texttt{"}yyyy-mm-dd\texttt{"}), If you don't want a third curve then the argument must be date3=NA &\\
q1 & This is the discharge for the first curve on the \texttt{plotConcTime} smooth graph. It is in units specified by qUnit &\\
q2 & This is the discharge for the second curve. If you don't want a second curve then the argument must be q2=NA &\\
q3 & This is the discharge for the third curve. If you don't want a third curve then the argument must be q3=NA &\\
qUnit & Determines what units will be used for discharge, see \texttt{printqUnitCheatSheet} & 2\\
qLow & The discharge value that should form the left edge of the plotConcQSmooth graph in the user-selected discharge units. & \\
qHigh & The discharge value that should form the right edge of the \texttt{plotConcQSmooth} graph in the user-selected discharge units. & \\
centerDate & This is the month and day at the center of the time window for the \texttt{plotConcTimeSmooth} graph. It must be in the form \texttt{"}mm-dd\texttt{"} in quotes &\\
yearStart & The starting year for the \texttt{plotConcTimeSmooth} graph &\\
yearEnd & The ending year for the \texttt{plotConcTimeSmooth} graph &\\

legendLeft & This determines the placement of the legend on the graph.  It establishes the left edge of the legend and is expressed in the discharge units being used.  The default (which is NA) will let it be placed automatically.  The legend can end up conflicting with one or more of the curves.  Once the location of the curves is established then this can be set in a way that avoids conflict. & 0\\
legendTop & This determines the placement of the legend on the graph.  It establishes the top edge of the legend and is expressed according to the concentration values on the y-axis.  The default (which is NA) will let it be placed automatically.  The legend can end up conflicting with one or more of the curves.  Once the location of the curves is established then this can be set in a way that avoids conflict. & 0\\
concMax & Maximum value for the vertical axis of the graph.  The reason to set concMax is if you want to make several plots that have the same vertical axis. & NA\\
concMin & [This one is only used when logScale=TRUE].  Minimum value for the vertical axis of the graph. The reason to set concMin is if you want to make several plots that have the same vertical axis. & NA\\
bw & Default is FALSE, which means we want a color plot.  If bw=TRUE that means it should be black and white.\\
printTitle & If TRUE the plot has a title.  If FALSE no title (useful for publications where there will be a caption). & FALSE\\
printValues & If TRUE the estimated values that make up the plotted lines are printed on the console.  If FALSE they are not printed.  This could be useful if you wanted to compute various comparisons across time periods. & FALSE\\
windowY & This is the half-window width for time in WRTDS.  It has units of years.  & 7 \\
windowQ & This is the half-window width for discharge in WRTDS.  It has units of ln(discharge).  & 2 \\
windowS & This is the half-window width for seasons in WRTDS.  It has units of years.  & 0.5 \\
edgeAdjust & Specify whether to use the modified method for calculating the windows at the edge of the record.  The modified method tends to reduce curvature near the start and end of record & TRUE \\
\hline
\end{tabularx}
}
\end{table}

\FloatBarrier


%------------------------------------------------------------ 
\section{Creating tables in Microsoft\textregistered\ software from an R data frame}
\label{app:createWordTable}
%------------------------------------------------------------
A few steps that are required to create a table in Microsoft\textregistered\ software (Excel, Word, PowerPoint, etc.) from an R data frame. There are a variety of good methods, one of which is detailed here. The example we will step through is creation of a table in Microsoft\textregistered\ Excel based on the data frame tableData:

<<label=getSiteApp, echo=TRUE,eval=TRUE>>=

tableData <- tableResults(eList)
@


First, save the data frame as a tab delimited file (you don't want to use comma delimited because there are commas in some of the data elements):


<<label=saveData, echo=TRUE, eval=FALSE>>=
write.table(tableData, file="tableData.tsv",sep="\t", 
            row.names = FALSE,quote=FALSE)
@

This will save a file in your working directory called tableData.tsv.  You can see your working directory by typing getwd() in the R console. Opening the file in a general-purpose text editor, you should see the following:

\singlespacing
\begingroup
    \fontsize{8pt}{10pt}
\begin{verbatim}
Year  Discharge [cms]	Conc [mg/L]	FN_Conc [mg/L]	Flux [10^6kg/yr]	FN_Flux [10^6kg/yr]
1980	   4.25	           0.949	      1.003	         0.1154	            0.106  
1981	   2.22	           1.035	      0.999	         0.0675	            0.108 
1982	   3.05	           1.036	      0.993	         0.0985	            0.110 
...
\end{verbatim}
\endgroup
% \doublespacing

Next, follow the steps below to open this file in Excel:
\begin{enumerate}
\item Open Excel
\item Click on the File tab
\item Click on the Open option
\item Navigate to the working directory (as shown in the results of getwd())
\item Next to the File name text box, change the dropdown type to All Files (*.*)
\item Double click tableData.tsv
\item A text import wizard will open up, in the first window, choose the Delimited radio button if it is not automatically picked, then click on Next.
\item In the second window, click on the Tab delimiter if it is not automatically checked, then click Finished.
\item Use the many formatting tools within Excel to customize the table
\end{enumerate}

From Excel, it is simple to copy and paste the tables in other word processing or presentation software products. An example using one of the default Excel table formats is here.

\begin{figure}[ht!]
\centering
 \resizebox{0.9\textwidth}{!}{\includegraphics{table1.png}} 
\caption{A simple table produced in Microsoft\textregistered\ Excel}
\label{overflow}
\end{figure}

\FloatBarrier

%------------------------------------------------------------ 
\section{Saving Plots}
\label{app:savingPlots}
%------------------------------------------------------------
Plots can be saved from R as JPG, PNG, PDF, and Postscript files. JPG and PNG are easy to use in any number of programs (Microsoft\textregistered\ Word or PowerPoint, for example), but the images cannot be resized later. PDF and Postscript images are easily re-sizable.

There are three steps to saving plots. The first is to open the \enquote{device} (and declare the output type and file name). The second step is to execute the function just as you would when plotting to the screen, but no output will appear. The third step is to turn off the device. It is also possible to put many plots within the same pdf.  Some simple examples should demonstrate this easily:

<<label=savePlots, echo=TRUE, eval=FALSE>>=
jpeg("plotFlowSingle.jpg")
plotFlowSingle(eList, 1)
dev.off()

png("plotFlowSingle.png")
plotFlowSingle(eList,1)
dev.off()

pdf("plotFlowSingle.pdf")
plotFlowSingle(eList,1)
dev.off()

postscript("plotFlowSingle.ps")
plotFlowSingle(eList,1)
dev.off()

#Many plots saved to one pdf:
pdf("manyPlots.pdf")
plotFlowSingle(eList,1)
plotFlowSingle(eList,2)
plotFlowSingle(eList,3)
plotFlowSingle(eList,4)
dev.off()

@

There are many additional options for each of these devices. See the R help files for more information. One useful option for the larger \texttt{fluxBiasMulti} graph is to adjust the height and width of the output. The output of fluxBiasMulti is larger than the default pdf or postscript devices. Therefore, specifying the height and width eliminates R having to re-size the graphic:

<<label=savePlots2, echo=TRUE, eval=FALSE>>=
postscript("fluxBiasMulti.ps", height=10,width=8)
fluxBiasMulti(eList)
dev.off()
@


\clearpage

%-------------------------------------
\section{Disclaimer}
%------------------------------------
Software created by USGS employees along with contractors and grantees (unless specific stipulations are made in a contract or grant award) are to be released as Public Domain and free of copyright or license. Contributions of software components such as specific algorithms to existing software licensed through a third party are encouraged, but those contributions should be annotated as freely available in the Public Domain wherever possible. If USGS software uses existing licensed components, those licenses must be adhered to and redistributed.

Although this software has been used by the U.S. Geological Survey (USGS), no warranty, expressed or implied, is made by the USGS or the U.S. Government as to accuracy and functionality, nor shall the fact of distribution constitute any such warranty, and no responsibility is assumed by the USGS in connection therewith.


%------------------------------------------------------------
% BIBLIO
%------------------------------------------------------------
\begin{thebibliography}{10}

% \bibitem{HirschI}
% Helsel, D.R. and R. M. Hirsch, 2002. Statistical Methods in Water Resources Techniques of Water Resources Investigations, Book 4, chapter A3. U.S. Geological Survey. 522 pages. \url{http://pubs.usgs.gov/twri/twri4a3/}

\bibitem{HirschI}
Hirsch, R.M., De Cicco, L.A. (2014), User guide to Exploration and Graphics for RivEr Trends (EGRET) and dataRetrieval: R packages for hydrologic data (version 2.0, February 2015): U.S. Geological Survey Techniques and Methods book 4, chap. A10, 93 p. \url{http://dx.doi.org/10.3133/tm4A10}


\bibitem{HirschII}
Hirsch, R.M., Moyer, D.L. and Archfield, S.A. (2010), Weighted Regressions on Time, Discharge, and Season (WRTDS), with an Application to Chesapeake Bay River Inputs. JAWRA Journal of the American Water Resources Association, 46: 857-880. doi: 10.1111/j.1752-1688.2010.00482.x \url{http://onlinelibrary.wiley.com/doi/10.1111/j.1752-1688.2010.00482.x/full}

\bibitem{HirschIII}
Sprague, L.A., Hirsch, R.M., and Aulenbach, B.T. (2011), Nitrate in the Mississippi River and Its Tributaries, 1980 to 2008: Are We Making Progress? Environmental Science \& Technology, 45 (17): 7209-7216. doi: 10.1021/es201221s 

\bibitem{HirschIV}
Moyer, D.L., Hirsch, R.M., and Hyer, K.E. (2012), Comparison of Two Regression-Based Approaches for Determining Nutrient and Sediment Fluxes and Trends in the Chesapeake Bay Watershed: U.S. Geological Survey Scientific Investigations Report 2012-5244, 118 p. \url{http://pubs.usgs.gov/sir/2012/5244/}

\bibitem{HirschV}
Rice, K.C., and Hirsch, R.M. (2012), Spatial and temporal trends in runoff at long-term streamgages within and near the Chesapeake Bay Watershed: U.S. Geological Survey Scientific Investigations Report 2012-5151, 56 p. \url{http://pubs.usgs.gov/sir/2012/5151}


\end{thebibliography}

\end{document}

