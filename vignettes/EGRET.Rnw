%\VignetteIndexEntry{Introduction to the EGRET package}
%\VignetteEngine{knitr::knitr}
%\VignetteDepends{}
%\VignetteSuggests{xtable, dataRetrieval}
%\VignetteImports{methods,survival, fields}
%\VignettePackage{EGRET}

\documentclass[a4paper,11pt]{article}

\usepackage{amsmath}
\usepackage{times}
\usepackage{hyperref}
\usepackage[numbers, round]{natbib}
\usepackage[american]{babel}
\usepackage{authblk}
\usepackage{subfig}
\usepackage{placeins}
\usepackage{footnote}
\usepackage{tabularx}
\renewcommand\Affilfont{\itshape\small}

\renewcommand{\topfraction}{0.85}
\renewcommand{\textfraction}{0.1}
\usepackage{graphicx}

\textwidth=6.2in
\textheight=8.5in
\parskip=.3cm
\oddsidemargin=.1in
\evensidemargin=.1in
\headheight=-.3in

%------------------------------------------------------------
% newcommand
%------------------------------------------------------------
\newcommand{\scscst}{\scriptscriptstyle}
\newcommand{\scst}{\scriptstyle}
\newcommand{\Robject}[1]{{\texttt{#1}}}
\newcommand{\Rfunction}[1]{{\texttt{#1}}}
\newcommand{\Rclass}[1]{\textit{#1}}
\newcommand{\Rpackage}[1]{\textit{#1}}
\newcommand{\Rexpression}[1]{\texttt{#1}}
\newcommand{\Rmethod}[1]{{\texttt{#1}}}
\newcommand{\Rfunarg}[1]{{\texttt{#1}}}

\begin{document}

<<openLibrary, echo=FALSE>>=
library(xtable)
options(continue=" ")
options(width=60)
library(knitr)

@


<<include=TRUE ,echo=FALSE,eval=TRUE>>=
opts_chunk$set(highlight=TRUE, tidy=TRUE, keep.space=TRUE, keep.blank.space=FALSE, keep.comment=TRUE, concordance=TRUE,tidy=FALSE)

knit_hooks$set(inline = function(x) {
   if (is.numeric(x)) round(x, 3)})
knit_hooks$set(crop = hook_pdfcrop)
@

%------------------------------------------------------------
\title{Introduction to the EGRET package}
%------------------------------------------------------------
\author[1]{Robert Hirsch}
\author[1]{Laura De Cicco}
\affil[1]{United States Geological Survey}

\maketitle

Exploration and Graphics for RivEr Trends (EGRET): An R-package for the analysis of long-term changes in water quality and streamflow, including the water-quality method Weighted Regressions on Time, Discharge, and Season (WRTDS)

\tableofcontents

%------------------------------------------------------------
\section{Introduction to Exploration and Graphics for RivEr Trends (EGRET)}
%------------------------------------------------------------ 

Exploration and Graphics for RivEr Trends (EGRET): An R-package for the analysis of long-term changes in water quality and streamflow. EGRET includes statistics and graphics for streamflow history, water quality trends, and the modeling algorithm Weighted Regressions on Time, Discharge, and Season (WRTDS). 


\textbf{Please see the official EGRET manual:}
(\href{https://github.com/USGS-R/EGRET/raw/Documentation/EGRET%2Bmanual_4.doc}{link to download}) 
\textbf{for more information on the EGRET package.}

For information on getting started in R, downloading and installing the package, see Appendix \ref{sec:appendix1}.

The best way to learn about the WRTDS approach and to see examples of its application to multiple large data sets is to read two journal articles.  They are available, for free, from the journals in which they were published.

The first relates to nitrate and total phosphorus data for 9 rivers draining to Chesapeake Bay.  The URL is \cite{HirschII}: 
\url{http://onlinelibrary.wiley.com/doi/10.1111/j.1752-1688.2010.00482.x/full}

The second is an application  to nitrate data for 8 monitoring sites on the Mississippi River or its major tributaries \cite{HirschIII}.  The URL is: \url{http://pubs.acs.org/doi/abs/10.1021/es201221s}

This vignette assumes that the user understands the concepts underlying WRTDS.  Thus, reading at least the first of these papers is necessary for understanding.  The method has been enhanced beyond what was published there.  The enhancement is that it now properly handles censored data by using survival regression rather than ordinary regression.  The details of that are in a report on Chesapeake Bay river input trends \cite{HirschIV}:\url{http://pubs.usgs.gov/sir/2012/5244/}.  The specific enhancements for handling censored data are on pages 9-11 of that report.

This vignette will walk through the major functions provided by the EGRET package. The package dataRetrieval is required for importing data in an EGRET-friendly format. The dataRetrieval package, along with installation instructions can be found at:
\url{https://github.com/USGS-R/dataRetrieval}

Installing dataRetrieval will provide a vignette similar to this document, with complete working examples of the main dataRetrieval functions.

This document assumes the reader is familiar with the dataRetrieval package. Further details can be found in the user guide that can be found on gitHub: \url{https://github.com/USGS-R/EGRET/raw/Documentation/EGRET%2Bmanual_4.doc}

%------------------------------------------------------------
\section{EGRET Workflow}
%------------------------------------------------------------ 
Subsequent sections of this vignette will discuss the EGRET workflow steps in greater detail. This section provides a handy cheat sheet for diving into an EGRET analysis.

<<workflow, echo=TRUE,eval=FALSE>>=
library(dataRetrieval)
library(EGRET)

############################
# Gather discharge data:
siteID <- "01491000" #Choptank River at Greensboro, MD
startDate <- "" #Gets earliest date
endDate <- "2011-09-30"
Daily <- getDVData(siteID,"00060",startDate,endDate)

# Gather sample data:
parameter_cd<-"00631" #5 digit USGS code
Sample <- getSampleData(siteID,parameter_cd,startDate,endDate)

# Gather site and parameter information:
INFO<- getMetaData(siteID,parameter_cd,interactive=FALSE)
INFO$shortName <- "Choptank River"
INFO <- setPA()

# Merge discharge with sample data:
Sample <- mergeReport()
############################

############################
# Check flow history data:
annualSeries <- makeAnnualSeries()
plotFlowSingle(istat=7,qUnit="thousandCfs")
plotSDLogQ()
plotQTimeDaily(1990,2010,qLower=1,qUnit=3)
plotFour(qUnit=3)
plotFourStats(qUnit=3)
############################

############################
# Check sample data:
boxConcMonth()
boxQTwice()
plotLogConcTime()
plotConcTime()
plotConcQ()
plotLogConcQ()
plotLogFluxQ()
multiPlotDataOverview()
############################

############################
# Run WRTDS model:
modelEstimation()
############################

############################
#Check model results:
yearStart <- 2000
yearEnd <- 2010

#Require Sample + INFO:
plotConcTimeDaily(yearStart, yearEnd)
plotFluxTimeDaily(yearStart, yearEnd)
plotConcPred()
plotFluxPred()
plotLogConcPred()
plotLogFluxPred()
plotResidPred()
plotResidQ()
plotResidTime()
boxResidMonth()
boxConcThree()

#Require annualResults + INFO:
plotConcHist()
plotFluxHist()

# Multi-line plots:
date1 <- "2000-09-01"
date2 <- "2005-09-01"
date3 <- "2009-09-01"
qBottom<-100
qTop<-5000
plotLogConcQSmooth(date1, date2, date3, qBottom, qTop, 
                   concMax=2, concMin=0.1,qUnit=1)
plotConcQSmooth(date1, date2, date3, qBottom, qTop, 
                   concMax=2,qUnit=1)
q1 <- 10
q2 <- 25
q3 <- 75
centerDate <- "07-01"
plotConcTimeSmooth(q1, q2, q3, centerDate, 2000, yearEnd)

# Multi-plot:
fluxBiasMulti()
fluxBiasMultiAlt()
fluxBiasEight()

#Contour plots:
clevel<-seq(0,2,0.5)
maxDiff<-0.8
plotContours(yearStart,yearEnd,qBottom,qTop, 
             contourLevels = clevel,qUnit=1)
plotDiffContours(year0=2000,yearEnd,
                 qBottom,qTop,maxDiff,qUnit=1)

@


%------------------------------------------------------------ 
\section{EGRET Dataframes and Units}
\label{sec:dataframes}
%------------------------------------------------------------ 
The EGRET package uses 3 default dataframes throughout the calculations, analysis, and graphing. These dataframes are Daily (\ref{sec:dataframesDaily}), Sample (\ref{sec:dataframesSample}), and INFO (\ref{sec:dataframesINFO}). EGRET uses entirely SI units to store the data, but for purposes of output, it can report results in a wide variety of units, which will be discussed in (\ref{sec:units}). To start our exploration, the packages must be installed (check the appendix for detailed instructions (\ref{sec:appendix1})), then opened with the following command:

<<openlibraries, echo=TRUE,eval=TRUE>>=
library(dataRetrieval)
library(EGRET)
@

%------------------------------------------------------------ 
\subsection{Daily}
\label{sec:dataframesDaily}
%------------------------------------------------------------ 
The Daily dataframe initially is populated with columns generated by the dataRetrieval package (Table \ref{table:Daily1}).  After running the WRTDS calculations (as will be described in section \ref{sec:wrtds}), additional columns are inserted (Table \ref{table:Daily2}).

\begin{table}[!ht]
\centering
\caption{Daily dataframe} 
\label{table:Daily1}
\begin{tabular}{llll}
  \hline
ColumnName & Type & Description & Units \\ 
  \hline
  Date & Date & Date & date \\ 
  Q & number & Discharge in cms & cms \\ 
  Julian & number & Number of days since January 1, 1850 & days \\ 
  Month & integer & Month of the year [1-12] & months \\ 
  Day & integer & Day of the year [1-366] & days \\ 
  DecYear & number & Decimal year & years \\ 
  MonthSeq & integer & Number of months since January 1, 1850 & months \\ 
  Qualifier & string & Qualifing code & character \\ 
  i & integer & Index of days, starting with 1 & days \\ 
  LogQ & number & Natural logarithm of Q & numeric \\ 
  Q7 & number & 7 day running average of Q & cms \\ 
  Q30 & number & 30 running average of Q & cms \\ 
   \hline
\end{tabular}
\end{table}

\begin{table}[!ht]
\centering
\caption{Daily dataframe, post-WRTDS} 
\label{table:Daily2}
\begin{tabular}{llll}
  \hline
ColumnName & Type & Description & Units \\ 
  \hline
yHat & number & The WRTDS estimate of the log of concentration & numeric \\ 
  SE & number & The WRTDS estimate of the standard error of yHat & numeric \\ 
  ConcDay & number & The WRTDS estimate of concentration & mg/L \\ 
  FluxDay & number & The WRTDS estimate of flux & kg/day \\ 
  FNConc & number & Flow normalized estimate of concentration & mg/L \\ 
  FNFlux & number & Flow Normalized estimate of flux & kg/day \\ 
   \hline
\end{tabular}
\end{table}

\FloatBarrier
\pagebreak


%------------------------------------------------------------ 
\subsection{Sample}
\label{sec:dataframesSample}
%------------------------------------------------------------ 
The Sample dataframe initially is populated with columns generated by the dataRetrieval package (Table \ref{table:Sample1}). After running the WRTDS calculations (as will be described in section \ref{sec:wrtds}), additional columns are inserted (Table \ref{table:Sample2}):

\begin{table}[!ht]
\begin{minipage}{\linewidth}
\centering
\caption{Sample dataframe} 
\label{table:Sample1}
\begin{tabular}{llll}
  \hline
ColumnName & Type & Description & Units \\ 
  \hline
Date & Date & Date & date \\ 
  ConcLow & number & Lower limit of concentration & mg/L \\ 
  ConcHigh & number & Upper limit of concentration & mg/L \\ 
  Uncen & integer & Uncensored data (1=true, 0=false) & integer \\ 
  ConcAve & number & Average concentration & mg/L \\ 
  Julian & number & Number of days since January 1, 1850 & days \\ 
  Month & integer & Month of the year [1-12] & months \\ 
  Day & integer & Day of the year [1-366] & days \\ 
  DecYear & number & Decimal year & years \\ 
  MonthSeq & integer & Number of months since January 1, 1850 & months \\ 
  SinDY & number & Sine of DecYear & numeric \\ 
  CosDY & number & Cosine of DecYear & numeric \\ 
  Q \footnotemark[1] & number & Discharge & cms \\ 
  LogQ \footnotemark[1] & number & Natural logarithm of flow & numeric \\ 
   \hline
\end{tabular}
\end{minipage}
\end{table}
\footnotetext[1]{Populated after calling mergeReport.}

\begin{table}[!ht]
\centering
\caption{Sample dataframe, post-WRTDS} 
\label{table:Sample2}
\begin{tabular}{llll}
  \hline
ColumnName & Type & Description & Units \\ 
  \hline
yHat\footnotemark[2] & number & estimate of the log of concentration & numeric \\ 
  SE\footnotemark[2] & number & estimate of the standard error of yHat & numeric \\ 
  ConcHat\footnotemark[2] & number & unbiased estimate of concentration & mg/L \\ 
   \hline
\end{tabular}
\end{table}

\footnotetext[2]{These estimates are 'leave-one-out cross validation' estimates.  They are computed for each sample value by withholding that particular sample value from the data set when they are estimated.}

\FloatBarrier
\pagebreak


%------------------------------------------------------------ 
\subsection{INFO}
\label{sec:dataframesINFO}
%------------------------------------------------------------ 
The INFO dataframe is used to store information about the measurements, such as station name, parameter name, drainage area, etc. There can be many additional, optional columns, but the columns in Table \ref{table:Info1} are required to initiate the EGRET analysis. After running the WRTDS calculations (as will be described in section \ref{sec:wrtds}), additional columns (Table \ref{table:Info2}) are automatically inserted into the INFO dataframe (the meaning of the values will be discussed further sections):


\begin{table}[!ht]
\begin{minipage}{\linewidth}
\begin{center}
\caption{INFO dataframe}
\label{table:Info1}
\begin{tabular}{lll}
  \hline
ColumnName & Type & Description \\ 
  \hline
  shortName & string & Name of site, suitable for use in graphical headings \\ 
  staAbbrev & string & Abbreviation for station name, used in saveResults \\ 
  paramShortName & string & Name of constituent, suitable for use in graphical headings \\ 
  constitAbbrev & string & Abbreviation for constituent name, used in saveResults \\ 
  drainSqKm & numeric & Drainage area in  km\verb@^@2 \\ 
  paStart \footnotemark[1] & integer (1-12) & Starting month of period of analysis \\ 
  paLong \footnotemark[1] & integer (1-12) & Length of period of analysis in months \\ 
   \hline
\end{tabular}
\end{center}
\end{minipage}
\end{table}

\footnotetext[1]{Inserted with the setPA function}

\begin{table}[!ht]
\centering
\caption{INFO dataframe, post-WRTDS} 
\label{table:Info2}
\begin{tabular}{lll}
  \hline
ColumnName & Description & Units \\ 
  \hline
bottomLogQ & Lowest discharge in prediction surfaces & numeric \\ 
  stepLogQ & Step size in log discharge in prediction surfaces & numeric \\ 
  nVectorLogQ & Number of steps in discharge, prediction surfaces & numeric \\ 
  bottomYear & Starting year in prediction surfaces & numeric \\ 
  stepYear & Step size in years in prediction surfaces & numeric \\ 
  nVectorYear & Number of steps in years in prediction surfaces & numeric \\ 
  windowY & Half-window width in the time dimension & years \\ 
  windowQ & Half-window width in the log discharge dimension & numeric \\ 
  windowS & Half-window width in the seasonal dimension & years \\ 
  minNumObs & Minimum number of observations for regression & integer \\ 
  minNumUncen & Minimum number of uncensored observations & integer \\ 
   \hline
\end{tabular}
\end{table}

\FloatBarrier
\pagebreak


%------------------------------------------------------------ 
\subsection{Units}
\label{sec:units}
%------------------------------------------------------------ 
EGRET uses entirely SI units to store the data, but for purposes of output, it can report results in a wide variety of units. The default is that concentration is measured in mg/L, discharge is cubic meters per second (cms), flux is kg/day, and drainage area is km\verb@^@2. When discharge values are imported from USGS web services (using the dataRetrieval package), they are automatically converted from cubic feet per second (cfs) to cms unless the argument convert is set to FALSE.  This can cause confusion if not careful. 

Although the data is stored in the dataframes in SI, it is possible to report the results in a variety of units. For all functions that provide output, there are two arguments that can be defined to set the output units: qUnit and fluxUnit.  qUnit and fluxUnit can be defined by a numeric code or name.  There are two functions that can be called to see the options are printqUnitCheatSheet and printFluxUnitCheatSheet.


<<cheatSheets,echo=TRUE,eval=TRUE>>=
printqUnitCheatSheet()
@

When a function has an input argument qUnit, you can define the flow units that will be used in the figure or table that is generated by the function with the index (1-6) as shown above. The choice should be based on the units that are customary for the audience, but also so that the discharge values don't have too many digits to the right or left of the decimal point.

<<cheatSheets2,echo=TRUE,eval=TRUE>>=
printFluxUnitCheatSheet()
@

When a function has an input argument fluxUnit, you can define the flux units with the index (1-12) as shown above. The choice should be based on the units that are customary for the audience, but also so that the flux values don't have too many digits to the right or left of the decimal point. Tons are always 'short tons' and not 'metric tons'.

\FloatBarrier

%------------------------------------------------------------ 
\section{Flow History}
\label{sec:flowHistory}
%------------------------------------------------------------ 
This section describes functions included in the EGRET package that provide a variety of table and graphical outputs looking only at flow statistics based on time-series smoothing. These functions were designed for studies of long-term streamflow change and work best for daily streamflow data sets of 50 years or longer. This type of analysis might be useful for studying issues such as the influence of land use change, water management change, or climate change on streamflow conditions.  This includes potential impacts on average flows, high flows, low flows, both at annual time scales as well as seasonal or monthly time scales. 

At this point it is assumed that you can load the daily discharge record into R, create the Daily dataframe, and enter the required meta-data into the INFO dataframe. If not, see the dataRetrieval vignette:

<<vignette1, eval=FALSE, echo=TRUE>>=
vignette("dataRetrieval")
@

We will walk through an example from Columbia River at Dalles, OR.

<<flowHistory,echo=TRUE,eval=TRUE>>=
siteID <- "14105700"  
startDate <- ""
endDate <- ""

Daily <- getDVData(siteID,"00060",startDate,endDate)
INFO <- getMetaData(siteID,"",interactive=FALSE)
INFO$shortName <- "Columbia River"
@

The first choice you need to make is what period of analysis to use (pa). What is the period of analysis?  If we want to examine our data set as a time series of water years, then the period of analysis is October through September.  If we want to examine the data set as calendar years then the period of analysis should be January through December.  We might want to examine the winter season, which we could define as December through February, then those 3 months become the period of analysis. The only constraints on the definition of a period of analysis are these: It must be defined in terms of whole months.  It must be a set of contiguous months (like March-April-May).  And it must have a length that is no less than 1 month and no more than 12 months.  It can be uniquely defined by two arguments: paLong and paStart.  paLong is the length of the period of analysis, and paStart is the first month of the period of analysis. Table \ref{table:paINFO} summarizes paLong and paStart.

\begin{table}[!ht]
\centering
\caption{Period of Analysis Information} 
\label{table:paINFO}
\begin{tabular}{lll}
  \hline
PeriodOfAnalysis & paStart & paLong \\ 
  \hline
Calendar Year & 1 & 12 \\ 
  Water Year & 10 & 12 \\ 
  Winter & 12 & 3 \\ 
  September & 9 & 1 \\ 
   \hline
\end{tabular}
\end{table}

To set a period running from December through February:
<<newChunckWinter, echo=TRUE,eval=FALSE>>=
INFO <- setPA(paStart=12,paLong=3)
@

To set the default value (water year):
<<newChunck, echo=TRUE,eval=TRUE>>=
INFO <- setPA()
@

The next step is to create the annual series of flow statistics.  These will be stored in a matrix called annualSeries that contain the statistics described in table \ref{table:istat}.

\begin{table}[!ht]
\centering
\caption{Index of Statistics Information} 
\label{table:istat}
\begin{tabular}{ll}
  \hline
istat & Name \\ 
  \hline
1 & 1-day minimum flow \\ 
  2 & 7-day minimum flow \\ 
  3 & 30-day minimum flow \\ 
  4 & median flow \\ 
  5 & mean flow \\ 
  6 & 30-day maximum flow \\ 
  7 & 7-day maximum flow \\ 
  8 & 1-day maximum flow \\ 
   \hline
\end{tabular}
\end{table}

To create the annualSeries matrix, using the function makeAnnualSeries:
<<newChunckAS, echo=TRUE,eval=TRUE>>=
annualSeries <- makeAnnualSeries()
@

Once the annualSeries matrix is created, the plots of any of the stored statistics can be generated with the plotFlowSingle function.

\FloatBarrier

%------------------------------------------------------------ 
\subsection{Plotting Options}
\label{sec:plotOptions}
%------------------------------------------------------------ 

\FloatBarrier

This section will give examples of the available plots appropriate for studying flow history once the annualSeries has been created. The plots here will use the default variable input options.  For any function, you can get a complete list of input variables (as described in the previous section) in a help file by typing a ? before the function name in the R console. See Appendix \ref{sec:flowHistoryVariables} for information on the available input variables for these plotting functions. Also, the complete EGRET manual has more detailed information for each plot type (\href{https://github.com/USGS-R/EGRET/raw/Documentation/EGRET%2Bmanual_4.doc}{link to download}). Finally, see section \ref{app:savingPlots} for information on saving plots.

Figure \ref{fig:plotSingleandSD}:
<<plotSingleandSD, echo=TRUE, fig.cap="Discharge statistics",fig.subcap=c("plotFlowSingle(istat=5,qUnit='thousandCfs')","plotSDLogQ()"),out.width='.5\\linewidth',out.height='.5\\linewidth',fig.show='hold'>>=
plotFlowSingle(istat=5,qUnit="thousandCfs")
plotSDLogQ()
@

Figure \ref{fig:plotFour}:
<<plotFour, echo=TRUE, fig.cap="plotFour(qUnit=3)",fig.show='asis',out.width='1\\linewidth',out.height='1\\linewidth'>>=
plotFour(qUnit=3)
@

Figure \ref{fig:plotFourStats}:
<<plotFourStats,echo=TRUE, fig.cap="plotFourStats(qUnit=3)",fig.show='asis',out.width='1\\linewidth',out.height='1\\linewidth'>>=
plotFourStats(qUnit=3)
@


\FloatBarrier

The simplest way to look at these time series is with the function plotFlowSingle. The statistic index (istat) must be defined, but other input arguements can defined. To see a list of these optional arguments and other information about the function, type ?plotFlowSingle in the R console. All of the graphs in plotFlowSingle, plotFourStats, and all but one of the graphs in plotFour, show both the individual annual values of the selected flow statistic (e.g. the annual mean or 7-day minimum), but they also show a curve that is a smooth fit to those data.  The curve is a LOWESS (locally weighted scatterplot smooth).  The algorithm for computing it is provided in \cite{HirschV}:\url{http://pubs.usgs.gov/sir/2012/5151/}  (pages 6 and 7).  The default is that they are smoothed with a "half-window width" of 30 years and the smoothing is performed on the log discharge values.  The smoothing window is a user-defined option. 

plotSDLogQ produces a graphic of the running standard deviation of the log of daily discharge over time.  The idea is to get some idea of how variability of daily discharge is changing over time.  By using the standard deviation of the log discharge the statistic becomes dimensionless.  It also means that it is a way of looking at variability quite aside from average values, so, in the case of a system where discharge might be increasing over a period of years, this provides a way of looking at the variability relative to that changing mean value.  It is much like a coefficient of variation, but it has sample properties that make it a smoother measure of variability.  There are often comments about how things like urbanization or enhanced greenhouse gases in the atmosphere are bringing about an increase in variability, this is one way to explore that idea. plotFour, plotFourStats, and plot15 are all designed to plot several graphs from the other functions all in a single figure. 

Here is an example of looking at mean daily streamflow for the full water year and then looking at mean daily streamflow for the winter season only.  The site being considered is the Merced River at Happy Isles Bridge in Yosemite National Park in California.  First, we can look at the mean daily streamflow for the full year (after having read in the data and metadata):

<<Merced, echo=TRUE,eval=TRUE,fig.cap="Merced River Winter Trend",fig.subcap=c("Water Year", "Dec-Feb"),out.width='.5\\linewidth',out.height='.5\\linewidth',fig.show='hold'>>=
sta<-"11264500"
Daily <-getDVData(sta,"00060",StartDate="",EndDate="")
INFO <- getMetaData(sta,"",interactive=FALSE)
INFO$shortName <- "Merced River, CA"
INFO <- setPA()
annualSeries <- makeAnnualSeries()
plotFlowSingle(istat=5)

INFO<-setPA(paStart=12,paLong=3)
annualSeries<-makeAnnualSeries()
plotFlowSingle(istat=5)

@

The two figures produced are shown below.  What they show us is that on an annual basis there is very little indication of a long-term trend in mean streamflow, but for the winter months there is a pretty strong indication of an upward trend.  This could well be related to the climate warming in the Sierra Nevada, resulting in a general increase in the ratio of rain to snow in the winter and more thawing events.

plotQTimeDaily is simply a time series plot of discharge.  But, it is most suited for showing events above some discharge threshold.  In the simplest case, it can plot the entire record, but given the line weight and use of an arithmetic scale it will primarily provide a visual focus on the higher values.

<<Mississippi, echo=TRUE,eval=TRUE,fig.cap="Mississippi River at Keokuk Iowa",fig.subcap=c("Water Year", "Dec-Feb"),out.width='1\\linewidth',out.height='1\\linewidth',fig.show='hold'>>=
sta<-"05474500"
Daily <-getDVData(sta,"00060",StartDate="",EndDate="")
INFO <- getMetaData(sta,"",interactive=FALSE)
INFO$shortName <- "Mississippi River at Keokuk Iowa"
INFO <- setPA()

plotQTimeDaily(startYear=1870,endYear=2020,qUnit=3,qLower=300)

@

The example shown here illustrates a very long record and the long gap of more than 60 years of no flows above 300,000 ft\verb|^|3/s, followed by the last 50 years with at least 5 events above that threshold. plotQTimeDaily requires startYear and endYear, along with some other optional arguements (see ?plotQTimeDaily for more details).

\FloatBarrier

%------------------------------------------------------------ 
\subsection{Table Options}
\label{sec:tableOptions}
%------------------------------------------------------------ 
Sometimes easier to consider the results in table formats rather than graphically. Similar to the function plotFlowSingle, the printSeries will print the requested flow statistics (Table \ref{table:istat}). A small sample of the output is printed below.


<<printSeries, eval=FALSE,echo=TRUE>>=
printSeries(istat=3, qUnit=3)
@

\begin{verbatim}
Mississippi River at Keokuk Iowa
 Water Year
    30-day minimum
    Thousand Cubic Feet per Second
   year   annual   smoothed
           value    value
   1879     22.6     30.1
   1880     31.7     28.7
   1881     23.0     27.5
...
   2011     51.0     32.4
   2012     34.3     32.1
   2013     16.2     31.8
\end{verbatim}

Another way to look at the results is to consider how much the smoothed values change between various pairs of years.  These changes can be represented in four different ways.  
\begin{itemize}
  \item As a change between the first and last year of the pair, expressed in the flow units selected.
  \item As a change between the first and last year of the pair, expressed as a percentage of the value in the first year
  \item As a slope between the first and last year of the pair, expressed in terms of the flow units per year.
  \item As a slope between the first and last year of the pair, expressed as a percentage change per year (a percentage based on the value in the first year).
\end{itemize}

There is another argument that can be very useful in this function: yearPoints.  In the default case, the set of years that are compared are at 5 year intervals along the whole data set.  If the data set was quite long this can be a daunting number of comparisons.  For example, in an 80 year record, there would be 136 such pairs. Instead, we could look at changes for every 20 years starting in 1930: 


<<tfc, eval=TRUE,echo=TRUE>>=
annualSeries <- makeAnnualSeries()
tableFlowChange(istat=3, qUnit=3,yearPoints=c(1890,1950,2010))
@

See Appendix \ref{app:createWordTable} for instructions on converting an R dataframe to a table in Microsoft. 

\FloatBarrier


%------------------------------------------------------------ 
\section{Summary of Water Quality Data (without using WRTDS)}
\label{sec:wqa}
%------------------------------------------------------------ 
\FloatBarrier

Before running the WRTDS model, it is very helpful to take a look at the measured data in a graphical way to understand its behavior and to identify things that might be errors in the data set or learn about the temporal distribution of the data (identify gaps) prior to running the model.  It is always best to clear up these issues before moving forward.

We will now use the Choptank River at Greensboro, MD as our example case. The Choptank River is a small tributary of the Chesapeake Bay. Inorganic nitrogen (nitrate and nitrite) has been measured from 1979 onward. First, we need to get the streamflow and nitrate data into R, then use the mergeReport function to associate flow with the discrete measured water quality data. Before the data can be graphed or entered into any of the WRTDS analysis the discharge data must be brought into the Sample dataframe.  This is done with the mergeReport function which does this merger of the discharge information and also provides a compact report about some major features of the data set.

<<wrtds1,eval=FALSE,echo=TRUE>>=
siteID <- "01491000" #Choptank River at Greensboro, MD
startDate <- "1979-10-01"
endDate <- "2011-09-30"
param<-"00631"
Daily <- getDVData(siteID,"00060",startDate,endDate)
INFO<- getMetaData(siteID,param,interactive=FALSE)
INFO$shortName <- "Choptank River"

Sample <- getSampleData(siteID,param,startDate,endDate)
Sample <- mergeReport()
@

<<wrtds2,eval=TRUE,echo=FALSE>>=
siteID <- "01491000" #Choptank River at Greensboro, MD
startDate <- "1979-10-01"
endDate <- "2011-09-30"
param<-"00631"
Daily <- ChopDaily
Sample <- ChopSample
INFO <- ChopINFO
annualSeries <- makeAnnualSeries()
@


%------------------------------------------------------------ 
\subsection{Plotting Options}
\label{sec:plotOptionsWQ}
%------------------------------------------------------------ 
\FloatBarrier

This section will give examples of the available plots appropriate for analyzing the data prior to performing a WRTDS analysis. The plots here will use the default variable input options.  For any function, you can get a complete list of input variables (as described in the previous section) in a help file by typing a ? before the function name in the R console. See Appendix \ref{sec:wqVariables} for information on the available input variables for these plotting functions. Also, the complete EGRET manual has more detailed information for each plot type (\href{https://github.com/USGS-R/EGRET/raw/Documentation/EGRET%2Bmanual_4.doc}{link to download}).

One note about any of the plotting functions that show the sample data:  If a value in the data set is a non-detect. Then it is displayed on a graph as a vertical line.  The top of the line is the reporting limit and the bottom is either zero, or if the graph is plotting log concentration values, the minimum value on the y-axis.  This line is an 'honest' representation of what we know about that observation and doesn't involve us using a statistical model to fill in what we don't know. 

Figure \ref{fig:plotBoxes}:
<<plotBoxes, echo=TRUE, fig.cap="Concentration box plots",fig.subcap=c("boxConcMonth()","boxQTwice(qUnit=1)"),out.width='.5\\linewidth',out.height='.5\\linewidth',fig.show='hold'>>=
boxConcMonth()
boxQTwice(qUnit=1)
@

Note that the statistics to create the boxplot in boxQTwice are performed after the data is log-transformed.

Figure \ref{fig:plotConcTime}:
<<plotConcTime,echo=TRUE, fig.cap="Concentration vs time",fig.subcap=c("plotConcTime()","plotLogConcTime()"),out.width='.5\\linewidth',out.height='.5\\linewidth',fig.show='hold'>>=
plotConcTime()
plotLogConcTime()
@

Figure \ref{fig:plotConcQ}:
<<plotConcQ, echo=TRUE, fig.cap="Concentration vs discharge",fig.subcap=c("plotConcQ(qUnit=1)","plotLogConcQ(qUnit=1)"),out.width='.5\\linewidth',out.height='.5\\linewidth',fig.show='hold'>>=
plotConcQ(qUnit=1)
plotLogConcQ(qUnit=1)
@

Figure \ref{fig:plotLogConcQ}:
<<plotLogConcQ, echo=TRUE, fig.cap="plotLogFluxQ(qUnit=1)",out.width='.5\\linewidth',out.height='.5\\linewidth',fig.show='hold'>>=
plotLogFluxQ(qUnit=1)
@

Figure \ref{fig:multiPlotDataOverview}:
<<multiPlotDataOverview, echo=TRUE, fig.cap="multiPlotDataOverview(qUnit=1)",fig.show='asis',out.width='1\\linewidth',out.height='1\\linewidth'>>=
multiPlotDataOverview(qUnit=1)
@

\FloatBarrier

%------------------------------------------------------------ 
\subsection{Extending Plots Past Defaults}
\label{sec:extendedPlots}
%------------------------------------------------------------ 

\FloatBarrier

The basic plotting options were shown in the previous section.  This section demonstrates some ideas on how to extend the capabilities of the EGRET plots. EGRET plots use R's base plotting options. Default graphical parameters (see ?mar) and plot parameters were chosen, but all can be overridden. Additionally, features can be added to a plot after calling the plot function. To change the plot margins, set the argument customPar to TRUE.

Figure \ref{fig:plotLogConcQComparison}:
<<plotLogConcQComparison,echo=TRUE,eval=TRUE,fig.cap="Modified plotLogConcQ", fig.subcap=c("Default","Modified"),out.width='.5\\linewidth',out.height='.5\\linewidth',fig.show='hold'>>=
plotLogConcQ()
par(mar=c(8,8,8,8))

plotLogConcQ(customPar=TRUE,col="blue",cex=1.1,
             cex.axis=1.4,cex.main=1.5,cex.lab=1.2,pch=18,lwd=2)
grid()
legend(4.5,.1,"Choptank Nitrogen", pch=18, col="blue",bg="white")
@

Figure \ref{fig:plot-fascinating2}:
<<plot-fascinating2,echo=TRUE,eval=TRUE,fig.cap="Modified box plot",fig.show='asis',out.width='1\\linewidth',out.height='1\\linewidth'>>=
# Adding text to a boxplot:
bp <- boxConcMonth()
mtext(paste(bp$n, sep = ""), at = seq_along(bp$n), 
      line = -1, side = 3, cex=0.75)
mtext("n = ",side=3,line=-1,adj=.01, cex=0.75)
@


\FloatBarrier

%------------------------------------------------------------ 
\subsection{Table Options}
\label{sec:tableOptionsWQ}
%------------------------------------------------------------ 
Another useful tool for checking the data before running the WRTDS estimations is flowDuration. This is a utility function that can help define the flow ranges that we want to explore.  It prints out key points on the flow duration curve.  They are defined for a particular part of the year, although they can be done for the entire year.  

<<flowDuration, eval=TRUE, echo=TRUE>>=
flowDuration()
@

\FloatBarrier
%------------------------------------------------------------ 
\section{WRTDS Analysis}
\label{sec:wrtds}
%------------------------------------------------------------ 
Weighted Regressions on Time, Discharge and Season (WRTDS) creates a model of long-term trends in river-water quality, seasonal components, and discharge-related components of the behavior of measured water-quality parameters. In this section, we will step though the process require for a WRTDS analysis. The next section (\ref{sec:wrtdsResults}) will detail the available methods to view and evaluate the model results. 

Once you have looked at your data using the tools described in section \ref{sec:wqa}, and have determined there is sufficient representative data, it is time to run the WRTDS model. There are a few inputs that can be defined before running the model (see Appendix \ref{sec:wrtdsInputVariables}).

Assuming you are using the defaults, with dataframes called Daily, Sample, and INFO, the modelEstimation function will run the WRTDS modeling algorithm:

<<wrtds3, eval=FALSE, echo=TRUE>>=
modelEstimation()
@

This function is slow, and shows the progress in percent complete. See the references and manual for more information. It's important to understand that this is the one function that will globally change your Daily, Sample, and INFO dataframes. It also creates a new matrix: surfaces, and a new dataframe: AnnualResults. It is unusual R programming behavior (and generally considered poor practice), but was chosen to make it easy for the user.

If you wish to change the period of analysis after comleting modelEstimation, there is one more step that needs to happen.  For example, if we want to consider a 3 month season that runs from December through Feburary, we would set paStart = 12 and paLong = 3.  (Note that the period of analysis and the arguments paStart and paLong are introduced at the begining of section \ref{sec:flowHistory} of this document.) After running modelEstimation run the command:

<<wrtds4, eval=FALSE, echo=TRUE>>=
AnnualResults<-setupYears(paStart=12,paLong=3)
@


Finally, it is a good idea to save your results because of the computational time that has been invested in producing these results. Assuming that you have already created the object savePath, the command is:

<<wrtds5, eval=FALSE, echo=TRUE>>=
savePath <- "C:/Users/ldecicco/WRTDS_Output"
saveResults(savePath) 
@

This will now save all of the objects in your workspace. If you have saved workspaces from R versions earlier than 3.0, there will be a warning when opening them in R 3.0 (or \verb|>|). Re-saving the workspace using R 3.0 (or \verb|>|) should get rid of the warning.

Alternatively, you can save the individually generated data in your working directory:

<<wrtds6, eval=FALSE, echo=TRUE>>=
save(Daily, file="Daily.RData")
save(Sample, file="Sample.RData")
save(INFO, file="INFO.RData")
save(AnnualResults, file="AnnualResults.RData")
save(surfaces, file="surfaces.RData")
@

To load this data later:
<<wrtds7, eval=FALSE, echo=TRUE>>=
load(Daily, file="Daily.RData")
load(Sample, file="Sample.RData")
load(INFO, file="INFO.RData")
load(AnnualResults, file="AnnualResults.RData")
load(surfaces, file="surfaces.RData")
@


\FloatBarrier

%------------------------------------------------------------ 
\section{WRTDS Results}
\label{sec:wrtdsResults}
%------------------------------------------------------------ 
At this point (after having run modelEstimation) we can start considering how to view the annual averages for the variables that have been calculated.  See Appendix \ref{sec:wrtdsOutputVariables} for common input variables for these functions. Additionally, check the help files (in the R console, type ? followed by the function name).

%------------------------------------------------------------ 
\subsection{Plotting Options}
\label{sec:wrtdsPlotting}
%------------------------------------------------------------ 

\FloatBarrier

Check the help files or manual for more details on the following functions.  See section \ref{app:savingPlots} for information on saving plots.


<<getChopData1,echo=FALSE,eval=TRUE>>=
Sample <- ChopSample
Daily <- ChopDaily
INFO <- ChopINFO
AnnualResults <- setupYears()
surfaces <- exsurfaces
@


Figure \ref{fig:plotConcTimeDaily}:
<<plotConcTimeDaily, echo=TRUE, fig.cap="Concentration and flux vs time",fig.subcap=c("plotConcTimeDaily(startYear=2008, endYear=2010)","plotFluxTimeDaily(startYear=2008, endYear=2010)"),out.width='.5\\linewidth',out.height='.5\\linewidth',fig.show='hold'>>=
yearStart <- 2008
yearEnd <- 2010

plotConcTimeDaily(yearStart, yearEnd)
plotFluxTimeDaily(yearStart, yearEnd)
@

Figure \ref{fig:plotFluxPred}:
<<plotFluxPred, echo=TRUE, fig.cap="Concentration predictions",fig.subcap=c('plotConcPred()','plotLogConcPred()'),out.width='.5\\linewidth',out.height='.5\\linewidth',fig.show='hold'>>=
plotConcPred()
plotLogConcPred()
@

Figure \ref{fig:plotLogFluxPred}:
<<plotLogFluxPred, echo=TRUE, fig.cap="Flux predictions",fig.subcap=c('plotFluxPred()','plotLogFluxPred()'), out.width='.5\\linewidth',out.height='.5\\linewidth',fig.show='hold'>>=
plotFluxPred()
plotLogFluxPred()
@

Figure \ref{fig:plotResidQ}:
<<plotResidQ, echo=TRUE, fig.cap="Residuals",fig.subcap=c("plotResidPred()","plotResidQ(qUnit=1)"),out.width='.5\\linewidth',out.height='.5\\linewidth',fig.show='hold'>>=
plotResidPred()
plotResidQ(qUnit=1)
@

Figure \ref{fig:boxResidMonth}:
<<boxResidMonth, echo=TRUE, fig.cap="Residuals with respect to time",fig.subcap=c("plotResidTime()","boxResidMonth()"),out.width='.5\\linewidth',out.height='.5\\linewidth',fig.show='hold'>>=
plotResidTime()
boxResidMonth()
@

Figure \ref{fig:boxConcThree}:
<<boxConcThree, echo=TRUE, fig.cap="Default boxConcThree()",out.width='.5\\linewidth',out.height='.5\\linewidth',fig.show='asis',results='hide'>>=
boxConcThree()
@

Figure \ref{fig:plotFluxHist}:
<<plotFluxHist, echo=TRUE, fig.cap="Concentration and flux history",fig.subcap=c("plotConcHist()", "plotFluxHist()"),out.width='.5\\linewidth',out.height='.5\\linewidth',fig.show='hold'>>=
plotConcHist()
plotFluxHist()
@

Figure \ref{fig:plotLogConcQSmooth}:
<<plotLogConcQSmooth, echo=TRUE, fig.cap="Concentration vs. discharge",fig.subcap=c("plotConcQSmooth","plotLogConcQSmooth"),out.width='.5\\linewidth',out.height='.5\\linewidth',fig.show='hold'>>=
qBottom<-1
qTop<-5000
date1 <- "2000-09-01"
date2 <- "2005-09-01"
date3 <- "2009-09-01"
plotConcQSmooth(date1, date2, date3, qBottom, qTop, 
                   concMax=1.5,qUnit=1)
plotLogConcQSmooth(date1, date2, date3, qBottom, qTop,
                   concMax=1.5, concMin=0.1,qUnit=1)
@

Figure \ref{fig:plotConcTimeSmooth}:
<<plotConcTimeSmooth, echo=TRUE, fig.cap="plotConcTimeSmooth",out.width='.5\\linewidth',out.height='.5\\linewidth',fig.show='asis',results='hide'>>=
q1 <- 10
q2 <- 25
q3 <- 75
centerDate <- "07-01"
plotConcTimeSmooth(q1, q2, q3, centerDate, 2000, 2010)
@

Figure \ref{fig:fluxBiasMulti}:
<<fluxBiasMulti, echo=TRUE, fig.cap="fluxBiasMulti(qUnit=1)",fig.show='asis',out.width='1\\linewidth',out.height='1\\linewidth'>>=
fluxBiasMulti(qUnit=1)
@

Figure \ref{fig:fluxBiasMultiAlt}:
<<fluxBiasMultiAlt, echo=TRUE, fig.cap="fluxBiasMultiAlt(qUnit=1)",fig.show='asis',out.width='1\\linewidth',out.height='1\\linewidth'>>=
fluxBiasMultiAlt(qUnit=1)
@

Figure \ref{fig:fluxBiasEight}:
<<fluxBiasEight, echo=TRUE, fig.cap="fluxBiasEight(qUnit=1)",fig.show='asis',fig.width=8, fig.height=10>>=
fluxBiasEight(qUnit=1)
@

Figure \ref{fig:plotContours}:
<<plotContours, echo=TRUE, fig.cap="plotContours(2008,2010,1,5000,clevel,qUnit=1)",fig.show='asis',out.width='1\\linewidth',out.height='1\\linewidth'>>=
clevel<-seq(0,2,0.5)
plotContours(yearStart,yearEnd,qBottom,qTop, 
             contourLevels = clevel,qUnit=1)
@

Figure \ref{fig:plotDiffContours}:
<<plotDiffContours, echo=TRUE, fig.cap="plotDiffContours(year0=2000,yearEnd=2010,qBottom=1,qTop=5000,maxDiff=2)",fig.show='asis',out.width='1\\linewidth',out.height='1\\linewidth'>>=
maxDiff<-2
plotDiffContours(year0=2000,yearEnd,
                 qBottom,qTop,maxDiff,qUnit=1)
@


To specify contourLevels the seq function in R should be used.  In general it would look like this: contourLevels = seq(from,to,by).  For example, if we want contours to run from 0 to 5 in steps of 0.2 we would say:  

<<setcontours, echo=TRUE, eval=FALSE>>=
contourLevels = seq(0,5,0.2)
@


\FloatBarrier
%------------------------------------------------------------ 
\subsection{Table Options}
\label{sec:wrtdsTable}
%------------------------------------------------------------ 
Sometimes easier to consider the results in table formats rather than graphically. The function tableResults produces a simple text table that contains the annual values for the results.  Each row of the output represents a year and it prints: year, average discharge, average concentration, flow normalized concentration, average flux, and flow normalized flux.  

<<tableResults1, echo=TRUE, eval=TRUE>>=
tableResults()
@

The other table option is tableChange. This is a function that provides for the computation of changes or slopes between any selected pairs of time points.  These computations are made only on the flow-normalized results.

<<tableChange1, eval=TRUE, echo=TRUE>>=
tableChange()
@

Finally, tableChangeSingle (Table \ref{table:tableChangeSingle}) operates exactly the same as tableChange except for the addition of two arguments: returnDataFrame and flux. This functions provides either concentration results or flux results, but not both.  This can be useful when producing many output tables for a report that is entirely focused on concentration or one that is entirely focused on flux.  The arguments are identical to those for tableChange, except that the final two arguments.  The first is a logical argument to indicate if a dataframe of output should be returned (for later manipulation or printing through other programs such as Excel), this argument is returnDataFrame, and its default is FALSE.  The final argument is flux, and the default is TRUE.  When flux=TRUE the output is only for flux, and when flux=FALSE the output is only for concentration.  Additionally, this function allows for the return of a dataframe.  See Appendix \ref{app:createWordTable} for instructions on converting an R dataframe to a table in Microsoft.

\begin{table}[ht]
\centering
\begin{tabular}{rrrrrrr}
  \hline
 & Year1 & Year2 & change [tons/yr] & slope [tons/yr/yr] & change[\%] & slope [\%/yr] \\ 
  \hline
1 & 2002 & 2004 & 3.10 & 1.60 & 2.00 & 1.00 \\ 
  2 & 2002 & 2006 & 6.60 & 1.70 & 4.20 & 1.10 \\ 
  3 & 2004 & 2006 & 3.50 & 1.70 & 2.20 & 1.10 \\ 
   \hline
\end{tabular}
\caption{Table created from tableChangeSingle function} 
\label{table:tableChangeSingle}
\end{table}


\clearpage

\FloatBarrier
\appendix
%------------------------------------------------------------ 
\section{Getting Started}
\label{sec:appendix1}
%------------------------------------------------------------ 
This section describes the options for downloading and installing the dataRetrieval package.

%------------------------------------------------------------
\subsection{New to R?}
%------------------------------------------------------------ 
If you are new to R, you will need to first install the latest version of R, which can be found here: \url{http://www.r-project.org/}.

There are many options for running and editing R code, one nice environment to learn R is RStudio. RStudio can be downloaded here: \url{http://rstudio.org/}. Once R and RStudio are installed, the environment package needs to be installed as described in the next section.

At any time, you can get information about any function in R by typing a question mark before the functions name.  This will open a file (in RStudio, in the Help window) that describes the function, the required arguments, and provides working examples.

<<helpFunc,eval = FALSE>>=
?getJulian
@

To see the raw code for a particular code, type the name of the function:
<<rawFunc,eval = FALSE>>=
getJulian
@


%------------------------------------------------------------
\subsection{R User: Installing EGRET}
%------------------------------------------------------------ 
To install the EGRET packages and it's dependencies:

<<installFromCran,eval = FALSE>>=
install.packages(c("zoo","survival","methods","fields","spam"))
install.packages("dataRetrieval", 
                 repos="http://usgs-r.github.com/", 
                 type="source")
install.packages("EGRET", 
                 repos="http://usgs-r.github.com/", 
                 type="source")
@

It is a good idea to re-start R after installing the package if installing an updated version. 

After installing the package, you need to open the library each time you re-start R.  This is done with the simple command:
<<openLibraryTest, eval=FALSE>>=
library(dataRetrieval)
library(EGRET)
@

\newpage
\FloatBarrier
%------------------------------------------------------------ 
\section{Common Function Variables}
\label{sec:appendixPlot}
%------------------------------------------------------------ 
%------------------------------------------------------------ 
\subsection{flowHistory Plotting Input}
\label{sec:flowHistoryVariables}
%------------------------------------------------------------
\begin{table}[ht]
\caption{Variables used in flow history plots  \label{tab:flowHistoryVariables}}
\begin{tabularx}{\textwidth}{lX}
\hline
  \textbf{Argument} & \textbf{Definition} \\
\hline
istat & Which flow statistic to plot: 1-8.  Must be specified, see Table \ref{table:istat}. \\
yearStart\footnotemark[1] & What is the decYear value where you want the graph to start? \\
yearEnd\footnotemark[1] & What is the decYear value where you want the graph to end? \\
qMax & User specified upper limit on y axis (can be used when we want several graphs to all share the same scale) will be specified in the discharge units that the user selects. \\
printTitle & can be TRUE or FALSE, you may want FALSE if it is going to be a figure with a caption or if it is a part of a multipanel plot. \\
tinyPlot & Can be TRUE or FALSE, the TRUE option assures that there will be a small number of tick marks, consistent with printing in a small space \\
runoff & Can be TRUE or FALSE.  If true then discharge values are reported as runoff in mm/day.  This can be very useful in multi-site analyses. \\
qUnit & An index indicating what discharge units to use.  Options run from 1 to 6 (see section \ref{sec:units}).  The choice should be based on the units that are customary for the audience but also, the choice should be made so that the discharge values don't have too many digits to the right or left of the decimal point.\\
printStaName\footnotemark[2] & Can be TRUE or FALSE, if TRUE the name of the streamgage is stated in the plot title. \\
printPA\footnotemark[2] & Can be TRUE or FALSE, if TRUE the period of analysis is stated in the plot title. \\
printIstat\footnotemark[2] & Can be TRUE or FALSE, if TRUE the name of the statistic (e.g. 7-day minimum flow) is stated in the plot title. \\

\hline
\end{tabularx}

\end{table}

\footnotetext[1]{Setting yearStart and yearEnd will determine where the graphs start and end, but they don't determine where the smoothing analysis starts and ends.  There are situations, typically where many sites are be analyzed together, where you may want to run the smoothing on a consistent period of record across all sites.  Doing this requires modifying the Daily data frame before running makeAnnualSeries}
\footnotetext[2]{If the printTitle argument is set to FALSE, then it really makes no difference what you do with printSta, printPA, or printIstat.  They can all be left as their default values and thus there is no need to include them in the call for the function.}

\FloatBarrier
\clearpage

%------------------------------------------------------------ 
\subsection{Water Quality Plotting Input}
\label{sec:wqVariables}
%------------------------------------------------------------

\begin{table}[ht]
\caption{Variables used in water quality analysis plots  \label{tab:wqVariables}}
\begin{tabularx}{\textwidth}{lX}
\hline
  \textbf{Argument} & \textbf{Definition} \\
\hline
qUnit & Determines what units will be used for discharge, see section \ref{sec:units}\\
printTitle & If TRUE the plot has a title.  If FALSE no title (useful for publications where there will be a caption)\\
qLower & The lower bound on the discharge on the day of sampling that will be used in forming a subset of the sample data set that will be displayed in the graph.  It is expressed in the units specified in qUnit.  If qLower = NA, then the lower bound is set to zero.\\
qUpper & The upper bound on the discharge on the day of sampling that will be used in forming a subset of the sample data set that will be displayed in the graph.  It is expressed in the units specified in qUnit.  If qUpper = NA, then the lower bound is set to infinity.\\
paLong & The length of the time period that will be used in forming a subset of the sample data set that will be displayed in the graph, expressed in months. \\ 
paStart & The starting month for the time period that will be used in forming a subset of the sample data set that will be displayed in the graph.  It is expressed in months (calendar months).\\
concMax & The upper limit on the vertical axis of graphs showing concentration values in mg/L (NA sets value to just above maximum).  \\
concMin & The lower limit on the vertical axis of graphs showing concentration values in mg/L (NA sets value to just below minimum for log scales, zero for linear).  \\
fluxUnit & Determines what units will be used for flux (see Section \ref{sec:units}).\\
fluxMax & The upper limit on the vertical axis of graphs showing flux values.  \\
\hline
\end{tabularx}

\end{table}

\FloatBarrier
\clearpage


%------------------------------------------------------------ 
\subsection{WRTDS Estimation Input}
\label{sec:wrtdsInputVariables}
%------------------------------------------------------------
\begin{table}[ht]
\caption{Variables in WRTDS  \label{tab:WRTDS}}
\begin{tabularx}{\textwidth}{lXl}
\hline
  \textbf{Argument} & \textbf{Definition} & \textbf{Default} \\
\hline
windowY & The half window width for the time weighting, measured in years.  Values much shorter than 10 usually result in a good deal of oscillations in the system that are likely not very realistic & 10\\
windowQ & The half window width for the weighting in terms of ln(Q).  For very large rivers (average discharge values in the range of many tens of thousands of cfs) a smaller value than 2 may be appropriate, but probably not less than 1 & 2 \\
windowS & The half window width for the seasonal weighting, measured in years.  Any value >0.5 will make data from all seasons have some weight.  Values should probably not be lower than 0.3 and there is no need to go higher than 0.5 & 0.5 \\
minNumObs & This is the minimum number of observations with non-zero weight that the individual regressions will require before they will be used.  If there too few observations the program will iterate, making the windows wider until the number increases above this minimum.  The only reason to lower this is in cases where the data set is rather small.  It should always be set to a number at least slightly smaller than the sample size.  Any value lower than about 60 is probably in the 'dangerous' range, in terms of the reliability of the regression & 100 \\ 
minNumUncen & This is the minimum number of uncensored observations with non-zero weight that the individual regressions will require before they will be used.  If there are too few uncensored observations the program will iterate, making the windows wider until the number increases above this minimum.  The only reason to lower this is in cases where the number of uncensored values is rather small.  The method has never been tested in situations where there are very few uncensored values & 50 \\
\hline
\end{tabularx}

\end{table}

\FloatBarrier
\clearpage

%------------------------------------------------------------ 
\subsection{WRTDS Plotting Input}
\label{sec:wrtdsOutputVariables}
%------------------------------------------------------------

\begin{table}[ht]
\caption{Variables used in WRTDS analysis plots  \label{tab:wrtdsVariables}}
\begin{tabularx}{\textwidth}{lX}
\hline
  \textbf{Argument} & \textbf{Definition} \\
\hline
qUnit & Determines what units will be used for discharge, see \ref{sec:units}\\
fluxUnit & An index indicating what flux units will be used , see \ref{sec:units}\\
stdResid & This is an option.  If FALSE, it prints the regular residuals (they are in ln concentration units).  If TRUE, it is the standardized residuals.  These are the residuals divided by their estimated standard error (each residual has its own unique standard error).  In theory, the standardized residuals should have mean zero and standard deviation of 1 \\
printTitle & If TRUE the plot has a title.  If FALSE no title (useful for publications where there will be a caption) \\
startYear & The starting date for the graph, expressed as decimal years, for example, 1989.0 \\
endYear & The ending date for the graph, expressed as decimal years, for example, 1996.0 \\
moreTitle & A character variable that adds additional information to the graphic title.  Typically used to indicate what the estimation method was (e.g. WRTDS or LOADEST).  Default is ' ' which indicates that nothing is added to title \\
fluxMax & The upper limit on the vertical axis of graphs showing flux values.  \\
plotFlowNorm & If TRUE the graph shows the annual flux values as circles and the flow-normalized values as a green curve.  If false, it only shows the annual flux values.\\
\hline
\end{tabularx}

\end{table}

\begin{table}[ht]
\caption{Variables used in WRTDS contour plots: plotContours and plotDiffContours \label{tab:wrtdsContourVariables}}
\begin{tabularx}{\textwidth}{lX}
\hline
  \textbf{Argument} & \textbf{Definition} \\
\hline
qUnit & Determines what units will be used for discharge, see \ref{sec:units}\\
qBottom & The discharge value that should form the bottom of the graph \\
qTop & The discharge value that should form the top of the graph \\
printTitle & If TRUE the plot has a title.  If FALSE no title (useful for publications where there will be a caption) \\
yearStart & The starting date for the graph, expressed as decimal years, for example, 1989.0 \\
yearEnd & The ending date for the graph, expressed as decimal years, for example, 1996.0 \\
whatSurface & default = 3.  This should generally be at its default value.  At whatSurface = 3, the plotted surface shows the expected value of concentration.  For whatSurface = 1, it shows the yHat surface (natural log of concentration).  For whatSurface = 2, it shows the SE surface (the standard error in log concentration).    \\
contourLevels & Default value is NA.  With the default value the contour intervals are set automatically.  These will generally NOT be a very good choice, but they may provide a starting point.  \\
maxDiff & In the plotDiffCountours function instead of using contourLevels, the contours are set by maxDiff which is the absolute value of the maximum difference to be plotted.  Contour intervals are set to run from -maxDiff to maxDiff.\\
span & Default value = 60.  Specifies the smoothness of the flow duration information that goes on this graph.  A larger value will make it smoother.  The default should work well in most cases.\\
pval & Default value = 0.05.  The probability value for the flow frequency information shown on the plot.  The plot has two black curves on it.  In the default value case these are at the 5 and 95 percent levels on the seasonal flow duration curve.  pval = 0.01 would place these at the 1 and 99 percent points.  pval = 0.1 would place them at 10  and 90.\\
vert1 & Default = NA.  This simply plots a vertical black line on the graph at a particular time (defined in decimal years).  It is used to illustrate the idea of a 'vertical slice' through the contour plot, which might then be shown in a subsequent use of plotConcQSmooth.  \\
vert2 & Default = NA.  This gives the location of a second vertical black line on the graph at a particular time (defined in decimal years). \\
horiz & Default = NA.  This simply plots a horizontal black line on the graph at a particular discharge value (defined in the units specified by qUnit).  It is used to illustrate the idea of the seasonal cycle in concentrations for a given discharge and the long-term change in this cycle.  \\
flowDuration & Default = TRUE.  If TRUE it draws the flow duration lines at the specified probabilities.  If FALSE, the flow duration lines are left off.\\
\hline
\end{tabularx}

\end{table}


\begin{table}[ht]
\caption{Variables used in WRTDS multi-line plots  \label{tab:wrtdsMultiVariables}}
\begin{tabularx}{\textwidth}{lX}
\hline
  \textbf{Argument} & \textbf{Definition} \\
\hline
date1 & This is the date for the first curve to be shown on the plot.  It must be in the form 'yyyy-mm-dd' (it must be in quotes)\\
date2 & This is the date for the second curve to be shown on the plot ('yyyy-mm-dd'), If you don't want a second curve then the argument must be date2=NA\\
date3 & This is the date for the third curve to be shown on the plot ('yyyy-mm-dd'), If you don't want a third curve then the argument must be date3=NA\\
qUnit & Determines what units will be used for discharge, see printqUnitCheatSheet() \\
qLow & The discharge value that should form the left edge of the graphic. \\
qHigh & The discharge value that should form the right edge of the graphic. \\
legendLeft & This determines the placement of the legend on the graph.  It establishes the left edge of the legend and is expressed in the flow units being used.  The default (which is NA) will let it be placed automatically.  The legend can end up conflicting with one or more of the curves.  Once the location of the curves is established then this can be set in a way that avoids conflict.\\
legendTop & This determines the placement of the legend on the graph.  It establishes the top edge of the legend and is expressed according to the concentration values on the y-axis.  The default (which is NA) will let it be placed automatically.  The legend can end up conflicting with one or more of the curves.  Once the location of the curves is established then this can be set in a way that avoids conflict.\\
concMax & Maximum value for the vertical axis of the graph.  The default is NA.  The reason to set concMax is if you want to make several plots that have the same vertical axis.\\
concMin & [This one is only used in plotLogConcQSmooth].  Minimum value for the vertical axis of the graph.  The default is NA.  The reason to set concMin is if you want to make several plots that have the same vertical axis.\\
bw & Default is FALSE, which means we want a color plot.  If bw=TRUE that means it should be black and white.\\
printTitle & If TRUE the plot has a title.  If FALSE no title (useful for publications where there will be a caption).\\
printValues & If TRUE the estimated values that make up the plotted lines are printed on the console.  If FALSE they are not printed.  Default is FALSE.  This could be useful if you wanted to compute various comparisons across time periods.\\
windowY & This is the half-window width for time in WRTDS.  It has units of years.  The default value is 10. \\
windowQ & This is the half-window width for discharge in WRTDS.  It has units of ln(discharge).  The default value is 2. \\
windowS & This is the half-window width for seasons in WRTDS.  It has units of years.  The default value is 0.5. \\
\hline
\end{tabularx}

\end{table}

\FloatBarrier


%------------------------------------------------------------ 
\section{Creating tables in Microsoft from R}
\label{app:createWordTable}
%------------------------------------------------------------
There are a few steps that are required in order to create a table in a Microsoft product (Excel, Word, Powerpoint, etc.) from an R dataframe. There are a variety of good methods, one of which is detailed here. The example we will step through here will be to create a table in Microsoft Word based on the dataframe tableData:

<<label=getSiteApp, echo=TRUE>>=
siteNumber <- '01491000'
ChopAvail <- getDataAvailability(siteNumber)
ChopDaily <- ChopAvail["dv" == ChopAvail$service,]
ChopDaily <- ChopDaily["00003" == ChopDaily$statCd,]
pCodeINFO <- getMultipleParameterNames(ChopDaily$parameter_cd,                                 interactive=FALSE)
ChopDaily <- merge(ChopDaily,pCodeINFO,by="parameter_cd")

tableData <- with(ChopDaily, data.frame(shortName=srsname, 
              Start=startDate, End=endDate, Count=count, 
              Units=parameter_units))
@

First, save the dataframe as a tab delimited file (you don't want to use comma delimited because there are commas in some of the data elements):


<<label=saveData, echo=TRUE, eval=FALSE>>=
write.table(tableData, file="tableData.tsv",sep="\t", 
            row.names = FALSE,quote=FALSE)
@

This will save a file in your working directory called tableData.tsv.  You can see your working directory by typing getwd() in the R console. Opening the file in a general-purpose text editor, you should see the following:

\begin{verbatim}
shortName  Start  End  Count	Units
Temperature, water	2010-10-01	2012-06-24	575	deg C
Stream flow, mean. daily	1948-01-01	2013-03-13	23814	cfs
Specific conductance	2010-10-01	2012-06-24	551	uS/cm @25C
Suspended sediment concentration (SSC)	1980-10-01	1991-09-30	3651	mg/l
Suspended sediment discharge	1980-10-01	1991-09-30	3652	tons/day
\end{verbatim}

To open this file in Excel:
\begin{enumerate}
\item Open Excel
\item Click on the File tab
\item Click on the Open option
\item Browse to the working directory (as shown in the results of getwd())
\item Next to the File name text box, change the dropdown type to All Files (*.*)
\item Double click tableData.tsv
\item A text import wizard will open up, in the first window, choose the Delimited radio button if it is not automatically picked, then click on Next.
\item In the second window, click on the Tab delimiter if it is not automatically checked, then click Finished.
\item Use the many formatting tools within Excel to customize the table
\end{enumerate}

From Excel, it is simple to copy and paste the tables in other Microsoft products. An example using one of the default Excel table formats is here.

\begin{figure}[ht!]
\centering
 \resizebox{0.9\textwidth}{!}{\includegraphics{table1.png}} 
\caption{A simple table produced in Microsoft Excel}
\label{overflow}
\end{figure}

%------------------------------------------------------------ 
\section{Saving Plots}
\label{app:savingPlots}
%------------------------------------------------------------
There are a variety of options for saving plots in R. Plots can be saved in JPG, PNG, PDF, and Postscript. JPG and PNG are easy to input into any number of programs (Microsoft Word or Powerpoint for example), but the images cannot be resized later. PDF and Postscript are easily re-sizable.

There are two steps to saving plots. The first is to declare the output type and file name by opening the 'device'. The second step is to turn off the device. Some simple examples should demonstrate this easily:

<<label=savePlots, echo=TRUE, eval=FALSE>>=
jpeg("plotFlowSingle.jpg")
plotFlowSingle(1)
dev.off()

png("plotFlowSingle.png")
plotFlowSingle(1)
dev.off()

pdf("plotFlowSingle.pdf")
plotFlowSingle(1)
dev.off()

postscript("plotFlowSingle.ps")
plotFlowSingle(1)
dev.off()

@

There are many additional options for each of these devices. See the R help files for more information. One option that would be useful for the larger fluxBiasEight graph is to adjust the height and width of the output. The output of fluxBiasEight is larger than the default pdf or postscript devices. Therefore, specifying the height and width eliminates R having to re-size the graphic:

<<label=savePlots2, echo=TRUE, eval=FALSE>>=
postscript("fluxBiasEight.ps", height=10,width=8)
fluxBiasEight()
dev.off()

pdf("fluxBiasEight.pdf", height=10,width=8)
fluxBiasEight()
dev.off()

@



\clearpage
%------------------------------------------------------------
% BIBLIO
%------------------------------------------------------------
\begin{thebibliography}{10}

\bibitem{HirschI}
Helsel, D.R. and R. M. Hirsch, 2002. Statistical Methods in Water Resources Techniques of Water Resources Investigations, Book 4, chapter A3. U.S. Geological Survey. 522 pages. \url{http://pubs.usgs.gov/twri/twri4a3/}

\bibitem{HirschII}
Hirsch, R. M., Moyer, D. L. and Archfield, S. A. (2010), Weighted Regressions on Time, Discharge, and Season (WRTDS), with an Application to Chesapeake Bay River Inputs. JAWRA Journal of the American Water Resources Association, 46: 857-880. doi: 10.1111/j.1752-1688.2010.00482.x \url{http://onlinelibrary.wiley.com/doi/10.1111/j.1752-1688.2010.00482.x/full}

\bibitem{HirschIII}
Sprague, L. A., Hirsch, R. M., and Aulenbach, B. T. (2011), Nitrate in the Mississippi River and Its Tributaries, 1980 to 2008: Are We Making Progress? Environmental Science \& Technology, 45 (17): 7209-7216. doi: 10.1021/es201221s \url{http://pubs.acs.org/doi/abs/10.1021/es201221s}

\bibitem{HirschIV}
Moyer, D.L., Hirsch, R.M., and Hyer, K.E. (2012), Comparison of Two Regression-Based Approaches for Determining Nutrient and Sediment Fluxes and Trends in the Chesapeake Bay Watershed: U.S. Geological Survey Scientific Investigations Report: 2012-5244, 118 p. \url{http://pubs.usgs.gov/sir/2012/5244/}

\bibitem{HirschV}
Rice, K.C., and Hirsch, R.M. (2012), Spatial and temporal trends in runoff at long-term streamgages within and near the Chesapeake Bay Watershed: U.S. Geological Survey Scientific Investigations Report: 2012-5151, 56 p. \url{http://pubs.usgs.gov/sir/2012/5151}


\end{thebibliography}

\end{document}

