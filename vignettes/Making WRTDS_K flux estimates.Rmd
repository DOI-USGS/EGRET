---
title: "Making WRTDS_K flux estimates"
author: "Robert M. Hirsch"
output: 
  rmarkdown::html_vignette
vignette: >
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteIndexEntry{WRTDS_K}
  \usepackage[utf8]{inputenc}
---
## Introduction
 
 This document provides a basic set of instructions for calculating WRTDS_K estimates of both concentration and flux (or "load").  It will not give the background or motivation for WRTDS_K.  There are now two published papers that lay out the motivation for using this approach, describe the mathematics, and show the results compared to other methods.  Those two publications can be found at:  https://pubs.er.usgs.gov/publication/sir20195084 and https://agupubs.onlinelibrary.wiley.com/doi/full/10.1029/2019WR025338 .
 
 The most important thing to know is that these are intended for use for providing the best possible estimates of the actual fluxes on each day, or month, or season, or year in the record.  They are not intended for use in evaluating trends (which we would do using flow-normalized flux).
 
## What one needs, in order to run this are the following items:
 
 * A data set which must already be in the form of an EGRET workspace containing an eList that has all four components of an **eList (INFO, Daily, Sample, surfaces)**.  That is, it must be the data and a WRTDS model based on that data that is already estimated.  The data set must be in the form of an ".RData" file that contains an **eList**,  Nothing else needs to be in that file but if there is other stuff, that's ok. The discussion and example shown here is carried out on a data set that contains no censored data ("less than values") and it never has more than one observation on a given day.  The description of the method and the examples run in the two papers mentioned are this kind of data set.  In the last section of this vignette there will be an explanation of how the method has been generalized to handle these two special cases.  The software presented here is able to properly handle data sets that have either or both of these properties (censoring and multiple observations per day).  
 
* There is a set of code (which you will see just below here). You need to copy that block of code into the working directory you will be using.  You can give it the name "functionsForK.R" although any name will be ok.  When you are ready to run an analysis you will need to source that code.  It contains several functions.

* There are four R packages required: **EGRET, lubridate, dplyr, and MASS**.  Make sure all of them are loaded on your computer from CRAN.  

Now here is the code.  

```{r include=TRUE, eval = FALSE}
################ you can call this functionsForK.R
# functionsForK is a set of add-ons to EGRET
# for doing the WRTDS_K estimation
# All fuctions (except for one) written by
# Robert M. Hirsch, in 2019
# this file is December 17, 2019
#############################
# makeDailyK
#  This function takes an existing eList 
#  Including the estimated model (the surfaces object in the eList)
#  And produces the daily WRTDS_K estimates of concentration and flux
# it requires MASS, EGRET, lubridate, and dplyr
###############################
makeDailyK <- function(eList, rho = 0.90, niter = 200, seed = 376168){
  set.seed(seed)
  # this part is to set up the array of runs of missing values
  localEList <- cleanUp(eList)
  localDaily <- populateDailySamp(localEList$Daily, localEList$Sample)
  numDays <- length(localDaily$Date)
  numDaysP <- numDays + 1
  # set up DailyGen which will hold the daily generated flux values for all days and all iterations
  DailyGen <- rep(0, numDays * niter)
  dim(DailyGen) <- c(numDays, niter)
  #   x is a vector of the standardized residuals for each day
  #   most of the elements of x will be NA but those from sampled days will have values
  x <- localDaily$stdResid
  #    xP is x that has been padded with a 0 at the start and a 0 at the end
  #    thus it is a vector that always starts and ends with non-missing values
  xP <- c(0,x,0)
  zz <- rle(is.na(xP))
  #    zz$lengths is a vector of run lengths
  #    zz$values is a vector of the values: TRUE means it is a run of missings, 
  #       FALSE means it is a run of non-missing values
  nRuns <- length(zz$lengths)
  zends <- cumsum(zz$lengths)
  nRunsM <- nRuns - 1
  zstarts <- c(0,zends[1:nRunsM])
  # doGap is the indexs of the runs that are missing values (it is just the even integers)
  doGap <- seq(2,nRunsM,2)
  # numGap is the number of groups of missing values to be filled in
  numGap <- length(doGap)
  # now we are ready to do the iterations to generate the series
  for(iter in 1:niter){
    localEList <- cleanUp(eList)
    # this next step adds a trueConc column to Daily, and it is NA if there is no sample value
    # it also adds the stdResid column to Daily
    localDaily <- populateDailySamp(localEList$Daily, localEList$Sample)
    x <- localDaily$stdResid
    #    xxP is x that has been padded with a 0 at the start and a 0 at the end
    #    thus it is a vector that always starts and ends with non-missing values
    xxP <- c(0,x,0)
    # now we are going to loop through all the gaps that need to be filled in
    for(i in 1:numGap) {
      iGap<-doGap[i]
      startFill<-zstarts[iGap]
      endFill<-zends[iGap]+1
      nFill<-zz$length[iGap]+2
      xfill<-genmissing(xxP[startFill],xxP[endFill],rho,nFill)
      xxP[startFill:endFill]<-xfill}
    # now we need to strip out the padded days
    xResid <- xxP[2:numDaysP]
    xConc <- exp((xResid*localDaily$SE)+localDaily$yHat)
    DailyGen[,iter] <- xConc * localDaily$Q * 86.4
  }
  # now we take means over all the iterations
  GenMean <- rep(NA, numDays)
  Daily <- eList$Daily
  for(i in 1 : numDays) {GenMean[i] <- mean(DailyGen[i,])}
  Daily$GenFlux <- GenMean
  Daily$GenConc <- Daily$GenFlux / (Daily$Q * 86.4)
  attr(Daily, "niter") <- niter
  attr(Daily, "rho") <- rho
  return(Daily)
}
#
#

genmissing<-function(X1,XN,rho,N){
  # this code was done by Tim Cohn
  #  X1 is the value before the gap
  #  XN is the value after the gap
  #  rho is the lag one autocorrelation
  #  N is the length of the sequence including X1 and XN 
  #     it is two more than the gap length
  #   it requires the MASS package
  C<-t(chol(rho^abs(outer(1:N,1:N, "-"))[c(1,N,2:(N-1)),c(1,N,2:(N-1))]))
  (C%*%c(ginv(C[1:2,1:2])%*%c(X1,XN),rnorm(N-2)))[c(1,3:N,2)]
}
#
#

####################################
# This function cleans up a Sample data frame
# It randomly picks one sample out of the multiple samples on a given day
# and it makes an augmented record substituting a random value on
# those days with censored data
#  Note, it must have an eList with a valid surfaces matrix 
# and the Sample data frame in the eList needs to have yHat and SE already calculated
####################################
cleanUp <- function(eList){
  Sample <- random_subset(eList$Sample, Julian)
  eListClean <- as.egret(eList$INFO, eList$Daily, Sample, eList$surfaces)
  eListClean <- makeAugmentedSample(eListClean)
  Sample <- eListClean$Sample
  Sample$Uncen <- 1
  Sample$ConcLow <- Sample$rObserved
  Sample$ConcHigh <- Sample$rObserved
  Sample$ConcAve <- Sample$rObserved
  eListClean <- as.egret(eList$INFO, eList$Daily, Sample, eList$surfaces)
  return(eListClean)
}
#
#
####################################
# function written by Laura De Cicco October 2019
#  requires library(dplyr)
# makes a copy of a data frame but when there are
# multiple values with the specified col_name it randomly
# picks one of them and drops the others
# useage newSample <- random_subset(Sample, Julian)
###################################
random_subset <- function(df, col_name){
  
  df_dups <- df %>% 
    group_by({{col_name}}) %>%
    filter(n() > 1) %>%
    group_by({{col_name}}) %>%
    sample_n(1)
  
  subDF <- df %>% 
    group_by({{col_name}}) %>%
    filter(n() == 1) %>%
    bind_rows(df_dups) %>% 
    arrange({{col_name}})
  
  return(subDF)
}
#
#
###########################
populateDailySamp<-function(localDaily=Daily,localSample=Sample) {
  numDays<-length(localDaily$Julian)
  numSamp<-length(localSample$Julian)
  trueConc <- rep(NA,numDays)
  trueFlux <- rep(NA, numDays)
  stdResid <- rep(NA,numDays)
  DailyOffset<-localDaily$Julian[1]-1
  for(samp in 1:numSamp){iday<-localSample$Julian[samp]-DailyOffset
  trueConc[iday]<-localSample$ConcAve[samp]
  trueFlux[iday] <- trueConc[iday] * localDaily$Q[iday] * 86.4
  stdResid[iday]<-(log(trueConc[iday])-localDaily$yHat[iday])/localDaily$SE[iday]
  }
  retDaily<-data.frame(localDaily,trueConc,trueFlux, stdResid)
  return(retDaily)
}
#
setupYearsKalmanFlux <- function (localDaily, paLong = 12, paStart = 10) 
{
# note that fluxes returned are the sum of the daily fluxes
# the units on the fluxes are all metric tons 
  numDays <- length(localDaily$MonthSeq)
  firstMonthSeq <- localDaily$MonthSeq[1]
  lastMonthSeq <- localDaily$MonthSeq[numDays]
  Starts <- seq(paStart, lastMonthSeq, 12)
  Ends <- Starts + paLong - 1
  StartEndSeq <- data.frame(Starts, Ends)
  StartEndSeq <- StartEndSeq[(StartEndSeq$Starts >= firstMonthSeq) & 
                               (StartEndSeq$Ends <= lastMonthSeq), ]
  firstMonth <- StartEndSeq[1, 1]
  numYears <- length(StartEndSeq$Starts)
  DecYear <- rep(NA, numYears)
  Q <- rep(NA, numYears)
  ConcDay <- rep(NA, numYears)
  GenConc <- rep(NA, numYears)
  FluxDay <- rep(NA, numYears)
  GenFlux <- rep(NA, numYears)
  for (i in 1:numYears) {
    startMonth <- (i - 1) * 12 + firstMonth
    stopMonth <- startMonth + paLong - 1
    DailyYear <- localDaily[which(localDaily$MonthSeq %in% 
                                    startMonth:stopMonth), ]
    counter <- ifelse(is.na(DailyYear$FluxDay), 0, 1)
    if (length(counter) > 0) {
      good <- (sum(counter) == length(counter))
    }
    else {
      good <- FALSE
    }
    DecYear[i] <- mean(DailyYear$DecYear)
    Q[i] <- mean(DailyYear$Q)
    if (good) {
      ConcDay[i] <- mean(DailyYear$ConcDay, na.rm = TRUE)
      GenConc[i] <- mean(DailyYear$GenConc, na.rm = TRUE)
      FluxDay[i] <- sum(DailyYear$FluxDay, na.rm = TRUE) / 1000
      GenFlux[i] <- sum(DailyYear$GenFlux, na.rm = TRUE) / 1000
    }
  }
  AnnualResults <- data.frame(DecYear, Q, ConcDay, GenConc, FluxDay, GenFlux)
  attr(AnnualResults,"paStart") <- paStart
  attr(AnnualResults,"paLong") <- paLong
  return(AnnualResults)
}
#
#
#
computeAnnual <- function(eList, Daily, paStart = 10, paLong = 12) {
  # it provides a printed list of the annual values and a set of plots
  # if you don't want that output 
  # you can get the same thing with AnnualResults <- setupYearsKalmanFlux()
  # This function creates an annual series of results
  # The annual results can be for a specific season, specified by paStart and paLong
  # The default is the water year
  # FluxDay is the traditional regression estimate of Flux
  # GenFlux is the Kalman Filter estimate
  # ConcDay is the traditional regression estimate of Concentration 
  # GenConc is the Kalman Filter estimate
  AnnualResults <- setupYearsKalmanFlux(Daily, paStart = paStart, paLong = paLong)
  # in the print out Q is the annual mean value in m^3/s
  # the two flux values are in metric tons kilograms (1000 kg)
  print(eList$INFO$shortName)
  print(eList$INFO$paramShortName)
  period <- paste("paStart is",paStart," paLong is",paLong, sep = " ")
  print(period)
  print(AnnualResults)
  yMax <- 1.1 * max(AnnualResults$FluxDay, AnnualResults$GenFlux)
  nYears <- length(AnnualResults[,1])
  # first a plot of just the WRTDS estimate
  xMin <- floor(AnnualResults[1,1])
  xMax <- ceiling(AnnualResults[nYears,1])
  xlim <- c(xMin,xMax)
  title1 <- paste(eList$INFO$shortName,eList$INFO$paramShortName,
                  "\nAnnual Flux Estimates: WRTDS in red, WRTDS-K in green\n",period,sep="  ")
  title2 <- paste(eList$INFO$shortName,eList$INFO$paramShortName,
                  "\nComparison of the two flux estimates\n",period,sep="  ")
  #
  plot(AnnualResults$DecYear, AnnualResults$FluxDay, pch = 20, cex = 1.3, xlim = xlim, xaxs = "i",
       ylim = c(0, yMax), yaxs = "i", xlab = "", ylab = "Annual flux, metric tons", 
       main = title1, las = 1, col = "red",
       tck = 0.02, cex.main = 1.1, cex.lab = 0.95)
  par(new = TRUE)
  plot(AnnualResults$DecYear, AnnualResults$GenFlux, pch = 20, cex = 1.4, col = "green", xlim = xlim, xaxs = "i",
       ylim = c(0, yMax), yaxs = "i", xlab = "", ylab = "", main = "", las = 1, tck = 0.02, axes = FALSE)
  # scatter plot
  plot(AnnualResults$FluxDay, AnnualResults$GenFlux, pch = 20, cex = 1.3, col = "red", xlim = c(0, yMax), xaxs = "i",
       ylim = c(0, yMax), las = 1, yaxs = "i", xlab = "WRTDS estimate of annual flux, in metric tons", ylab = 
         "WRTDS_K estimate of annual flux, in metric tons", cex.main = 1.1, cex.lab = 0.95, cex.axis = 1.2, 
       main = title2)
  abline(a = 0, b = 1)
  return(AnnualResults)
}
#
#
#
plotTimeSlice <- function(eList, Daily, start, end){
  Daily <- subset(Daily,DecYear >= start & DecYear <= end)
  concHigh <- 1.1 * max(Daily$trueConc,Daily$GenConc,Daily$ConcDay,na.rm = TRUE)
  concLow <- 0.9 * min(Daily$trueConc,Daily$GenConc,Daily$ConcDay,na.rm = TRUE)
  fluxHigh <- 1.1 * max(Daily$trueFlux,Daily$GenFlux,Daily$FluxDay,na.rm = TRUE)
  fluxLow <- 0.9 * min(Daily$trueFlux,Daily$GenFlux,Daily$FluxDay,na.rm = TRUE)
  # figure out which data symbol to use, red for uncensored, brown for censored
  eList$Sample$color <- ifelse(eList$Sample$Uncen == 1, "red", "cyan4")
  par(tck = 0.02, las = 1)
  # first concentration, then flux
  name <- paste(eList$INFO$shortName, eList$INFO$paramShortName, sep = " ")
  ratio <- mean(Daily$GenConc) / mean(Daily$ConcDay)
  fratio <- format(ratio, digits = 2)
  concTitle <- paste(name,"\nConcentrations, Black is WRTDS, Green is WRTDS_K\nData in red, (rl in blue if <), Ratio of means is", fratio, sep = " ")
  
  plot(Daily$DecYear, Daily$ConcDay, log = "y", type = "l", las = 1, xlim = c(start, end), 
       xaxs = "i", ylim = c(concLow,concHigh), yaxs = "i", xlab = "", cex.main = 0.9, 
       ylab = "Concentration, in milligrams per Liter",
       main = concTitle)
  par(new = TRUE)
  plot(eList$Sample$DecYear, eList$Sample$ConcHigh, log = "y", pch = 20, cex = 1.1, col = eList$Sample$color, 
       xlim = c(start, end), xaxs = "i", ylim = c(concLow,concHigh), yaxs = "i", xlab = "",
       ylab = "", main = "", axes = FALSE)
  par(new = TRUE)
  plot(Daily$DecYear, Daily$GenConc, log = "y", type = "l", xlim = c(start, end), 
       xaxs = "i", ylim = c(concLow,concHigh), yaxs = "i", xlab = "", col = "green", 
       ylab = "", main = "", axes = FALSE)
  # flux graph
  ratio <- mean(Daily$GenFlux) / mean(Daily$FluxDay)
  fratio <- format(ratio, digits = 2)
  fluxTitle <- paste(name,"\nFlux, Black is WRTDS, Green is WRTDS_K\nData in red, (rl in blue if <), Ratio of means is", fratio, sep = " ")
  plot(Daily$DecYear, Daily$FluxDay, log = "y", type = "l", xlim = c(start, end), 
       xaxs = "i", ylim = c(fluxLow,fluxHigh), yaxs = "i", xlab = "", las = 1,  
       ylab = "Flux, in kg per day", cex.main = 0.95,
       main = fluxTitle)
  par(new = TRUE)
  plot(eList$Sample$DecYear, eList$Sample$ConcHigh * eList$Sample$Q * 86.4, log = "y", pch = 20, 
       cex = 1.1, col = eList$Sample$color, 
       xlim = c(start, end), xaxs = "i", ylim = c(fluxLow,fluxHigh), yaxs = "i", xlab = "",
       ylab = "", main = "", axes = FALSE)
  par(new = TRUE)
  plot(Daily$DecYear, Daily$GenFlux, log = "y", type = "l", xlim = c(start, end), 
       xaxs = "i", ylim = c(fluxLow,fluxHigh), yaxs = "i", xlab = "", col = "green", 
       ylab = "", main = "", axes = FALSE)
}
#
#
#
# script for identifying if a Sample data frame has
# any days with multiple samples
# or any samples that are uncensored
specialCase <- function(eList) {
  Sample <- eList$Sample
  n <- length(Sample$Date)
  days <- unique(Sample$Julian)
  nDays <- length(days)
  mult <- if(n > nDays) TRUE else FALSE
  nUncen <- sum(Sample$Uncen)
  cen <- if(nUncen < n) TRUE else FALSE
  # when mult is TRUE, needs to go through the subsampling process each time
  # when cen is TRUE it needs to go through the random augmentation 
  special <- data.frame(mult, cen)
  return(special)
}

```

## Running the software

Make sure that the **.RData** file and the code are in the workspace you want to work in and that you have properly set the working directory with the **setwd()** command.  Be sure that the eList contains the **surfaces** matrix, which is the estimated WRTDS model, based on the data set in the **Sample** data frame.  Here is an example of a run, with some comments along the way.  The actual commands are shown in the shaded part.





```{r eval = TRUE, echo = TRUE}

library(EGRET)
library(MASS)
library(lubridate)
library(dplyr)
load("rockCr.tp.RData") # the name of your EGRET workspace
source("functionsForK.R")
# now we will run the WRTDS_K estimation (I'm using the defaults for now)
DailyK <- makeDailyK(eList)
print(summary(DailyK))
print(attr(DailyK,"niter"))
print(attr(DailyK,"rho"))

```
The object being created here is a data frame called **DailyK**.  It looks just like **Daily** but with some columns added.  What are the extra columns?

**trueConc** is the measured concentration on the days when there is a value, many will be NA
 
**trueFlux** (in kg/day) flux for the days when concentration is measured

**stdResid** (dimensionless) this is the standardized residual from the WRTDS model estimate, for the days when concentration is measured.  It is (ln(trueConc) - yHat) / SE.

**yHat** is the WRTDS estimate of the natural log of concentration and SE is the standard error of the WRTDS model for the specific time, discharge, and season.

**GenFlux** is the WRTDS_K estimate of the flux for each day (in kg/day)
**GenConc** is the WRTDS_K estimate of the concentration for each day (in kg/day)

# Summarizing results at an annual time step

Now we can take the results from the **DailyK** data frame and compute annual flux values.  We will do that for the regular WRTDS and for WRTDS_K (using the data in two columns of **DailyK** the **FluxDay** column and the **GenFlux** column).  We can also do these computations for some period of analysis other than the water year, but for now, we will just show the water year computations.  The function used is called **computeAnnual** and it produces a data frame called **AnnualResults** and also produces some graphics that show the two types of results.  Notice that **AnnualResults** has some attributes that go with it that will tell you what the period of analysis was.  They are printed here.  Note that the function **computeAnnual** actually does more than the computations, it prints out the content of AnnualResults and it also makes two graphs that tell you some things about how the two types of estimates compare to each other.

```{r eval = TRUE, echo = TRUE}
AnnualResults <- computeAnnual(eList, Daily = DailyK)
print(attr(AnnualResults, "paStart"))
print(attr(AnnualResults, "paLong"))
```

The content of **AnnualResults** is fairly obvious.

**DecYear** is the mean day of the year for the period of analysis, for example, water year 2007 would have a mean day of 2007.247 (end of March of 2007).

**Q** is the mean discharge in m^3/s.

**ConcDay** is the mean value of concentration for the year from the regular WRTDS model, in mg/L.

**GenConc** is the mean value of concentration for the year from the WRTDS_K model, in mg/L.

**FluxDay** is the sum of the daily flux values from the regular WRTDS model, in units of metric tons (same as 10^3 kg).

**GenFlux** is the sum of the daily flux values from the WRTDS_K model, in units of metric tons (same as 10^3 kg).

The first graph compares the time series of the two flux records: WRTDS in red and WRTDS_K in green.  This graph is fairly typical of what we have seen in a number of studies so far.  There are a number of years in which the two estimates are practically identical (e.g. 2011, 2012, 2014) but there are a few cases where they diverge significantly.  In this example, in those cases the WRTDS_K estimate is substantially lower than the WRTDS estimate.  What that means is that the tendency for WRTDS to predict values that are too high has been reduced.  It happens that the flux bias statistic for this data set is 0.35, suggesting that the model has a tendency to estimate fluxes that are too high.

The second graph is just another way to look at these same results, but as a scatter plot of results from the two methods.  What we see is a tendency for a fair number of years to plot close to the 1:1 line but four of them are substantially below the line (meaning their WRTDS_K estimates are lower than their WRTDS estimates).

Seeing this, we'd like to dig in a bit and see what's going on.  

# Looking at parts of the record to see how WRTDS and WRTDS_K are working

We have a function that produces graphics that show, as a time series, the daily true values (for the days that have samples), the WRTDS estimates for every day, and the WRTDS_K estimates for every day.  We could plot the whole record, but the ups and downs of the curves would be so tight that we really couldn't see what's going on.  So, we let the user pick a time slice to look at.  It produces two plots, the first is for concentration (it is typically easier to see what is happening in the estimation with the concentration graphs) and the second is for flux (discharge plays such a big role here that the propagation of error from the concentrations gets somewhat obscured, but flux is, after all, what we are interested in here).

We will look at two examples here, in each case looking at about a half a year.  The first one (2018) is a year in which there was very good agreement between the estimates, and the second one (2015) is a year with a large difference between the methods.  Let's see what those results look like.

```{r eval = TRUE, echo = TRUE}
plotTimeSlice(eList, Daily = DailyK, start = 2018.2, end = 2018.7)
plotTimeSlice(eList, Daily = DailyK, start = 2015.2, end = 2015.7)
```

What can we learn from these figures?  The first, which is a half year in 2018.  Note before we start that all of these graphs show either concentration or flux on a log scale.  We will start with the concentration graph.  We see that there are 7 observed values.  When we compare the observed values (red dots) with the WRTDS estimates (black line) for each of those days we see two negative residuals followed by a positive residual a negative a positive and the final two have residuals that are close to zero.  Once we get past the second observation (around 2018.25) we see the WRTDS_K curve (in green) going back a forth from being a little above the WRTDS curve to a little below.  This alternation indicates that the errors of the model are not ones that persist for very many days and thus the corrections based on these errors don't persist for very many days.  One other thing to note is that the model suggests a big spike in concentration around 2018.4 but there are no samples very close in time to this event (there is a gap of about a month) and the big discharge event took place about half way between these two sample dates.  Given this lack of information close to the date of the large flow event, the WRTDS_K will not result in much of a change during this event as compared to what we estimate from WRTDS.  The upshot of all of this is that the residuals information doesn't make much difference during this half year in WRTDS_K results and we really don't change our estimate by much.

Now, contrast this with the 2015 results.  Here we see the first residual being very near zero, but after that every one of them is substantially negative (below the black line).  What this is telling us is that something was going on in this part of 2015 such that concentrations were always lower than the WRTDS model (based on the years around 2015, both before and after).  We don't happen to know why this departure exists (perhaps it relates to what tributaries were most active or to something about antecedent conditions).  What it does tell us is that generally, throughout the period 2015.35 through 2015.65 it is a good guess that concentrations are likely to have been lower than what the WRTDS model estimates they should be.  The math of the WRTDS_K model ends up giving us a green curve that is always below the black curve throughout this period.  That means, when the WRTDS_K estimate is summarized for the whole 2015 water year it will be a good deal lower than what WRTDS would give us.  Looking at the last of the four graphs (for flux), even though the black and green curves track pretty close to each other the ratio of the two mean fluxes is 0.45.  So, the persistant negative residuals causes us to make a major downward adjustment to our estimated flux for the period.

As a generalization we can say that WRTDS_K will result in large adjustments when two things happen: 1) there is a strong persistence of the residuals (long runs of positives or long runs of negatives), and 2) when there are samples collected on or close to the days of maximum flux and they show large absolute residuals. 

One final note.  These graphs also have an additional bit of information.  They say, in their title "Ratio of means is xx".  This ratio is specific to the time slice shown in the plot (not the whole record being estimated) and the ratio stated is WRTDS_K estimate divided by the regular WRTDS estimate.  

# Two options available (setting rho and setting niter)

One of the things that the user must select is the rho value.  We don't have a strong theoretical basis for selecting the right value of rho, although our research has shown that the results are not highly sensitive to it.  In the paper published in Zhang and Hirsch (2019) (see the second URL near the start of this document) we do make some generalizations about the selection of rho.  We found that for nitrate a slightly higher rho value (such as 0.95) may be better than for other constitutents such as TP, OrthoP, Suspended Sediment, or Chloride for which values like 0.85 or 0.9 may be better.  The somewhat different behavior for nitrate is explained by the fact that at many sites there are some other factors probably related to denitrification the discharge and term does not have a great amount of explanatory power in the WRTDS model. As such, placing more reliance on the results of samples that are close in time is appropriate.  One can experiment with different values of rho because it is an argument in the **makeDailyK** function.

We can re-run the analysis with rho of 0.85 and see how much difference it makes.  The table shown here lists the ratio the difference in annual values divided by the original annual value (so multiplying these numbers by 100 would result in differences in percent).

```{r eval = TRUE, echo = TRUE}
DailyK2 <- makeDailyK(eList, rho = 0.85)
print(attr(DailyK2,"rho"))
print(attr(DailyK2,"niter"))
AnnualResults2 <- computeAnnual(eList, Daily = DailyK2)
Ratios <- (AnnualResults2 - AnnualResults) / AnnualResults
print(Ratios)
```

What we see here is that the change in rho from 0.9 to 0.85 makes less than a 10% difference in any of the years.

Setting the number of iterations to 200 (the default) seems to be sufficient.  We can set a different random number seed and see how much difference it makes in the results. 


```{r eval = TRUE, echo = TRUE}
DailyK2 <- makeDailyK(eList, seed = 38109695)
AnnualResults2 <- computeAnnual(eList, Daily = DailyK2)
Ratios <- (AnnualResults2 - AnnualResults) / AnnualResults
print(Ratios)
```

This table shows us that, at worst the individual annual fluxes differ by about 11% and many of them differ by less than 2% in successive runs with different seeds  The annual concentrations never differ by more than about 2%.  This suggests that 200 is a sufficient number of iterations to run to obtain a stable result.  For final calculations for publication one might want to specify niter = 500 or niter = 1000 in the call to **makeDailyK**, but the idea here is not perfection, but rather to make a reasonable adjustment of the flux record to account for the serial correlation of the residuals.

# What about putting these results into the **plotConcHist** or **plotFluxHist** graphs

Typically when we do some type of trend study we may want to create graphic outputs showing the Flow Normalized values (connected by a line) and the estimated annual values (as dots).  If you want to put the WRTDS_K values on these graphs you can use some substitute functions.  

**plotConcHistK** works exactly like **plotConcHist** but there is a new second argument, it is **DailyK**.  So in it's simplest form the call would be **plotConcHistK(eList, DailyK)**.  The markings in the title have a *(K)* after the word *Estimates*.

**plotFluxHistK** works exactly like **plotFluxHist** but there is a new second argument, it is **DailyK**.  So in it's simplest form the call would be **plotFluxHistK(eList, DailyK)**.  The markings in the title have a *(K)* after the word *Estimates*.

Here they are being used.

```{r EVAL = TRUE, echo = TRUE}
plotConcHistK(eList, DailyK)
plotFluxHistK(eList, DailyK, fluxUnit = 8)
```

# Operationalizing this in a batch job

The process after loading the necessary packages and the functionsForK code is this.

* load the workspace for a site
* give the command **DailyK <- makeDailyK(eList)**
* give the command **AnnualResults <- computeAnnual(eList, Daily = DailyK)**
* then save the object **AnnualResults**
* there is an alternative to this that doesn't print out anything or plot anything.  It just **AnnualResults <- setupYearsKalmanFlux()**
* the tables and graphs you will probably want can all be made from the content of Annual Results, and the meta data are all in the **INFO** object in the **eList**
* if the iterest is in only one season of the year then modify the call to **computeAnnual** by adding the **paStart** and **paLong** arguments for the season you are interested in.
* note that **AnnualResults** has two attributes that tell the **paStart** and **paLong**, so you can always tell what you used as your period of analysis

# How the code handles two kinds of special situations

The description in the two publications mentioned here accurately describes how the computations work in the situation where no days in the record have more than one observed value and where no censored data exist in the record.  The following section describes how the code handles data sets that violate one or both of these constraints.  You don't really have to understand this to use the software, but for completeness we are describing how these situations are handled in the code.  

## Censored data (less than values)

In order to fill in estimates for all the days with no samples, we must have a known value on each of the sampled days.  We use this known value to compute a residual on the sampled day.  These known residuals on either side of a data gap are what initialize the AR(1) process that fills in all the missing values of residuals in the gap.  It would be incorrect to set these censored values with the reporting limit (**ConcHigh**) and also incorrect to set these censored values to half the reporting limit (this would be **ConcAve**).  The solution is this.  Since we are estimating the entire time series of residuals using Monte Carlo simulation we can also use Monte Carlo methods to create an appropriate value for the sampled day.  For each replicate of the whole time series we will generate random values for each of the censored sample days.  We already have a function that allows us to generate these random realizations of the censored day concentration.  It is the function in EGRET called **makeAugmentedSample**.  What it does is that on each day with a censored value it defines a truncated log normal distribution and takes a random sample from that distribution to represent the unknown true value on that day.  The truncated lognormal distribution is defined by the WRTDS model for that day.  It uses estimates of the conditional mean of the log concentration and the conditional standard deviation of the log concentration and assumes that the conditional distribution of the logs is normal.  The distribution is not the whole normal distribution, rather it is made up of the left hand tail of the distribution.  The truncation point is at the log of the reporting limit for the data value.    What this means is that for the censored day, we can create a Monte Carlo sample value of the log of concentration by sampling from that truncated normal distribution (which is specific to the conditions on that day).  Note here that the random observation values generated here are NOT used to estimate the WRTDS model; that is done in the usual fashion using the censored sample information.

So now the data generation scheme is done in two phases for each iteration: First we fill in values for each of the censored days and these, combined with the uncensored values constitute the sample data set we will work from.  Then we use the AR(1) process to fill in the missing days between all the sampled days.  Then when we move on to the next iteration we start all over with estimating all the censored days again and then fill the gaps, etc.

When we plot these data sets using **plotTimeSlice**, when there are censored values they plot in the blue color and they are located on the vertical scale at the reporting limit (meaning that the true value lies somewhere at or below the blue dot).  

## Multiple observations on a day

When a day has more than one observation (i.e. more than one sample value) the approach we use here is much the same as what we used in the censored value situation.  What we need in order to generate the full WRTDS_K record is a set of unique values for the sampled days.  So, for any day with two or more samples we will randomly pick one of them and use it as an end point for the adjacent gap periods.  Then once we pick it we generate the values between the sampled days regardless of whether the sample value used is a unique value for that day or if it is randomly selected from the multiple values observed on that day.  Then on the next iteration we randomly select the values for these multiply sampled days and proceed with the AR(1) process to fill in between the sampled days.  In the plot produced by plotTimeSlice all of the observations are shown as red dots.  Thus if there are multiple sample values on a given day they will plot along a vertical line situated on that day.

## An example with both special issues

```{r EVAL = TRUE, echo = TRUE}
rm(eList, DailyK)
load("test.RData") # the name of your EGRET workspace
DailyK <- makeDailyK(eList)
plotTimeSlice(eList, DailyK, start = 2010.2, end = 2010.4)
```

If we look at the concentration graph we see two things of interest here.  Right after 2010.30 there appear to be three days with censored values with a reporting limit at 0.004 mg/L.  What is interesting here is that generally the green line (WRTDS_K estimates) go right through the samples (the red dots), here they don't.  That is because in the Monte Carlo simulation to "true" samples generated are always less than the reporting limit (the blue dots) so the green line lies below these points.  Also, there is a case at around 2010.38 where there are two data points (red points) that lie one directly above the other.  These are two sample values from the same day.  The WRTDS_K estimate (green line) goes right between these two sample values because in the Monte Carlo simulation sometimes the sample value is the lower one and sometimes it is the upper one and the result is that the estimate lies between the two.

# Final thoughts
In a few months we expect to better integrate these WRTDS_K computations into EGRET for a new release of EGRET, but for now (December 2019) this is the way it should be implemented.  Feedback is welcome before we more fully integrate it.  The citations for the two papers that introduce and experiment with WRTDS_K are as follows.

Lee, C.J., Hirsch, R.M., and Crawford, C.G., 2019, An evaluation of methods for computing annual water-quality loads: U.S. Geological Survey Scientific Investigations Report 2019–5084, 59 p., https://doi.org/10.3133/sir20195084.

Zhang, Q. and Hirsch, R.M., 2019. River water‐quality concentration and flux estimation can be improved by accounting for serial correlation through an autoregressive model. Water Resources Research.  https://agupubs.onlinelibrary.wiley.com/doi/epdf/10.1029/2019WR025338